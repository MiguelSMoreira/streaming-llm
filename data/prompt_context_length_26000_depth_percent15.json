[{"role": "system", "content": "You are a helpful AI bot that answers questions for a user. Keep your response short and direct"}, {"role": "user", "content": "July 2006I've discovered a handy test for figuring out what you're addicted\nto.  Imagine you were going to spend the weekend at a friend's house\non a little island off the coast of Maine.  There are no shops on\nthe island and you won't be able to leave while you're there.  Also,\nyou've never been to this house before, so you can't assume it will\nhave more than any house might.What, besides clothes and toiletries, do you make a point of packing?\nThat's what you're addicted to.  For example, if you find yourself\npacking a bottle of vodka (just in case), you may want to stop and\nthink about that.For me the list is four things: books, earplugs, a notebook, and a\npen.There are other things I might bring if I thought of it, like music,\nor tea, but I can live without them.  I'm not so addicted to caffeine\nthat I wouldn't risk the house not having any tea, just for a\nweekend.Quiet is another matter.  I realize it seems a bit eccentric to\ntake earplugs on a trip to an island off the coast of Maine.  If\nanywhere should be quiet, that should.  But what if the person in\nthe next room snored?  What if there was a kid playing basketball?\n(Thump, thump, thump... thump.)  Why risk it?  Earplugs are small.Sometimes I can think with noise.  If I already have momentum on\nsome project, I can work in noisy places.  I can edit an essay or\ndebug code in an airport.  But airports are not so bad: most of the\nnoise is whitish.  I couldn't work with the sound of a sitcom coming\nthrough the wall, or a car in the street playing thump-thump music.And of course there's another kind of thinking, when you're starting\nsomething new, that requires complete quiet.   You never\nknow when this will strike. It's just as well to carry plugs.The notebook and pen are professional equipment, as it were.  Though\nactually there is something druglike about them, in the sense that\ntheir main purpose is to make me feel better.  I hardly ever go\nback and read stuff I write down in notebooks.  It's just that if\nI can't write things down, worrying about remembering one idea gets\nin the way of having the next.  Pen and paper wick ideas.The best notebooks I've found are made by a company called Miquelrius.\nI use their smallest size, which is about 2.5 x 4 in.\nThe secret to writing on such\nnarrow pages is to break words only when you run out of space, like\na Latin inscription.  I use the cheapest plastic Bic ballpoints,\npartly because their gluey ink doesn't seep through pages, and\npartly so I don't worry about losing them.I only started carrying a notebook about three years ago.  Before\nthat I used whatever scraps of paper I could find.  But the problem\nwith scraps of paper is that they're not ordered.  In a notebook\nyou can guess what a scribble means by looking at the pages\naround it.  In the scrap era I was constantly finding notes I'd\nwritten years before that might say something I needed to remember,\nif I could only figure out what.As for books, I know the house would probably have something to\nread.  On the average trip I bring four books and only read one of\nthem, because I find new books to read en route.  Really bringing\nbooks is insurance.I realize this dependence on books is not entirely good\u2014that what\nI need them for is distraction.  The books I bring on trips are\noften quite virtuous, the sort of stuff that might be assigned\nreading in a college class.  But I know my motives aren't virtuous.\nI bring books because if the world gets boring I need to be able\nto slip into another distilled by some writer.  It's like eating\njam when you know you should be eating fruit.There is a point where I'll do without books.  I was walking in\nsome steep mountains once, and decided I'd rather just think, if I\nwas bored, rather than carry a single unnecessary ounce.  It wasn't\nso bad.  I found I could entertain myself by having ideas instead\nof reading other people's.  If you stop eating jam, fruit starts\nto taste better.So maybe I'll try not bringing books on some future trip.  They're\ngoing to have to pry the plugs out of my cold, dead ears, however.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nMarch 2008, rev. June 2008Technology tends to separate normal from natural.  Our bodies\nweren't designed to eat the foods that people in rich countries eat, or\nto get so little exercise.  \nThere may be a similar problem with the way we work: \na normal job may be as bad for us intellectually as white flour\nor sugar is for us physically.I began to suspect this after spending several years working \nwith startup founders.  I've now worked with over 200 of them, and I've\nnoticed a definite difference between programmers working on their\nown startups and those working for large organizations.\nI wouldn't say founders seem happier, necessarily;\nstarting a startup can be very stressful. Maybe the best way to put\nit is to say that they're happier in the sense that your body is\nhappier during a long run than sitting on a sofa eating\ndoughnuts.Though they're statistically abnormal, startup founders seem to be\nworking in a way that's more natural for humans.I was in Africa last year and saw a lot of animals in the wild that\nI'd only seen in zoos before. It was remarkable how different they\nseemed. Particularly lions. Lions in the wild seem about ten times\nmore alive. They're like different animals. I suspect that working\nfor oneself feels better to humans in much the same way that living\nin the wild must feel better to a wide-ranging predator like a lion.\nLife in a zoo is easier, but it isn't the life they were designed\nfor.\nTreesWhat's so unnatural about working for a big company?  The root of\nthe problem is that humans weren't meant to work in such large\ngroups.Another thing you notice when you see animals in the wild is that\neach species thrives in groups of a certain size.  A herd of impalas\nmight have 100 adults; baboons maybe 20; lions rarely 10.  Humans\nalso seem designed to work in groups, and what I've read about\nhunter-gatherers accords with research on organizations and my own\nexperience to suggest roughly what the ideal size is: groups of 8\nwork well; by 20 they're getting hard to manage; and a group of 50\nis really unwieldy.\n[1]\nWhatever the upper limit is, we are clearly not meant to work in\ngroups of several hundred.  And yet\u2014for reasons having more\nto do with technology than human nature\u2014a great many people\nwork for companies with hundreds or thousands of employees.Companies know groups that large wouldn't work, so they divide\nthemselves into units small enough to work together.  But to\ncoordinate these they have to introduce something new: bosses.These smaller groups are always arranged in a tree structure.  Your\nboss is the point where your group attaches to the tree.  But when\nyou use this trick for dividing a large group into smaller ones,\nsomething strange happens that I've never heard anyone mention\nexplicitly.  In the group one level up from yours, your boss\nrepresents your entire group.  A group of 10 managers is not merely\na group of 10 people working together in the usual way.  It's really\na group of groups.  Which means for a group of 10 managers to work\ntogether as if they were simply a group of 10 individuals, the group\nworking for each manager would have to work as if they were a single\nperson\u2014the workers and manager would each share only one\nperson's worth of freedom between them.In practice a group of people are never able to act as if they were\none person.  But in a large organization divided into groups in\nthis way, the pressure is always in that direction.  Each group\ntries its best to work as if it were the small group of individuals\nthat humans were designed to work in.  That was the point of creating\nit.  And when you propagate that constraint, the result is that\neach person gets freedom of action in inverse proportion to the\nsize of the entire tree.\n[2]Anyone who's worked for a large organization has felt this.  You\ncan feel the difference between working for a company with 100\nemployees and one with 10,000, even if your group has only 10 people.\nCorn SyrupA group of 10 people within a large organization is a kind of fake\ntribe.  The number of people you interact with is about right.  But\nsomething is missing: individual initiative.  Tribes of hunter-gatherers\nhave much more freedom.  The leaders have a little more power than other\nmembers of the tribe, but they don't generally tell them what to\ndo and when the way a boss can.It's not your boss's fault.  The real problem is that in the group\nabove you in the hierarchy, your entire group is one virtual person.\nYour boss is just the way that constraint is imparted to you.So working in a group of 10 people within a large organization feels\nboth right and wrong at the same time.   On the surface it feels\nlike the kind of group you're meant to work in, but something major\nis missing.  A job at a big company is like high fructose corn\nsyrup: it has some of the qualities of things you're meant to like,\nbut is disastrously lacking in others.Indeed, food is an excellent metaphor to explain what's wrong with\nthe usual sort of job.For example, working for a big company is the default thing to do,\nat least for programmers.  How bad could it be?  Well, food shows\nthat pretty clearly.  If you were dropped at a random point in\nAmerica today, nearly all the food around you would be bad for you.\nHumans were not designed to eat white flour, refined sugar, high\nfructose corn syrup, and hydrogenated vegetable oil.  And yet if\nyou analyzed the contents of the average grocery store you'd probably\nfind these four ingredients accounted for most of the calories.\n\"Normal\" food is terribly bad for you.  The only people who eat\nwhat humans were actually designed to eat are a few Birkenstock-wearing\nweirdos in Berkeley.If \"normal\" food is so bad for us, why is it so common?  There are\ntwo main reasons. One is that it has more immediate appeal.  You\nmay feel lousy an hour after eating that pizza, but eating the first\ncouple bites feels great.  The other is economies of scale.\nProducing junk food scales; producing fresh vegetables doesn't.\nWhich means (a) junk food can be very cheap, and (b) it's worth\nspending a lot to market it.If people have to choose between something that's cheap, heavily\nmarketed, and appealing in the short term, and something that's\nexpensive, obscure, and appealing in the long term, which do you\nthink most will choose?It's the same with work.  The average MIT graduate wants to work\nat Google or Microsoft, because it's a recognized brand, it's safe,\nand they'll get paid a good salary right away.  It's the job\nequivalent of the pizza they had for lunch.  The drawbacks will\nonly become apparent later, and then only in a vague sense of\nmalaise.And founders and early employees of startups, meanwhile, are like\nthe Birkenstock-wearing weirdos of Berkeley:  though a tiny minority\nof the population, they're the ones living as humans are meant to.\nIn an artificial world, only extremists live naturally.\nProgrammersThe restrictiveness of big company jobs is particularly hard on\nprogrammers, because the essence of programming is to build new\nthings.  Sales people make much the same pitches every day; support\npeople answer much the same questions; but once you've written a\npiece of code you don't need to write it again.  So a programmer\nworking as programmers are meant to is always making new things.\nAnd when you're part of an organization whose structure gives each\nperson freedom in inverse proportion to the size of the tree, you're\ngoing to face resistance when you do something new.This seems an inevitable consequence of bigness.  It's true even\nin the smartest companies.  I was talking recently to a founder who\nconsidered starting a startup right out of college, but went to\nwork for Google instead because he thought he'd learn more there.\nHe didn't learn as much as he expected.  Programmers learn by doing,\nand most of the things he wanted to do, he couldn't\u2014sometimes\nbecause the company wouldn't let him, but often because the company's\ncode wouldn't let him.  Between the drag of legacy code, the overhead\nof doing development in such a large organization, and the restrictions\nimposed by interfaces owned by other groups, he could only try a\nfraction of the things he would have liked to.  He said he has\nlearned much more in his own startup, despite the fact that he has\nto do all the company's errands as well as programming, because at\nleast when he's programming he can do whatever he wants.An obstacle downstream propagates upstream.  If you're not allowed\nto implement new ideas, you stop having them.  And vice versa: when\nyou can do whatever you want, you have more ideas about what to do.\nSo working for yourself makes your brain more powerful in the same\nway a low-restriction exhaust system makes an engine more powerful.Working for yourself doesn't have to mean starting a startup, of\ncourse.  But a programmer deciding between a regular job at a big\ncompany and their own startup is probably going to learn more doing\nthe startup.You can adjust the amount of freedom you get by scaling the size\nof company you work for.  If you start the company, you'll have the\nmost freedom.  If you become one of the first 10 employees you'll\nhave almost as much freedom as the founders.  Even a company with\n100 people will feel different from one with 1000.Working for a small company doesn't ensure freedom.  The tree\nstructure of large organizations sets an upper bound on freedom,\nnot a lower bound.  The head of a small company may still choose\nto be a tyrant.  The point is that a large organization is compelled\nby its structure to be one.\nConsequencesThat has real consequences for both organizations and individuals.\nOne is that companies will inevitably slow down as they grow larger,\nno matter how hard they try to keep their startup mojo.  It's a\nconsequence of the tree structure that every large organization is\nforced to adopt.Or rather, a large organization could only avoid slowing down if\nthey avoided tree structure.  And since human nature limits the\nsize of group that can work together, the only way I can imagine\nfor larger groups to avoid tree structure would be to have no\nstructure: to have each group actually be independent, and to work\ntogether the way components of a market economy do.That might be worth exploring.  I suspect there are already some\nhighly partitionable businesses that lean this way.  But I don't\nknow any technology companies that have done it.There is one thing companies can do short of structuring themselves\nas sponges:  they can stay small.  If I'm right, then it really\npays to keep a company as small as it can be at every stage.\nParticularly a technology company.  Which means it's doubly important\nto hire the best people.  Mediocre hires hurt you twice: they get\nless done, but they also make you big, because you need more of\nthem to solve a given problem.For individuals the upshot is the same: aim small.  It will always\nsuck to work for large organizations, and the larger the organization,\nthe more it will suck.In an essay I wrote a couple years ago \nI advised graduating seniors\nto work for a couple years for another company before starting their\nown.  I'd modify that now.  Work for another company if you want\nto, but only for a small one, and if you want to start your own\nstartup, go ahead.The reason I suggested college graduates not start startups immediately\nwas that I felt most would fail.  And they will.  But ambitious\nprogrammers are better off doing their own thing and failing than\ngoing to work at a big company.  Certainly they'll learn more.  They\nmight even be better off financially.  A lot of people in their\nearly twenties get into debt, because their expenses grow even\nfaster than the salary that seemed so high when they left school.\nAt least if you start a startup and fail your net worth will be\nzero rather than negative.  \n[3]We've now funded so many different types of founders that we have\nenough data to see patterns, and there seems to be no benefit from\nworking for a big company.  The people who've worked for a few years\ndo seem better than the ones straight out of college, but only\nbecause they're that much older.The people who come to us from big companies often seem kind of\nconservative.  It's hard to say how much is because big companies\nmade them that way, and how much is the natural conservatism that\nmade them work for the big companies in the first place.  But\ncertainly a large part of it is learned.\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n  I know because I've seen\nit burn off.Having seen that happen so many times is one of the things that\nconvinces me that working for oneself, or at least for a small\ngroup, is the natural way for programmers to live.  Founders arriving\nat Y Combinator often have the downtrodden air of refugees.  Three\nmonths later they're transformed: they have so much more \nconfidence\nthat they seem as if they've grown several inches taller. \n[4]\nStrange as this sounds, they seem both more worried and happier at the same\ntime.  Which is exactly how I'd describe the way lions seem in the\nwild.Watching employees get transformed into founders makes it clear\nthat the difference between the two is due mostly to environment\u2014and\nin particular that the environment in big companies is toxic to\nprogrammers.   In the first couple weeks of working on their own\nstartup they seem to come to life, because finally they're working\nthe way people are meant to.Notes[1]\nWhen I talk about humans being meant or designed to live a\ncertain way, I mean by evolution.[2]\nIt's not only the leaves who suffer.  The constraint propagates\nup as well as down.  So managers are constrained too; instead of\njust doing things, they have to act through subordinates.[3]\nDo not finance your startup with credit cards.  Financing a\nstartup with debt is usually a stupid move, and credit card debt\nstupidest of all.  Credit card debt is a bad idea, period.  It is\na trap set by evil companies for the desperate and the foolish.[4]\nThe founders we fund used to be younger (initially we encouraged\nundergrads to apply), and the first couple times I saw this I used\nto wonder if they were actually getting physically taller.Thanks to Trevor Blackwell, Ross Boucher, Aaron Iba, Abby\nKirigin, Ivan Kirigin, Jessica Livingston, and Robert Morris for\nreading drafts of this.April 2004To the popular press, \"hacker\" means someone who breaks\ninto computers.  Among programmers it means a good programmer.\nBut the two meanings are connected.  To programmers,\n\"hacker\" connotes mastery in the most literal sense: someone\nwho can make a computer do what he wants\u2014whether the computer\nwants to or not.To add to the confusion, the noun \"hack\" also has two senses.  It can\nbe either a compliment or an insult.  It's called a hack when\nyou do something in an ugly way.  But when you do something\nso clever that you somehow beat the system, that's also\ncalled a hack.  The word is used more often in the former than\nthe latter sense, probably because ugly solutions are more\ncommon than brilliant ones.Believe it or not, the two senses of \"hack\" are also\nconnected.  Ugly and imaginative solutions have something in\ncommon: they both break the rules.  And there is a gradual\ncontinuum between rule breaking that's merely ugly (using\nduct tape to attach something to your bike) and rule breaking\nthat is brilliantly imaginative (discarding Euclidean space).Hacking predates computers.  When he\nwas working on the Manhattan Project, Richard Feynman used to\namuse himself by breaking into safes containing secret documents.\nThis tradition continues today.\nWhen we were in grad school, a hacker friend of mine who spent too much\ntime around MIT had\nhis own lock picking kit.\n(He now runs a hedge fund, a not unrelated enterprise.)It is sometimes hard to explain to authorities why one would\nwant to do such things.\nAnother friend of mine once got in trouble with the government for\nbreaking into computers.  This had only recently been declared\na crime, and the FBI found that their usual investigative\ntechnique didn't work.  Police investigation apparently begins with\na motive.  The usual motives are few: drugs, money, sex,\nrevenge.  Intellectual curiosity was not one of the motives on\nthe FBI's list.  Indeed, the whole concept seemed foreign to\nthem.Those in authority tend to be annoyed by hackers'\ngeneral attitude of disobedience.  But that disobedience is\na byproduct of the qualities that make them good programmers.\nThey may laugh at the CEO when he talks in generic corporate\nnewspeech, but they also laugh at someone who tells them\na certain problem can't be solved.\nSuppress one, and you suppress the other.This attitude is sometimes affected.  Sometimes young programmers\nnotice the eccentricities of eminent hackers and decide to\nadopt some of their own in order to seem smarter.\nThe fake version is not merely\nannoying; the prickly attitude of these posers\ncan actually slow the process of innovation.But even factoring in their annoying eccentricities,\nthe disobedient attitude of hackers is a net win.  I wish its\nadvantages were better understood.For example, I suspect people in Hollywood are\nsimply mystified by\nhackers' attitudes toward copyrights.  They are a perennial\ntopic of heated discussion on Slashdot.\nBut why should people who program computers\nbe so concerned about copyrights, of all things?Partly because some companies use mechanisms to prevent\ncopying.  Show any hacker a lock and his first thought is\nhow to pick it.  But there is a deeper reason that\nhackers are alarmed by measures like copyrights and patents.\nThey see increasingly aggressive measures to protect\n\"intellectual property\"\nas a threat to the intellectual\nfreedom they need to do their job.\nAnd they are right.It is by poking about inside current technology that\nhackers get ideas for the next generation.  No thanks,\nintellectual homeowners may say, we don't need any\noutside help.  But they're wrong.\nThe next generation of computer technology has\noften\u2014perhaps more often than not\u2014been developed by outsiders.In 1977 there was no doubt some group within IBM developing\nwhat they expected to be\nthe next generation of business computer.  They were mistaken.\nThe next generation of business computer was\nbeing developed on entirely different lines by two long-haired\nguys called Steve in a garage in Los Altos.  At about the\nsame time, the powers that be\nwere cooperating to develop the\nofficial next generation operating system, Multics.\nBut two guys who thought Multics excessively complex went off\nand wrote their own.  They gave it a name that\nwas a joking reference to Multics: Unix.The latest intellectual property laws impose\nunprecedented restrictions on the sort of poking around that\nleads to new ideas. In the past, a competitor might use patents\nto prevent you from selling a copy of something they\nmade, but they couldn't prevent you from\ntaking one apart to see how it worked.   The latest\nlaws make this a crime.  How are we\nto develop new technology if we can't study current\ntechnology to figure out how to improve it?Ironically, hackers have brought this on themselves.\nComputers are responsible for the problem.  The control systems\ninside machines used to be physical: gears and levers and cams.\nIncreasingly, the brains (and thus the value) of products is\nin software. And by this I mean software in the general sense:\ni.e. data.  A song on an LP is physically stamped into the\nplastic.  A song on an iPod's disk is merely stored on it.Data is by definition easy to copy.  And the Internet\nmakes copies easy to distribute.  So it is no wonder\ncompanies are afraid.  But, as so often happens, fear has\nclouded their judgement.  The government has responded\nwith draconian laws to protect intellectual property.\nThey probably mean well. But\nthey may not realize that such laws will do more harm\nthan good.Why are programmers so violently opposed to these laws?\nIf I were a legislator, I'd be interested in this\nmystery\u2014for the same reason that, if I were a farmer and suddenly\nheard a lot of squawking coming from my hen house one night,\nI'd want to go out and investigate.  Hackers are not stupid,\nand unanimity is very rare in this world.\nSo if they're all squawking,   \nperhaps there is something amiss.Could it be that such laws, though intended to protect America,\nwill actually harm it?  Think about it.  There is something\nvery American about Feynman breaking into safes during\nthe Manhattan Project.  It's hard to imagine the authorities\nhaving a sense of humor about such things over\nin Germany at that time.  Maybe it's not a coincidence.Hackers are unruly.  That is the essence of hacking.  And it\nis also the essence of Americanness.  It is no accident\nthat Silicon Valley\nis in America, and not France, or Germany,\nor England, or Japan. In those countries, people color inside\nthe lines.I lived for a while in Florence.  But after I'd been there\na few months I realized that what I'd been unconsciously hoping\nto find there was back in the place I'd just left.\nThe reason Florence is famous is that in 1450, it was New York.\nIn 1450 it was filled with the kind of turbulent and ambitious\npeople you find now in America.  (So I went back to America.)It is greatly to America's advantage that it is\na congenial atmosphere for the right sort of unruliness\u2014that\nit is a home not just for the smart, but for smart-alecks.\nAnd hackers are invariably smart-alecks.  If we had a national\nholiday, it would be April 1st.  It says a great deal about\nour work that we use the same word for a brilliant or a\nhorribly cheesy solution.   When we cook one up we're not\nalways 100% sure which kind it is.  But as long as it has\nthe right sort of wrongness, that's a promising sign.\nIt's odd that people\nthink of programming as precise and methodical.  Computers\nare precise and methodical.  Hacking is something you do\nwith a gleeful laugh.In our world some of the most characteristic solutions\nare not far removed from practical\njokes.  IBM was no doubt rather surprised by the consequences\nof the licensing deal for DOS, just as the hypothetical\n\"adversary\" must be when Michael Rabin solves a problem by\nredefining it as one that's easier to solve.Smart-alecks have to develop a keen sense of how much they\ncan get away with.  And lately hackers \nhave sensed a change\nin the atmosphere.\nLately hackerliness seems rather frowned upon.To hackers the recent contraction in civil liberties seems\nespecially ominous.  That must also mystify outsiders. \nWhy should we care especially about civil\nliberties?  Why programmers, more than\ndentists or salesmen or landscapers?Let me put the case in terms a government official would appreciate.\nCivil liberties are not just an ornament, or a quaint\nAmerican tradition.  Civil liberties make countries rich.\nIf you made a graph of\nGNP per capita vs. civil liberties, you'd notice a definite\ntrend.  Could civil liberties really be a cause, rather\nthan just an effect?  I think so.  I think a society in which\npeople can do and say what they want will also tend to\nbe one in which the most efficient solutions win, rather than\nthose sponsored by the most influential people.\nAuthoritarian countries become corrupt;\ncorrupt countries become poor; and poor countries are weak. \nIt seems to me there is\na Laffer curve for government power, just as for\ntax revenues.  At least, it seems likely enough that it\nwould be stupid to try the experiment and find out.  Unlike\nhigh tax rates, you can't repeal totalitarianism if it\nturns out to be a mistake.This is why hackers worry.  The government spying on people doesn't\nliterally make programmers write worse code.  It just leads\neventually to a world in which bad ideas win.  And because\nthis is so important to hackers, they're especially sensitive\nto it.  They can sense totalitarianism approaching from a\ndistance, as animals can sense an approaching  \nthunderstorm.It would be ironic if, as hackers fear, recent measures\nintended to protect national security and intellectual property\nturned out to be a missile aimed right at what makes   \nAmerica successful.  But it would not be the first time that\nmeasures taken in an atmosphere of panic had\nthe opposite of the intended effect.There is such a thing as Americanness.\nThere's nothing like living abroad to teach you that.   \nAnd if you want to know whether something will nurture or squash\nthis quality, it would be hard to find a better focus\ngroup than hackers, because they come closest of any group\nI know to embodying it.  Closer, probably,  than\nthe men running our government,\nwho for all their talk of patriotism\nremind me more of Richelieu or Mazarin\nthan Thomas Jefferson or George Washington.When you read what the founding fathers had to say for\nthemselves, they sound more like hackers.\n\"The spirit of resistance to government,\"\nJefferson wrote, \"is so valuable on certain occasions, that I wish\nit always to be kept alive.\"Imagine an American president saying that today.\nLike the remarks of an outspoken old grandmother, the sayings of\nthe founding fathers have embarrassed generations of\ntheir less confident successors.  They remind us where we come from.\nThey remind us that it is the people who break rules that are\nthe source of America's wealth and power.Those in a position to impose rules naturally want them to be\nobeyed.  But be careful what you ask for. You might get it.Thanks to Ken Anderson, Trevor Blackwell, Daniel Giffin, \nSarah Harlin,  Shiro Kawai, Jessica Livingston, Matz, \nJackie McDonough, Robert Morris, Eric Raymond, Guido van Rossum,\nDavid Weinberger, and\nSteven Wolfram for reading drafts of this essay.\n(The image shows Steves Jobs and Wozniak \nwith a \"blue box.\"\nPhoto by Margret Wozniak. Reproduced by permission of Steve\nWozniak.)May 2021Noora Health, a nonprofit I've \nsupported for years, just launched\na new NFT. It has a dramatic name, Save Thousands of Lives,\nbecause that's what the proceeds will do.Noora has been saving lives for 7 years. They run programs in\nhospitals in South Asia to teach new mothers how to take care of\ntheir babies once they get home. They're in 165 hospitals now. And\nbecause they know the numbers before and after they start at a new\nhospital, they can measure the impact they have. It is massive.\nFor every 1000 live births, they save 9 babies.This number comes from a study\nof 133,733 families at 28 different\nhospitals that Noora conducted in collaboration with the Better\nBirth team at Ariadne Labs, a joint center for health systems\ninnovation at Brigham and Women\u0092s Hospital and Harvard T.H. Chan\nSchool of Public Health.Noora is so effective that even if you measure their costs in the\nmost conservative way, by dividing their entire budget by the number\nof lives saved, the cost of saving a life is the lowest I've seen.\n$1,235.For this NFT, they're going to issue a public report tracking how\nthis specific tranche of money is spent, and estimating the number\nof lives saved as a result.NFTs are a new territory, and this way of using them is especially\nnew, but I'm excited about its potential. And I'm excited to see\nwhat happens with this particular auction, because unlike an NFT\nrepresenting something that has already happened,\nthis NFT gets better as the price gets higher.The reserve price was about $2.5 million, because that's what it\ntakes for the name to be accurate: that's what it costs to save\n2000 lives. But the higher the price of this NFT goes, the more\nlives will be saved. What a sentence to be able to write.February 2020What should an essay be? Many people would say persuasive. That's\nwhat a lot of us were taught essays should be. But I think we can\naim for something more ambitious: that an essay should be useful.To start with, that means it should be correct. But it's not enough\nmerely to be correct. It's easy to make a statement correct by\nmaking it vague. That's a common flaw in academic writing, for\nexample. If you know nothing at all about an issue, you can't go\nwrong by saying that the issue is a complex one, that there are\nmany factors to be considered, that it's a mistake to take too\nsimplistic a view of it, and so on.Though no doubt correct, such statements tell the reader nothing.\nUseful writing makes claims that are as strong as they can be made\nwithout becoming false.For example, it's more useful to say that Pike's Peak is near the\nmiddle of Colorado than merely somewhere in Colorado. But if I say\nit's in the exact middle of Colorado, I've now gone too far, because\nit's a bit east of the middle.Precision and correctness are like opposing forces. It's easy to\nsatisfy one if you ignore the other. The converse of vaporous\nacademic writing is the bold, but false, rhetoric of demagogues.\nUseful writing is bold, but true.It's also two other things: it tells people something important,\nand that at least some of them didn't already know.Telling people something they didn't know doesn't always mean\nsurprising them. Sometimes it means telling them something they\nknew unconsciously but had never put into words. In fact those may\nbe the more valuable insights, because they tend to be more\nfundamental.Let's put them all together. Useful writing tells people something\ntrue and important that they didn't already know, and tells them\nas unequivocally as possible.Notice these are all a matter of degree. For example, you can't\nexpect an idea to be novel to everyone. Any insight that you have\nwill probably have already been had by at least one of the world's\n7 billion people. But it's sufficient if an idea is novel to a lot\nof readers.Ditto for correctness, importance, and strength. In effect the four\ncomponents are like numbers you can multiply together to get a score\nfor usefulness. Which I realize is almost awkwardly reductive, but\nnonetheless true._____\nHow can you ensure that the things you say are true and novel and\nimportant? Believe it or not, there is a trick for doing this. I\nlearned it from my friend Robert Morris, who has a horror of saying\nanything dumb. His trick is not to say anything unless he's sure\nit's worth hearing. This makes it hard to get opinions out of him,\nbut when you do, they're usually right.Translated into essay writing, what this means is that if you write\na bad sentence, you don't publish it. You delete it and try again.\nOften you abandon whole branches of four or five paragraphs. Sometimes\na whole essay.You can't ensure that every idea you have is good, but you can\nensure that every one you publish is, by simply not publishing the\nones that aren't.In the sciences, this is called publication bias, and is considered\nbad. When some hypothesis you're exploring gets inconclusive results,\nyou're supposed to tell people about that too. But with essay\nwriting, publication bias is the way to go.My strategy is loose, then tight. I write the first draft of an\nessay fast, trying out all kinds of ideas. Then I spend days rewriting\nit very carefully.I've never tried to count how many times I proofread essays, but\nI'm sure there are sentences I've read 100 times before publishing\nthem. When I proofread an essay, there are usually passages that\nstick out in an annoying way, sometimes because they're clumsily\nwritten, and sometimes because I'm not sure they're true. The\nannoyance starts out unconscious, but after the tenth reading or\nso I'm saying \"Ugh, that part\" each time I hit it. They become like\nbriars that catch your sleeve as you walk past. Usually I won't\npublish an essay till they're all gone \u0097 till I can read through\nthe whole thing without the feeling of anything catching.I'll sometimes let through a sentence that seems clumsy, if I can't\nthink of a way to rephrase it, but I will never knowingly let through\none that doesn't seem correct. You never have to. If a sentence\ndoesn't seem right, all you have to do is ask why it doesn't, and\nyou've usually got the replacement right there in your head.This is where essayists have an advantage over journalists. You\ndon't have a deadline. You can work for as long on an essay as you\nneed to get it right. You don't have to publish the essay at all,\nif you can't get it right. Mistakes seem to lose courage in the\nface of an enemy with unlimited resources. Or that's what it feels\nlike. What's really going on is that you have different expectations\nfor yourself. You're like a parent saying to a child \"we can sit\nhere all night till you eat your vegetables.\" Except you're the\nchild too.I'm not saying no mistake gets through. For example, I added condition\n(c) in \"A Way to Detect Bias\" \nafter readers pointed out that I'd\nomitted it. But in practice you can catch nearly all of them.There's a trick for getting importance too. It's like the trick I\nsuggest to young founders for getting startup ideas: to make something\nyou yourself want. You can use yourself as a proxy for the reader.\nThe reader is not completely unlike you, so if you write about\ntopics that seem important to you, they'll probably seem important\nto a significant number of readers as well.Importance has two factors. It's the number of people something\nmatters to, times how much it matters to them. Which means of course\nthat it's not a rectangle, but a sort of ragged comb, like a Riemann\nsum.The way to get novelty is to write about topics you've thought about\na lot. Then you can use yourself as a proxy for the reader in this\ndepartment too. Anything you notice that surprises you, who've\nthought about the topic a lot, will probably also surprise a\nsignificant number of readers. And here, as with correctness and\nimportance, you can use the Morris technique to ensure that you\nwill. If you don't learn anything from writing an essay, don't\npublish it.You need humility to measure novelty, because acknowledging the\nnovelty of an idea means acknowledging your previous ignorance of\nit. Confidence and humility are often seen as opposites, but in\nthis case, as in many others, confidence helps you to be humble.\nIf you know you're an expert on some topic, you can freely admit\nwhen you learn something you didn't know, because you can be confident\nthat most other people wouldn't know it either.The fourth component of useful writing, strength, comes from two\nthings: thinking well, and the skillful use of qualification. These\ntwo counterbalance each other, like the accelerator and clutch in\na car with a manual transmission. As you try to refine the expression\nof an idea, you adjust the qualification accordingly. Something\nyou're sure of, you can state baldly with no qualification at all,\nas I did the four components of useful writing. Whereas points that\nseem dubious have to be held at arm's length with perhapses.As you refine an idea, you're pushing in the direction of less\nqualification. But you can rarely get it down to zero. Sometimes\nyou don't even want to, if it's a side point and a fully refined\nversion would be too long.Some say that qualifications weaken writing. For example, that you\nshould never begin a sentence in an essay with \"I think,\" because\nif you're saying it, then of course you think it. And it's true\nthat \"I think x\" is a weaker statement than simply \"x.\" Which is\nexactly why you need \"I think.\" You need it to express your degree\nof certainty.But qualifications are not scalars. They're not just experimental\nerror. There must be 50 things they can express: how broadly something\napplies, how you know it, how happy you are it's so, even how it\ncould be falsified. I'm not going to try to explore the structure\nof qualification here. It's probably more complex than the whole\ntopic of writing usefully. Instead I'll just give you a practical\ntip: Don't underestimate qualification. It's an important skill in\nits own right, not just a sort of tax you have to pay in order to\navoid saying things that are false. So learn and use its full range.\nIt may not be fully half of having good ideas, but it's part of\nhaving them.There's one other quality I aim for in essays: to say things as\nsimply as possible. But I don't think this is a component of\nusefulness. It's more a matter of consideration for the reader. And\nit's a practical aid in getting things right; a mistake is more\nobvious when expressed in simple language. But I'll admit that the\nmain reason I write simply is not for the reader's sake or because\nit helps get things right, but because it bothers me to use more\nor fancier words than I need to. It seems inelegant, like a program\nthat's too long.I realize florid writing works for some people. But unless you're\nsure you're one of them, the best advice is to write as simply as\nyou can._____\nI believe the formula I've given you, importance + novelty +\ncorrectness + strength, is the recipe for a good essay. But I should\nwarn you that it's also a recipe for making people mad.The root of the problem is novelty. When you tell people something\nthey didn't know, they don't always thank you for it. Sometimes the\nreason people don't know something is because they don't want to\nknow it. Usually because it contradicts some cherished belief. And\nindeed, if you're looking for novel ideas, popular but mistaken\nbeliefs are a good place to find them. Every popular mistaken belief\ncreates a dead zone of ideas around \nit that are relatively unexplored because they contradict it.The strength component just makes things worse. If there's anything\nthat annoys people more than having their cherished assumptions\ncontradicted, it's having them flatly contradicted.Plus if you've used the Morris technique, your writing will seem\nquite confident. Perhaps offensively confident, to people who\ndisagree with you. The reason you'll seem confident is that you are\nconfident: you've cheated, by only publishing the things you're\nsure of.  It will seem to people who try to disagree with you that\nyou never admit you're wrong. In fact you constantly admit you're\nwrong. You just do it before publishing instead of after.And if your writing is as simple as possible, that just makes things\nworse. Brevity is the diction of command. If you watch someone\ndelivering unwelcome news from a position of inferiority, you'll\nnotice they tend to use lots of words, to soften the blow. Whereas\nto be short with someone is more or less to be rude to them.It can sometimes work to deliberately phrase statements more weakly\nthan you mean. To put \"perhaps\" in front of something you're actually\nquite sure of. But you'll notice that when writers do this, they\nusually do it with a wink.I don't like to do this too much. It's cheesy to adopt an ironic\ntone for a whole essay. I think we just have to face the fact that\nelegance and curtness are two names for the same thing.You might think that if you work sufficiently hard to ensure that\nan essay is correct, it will be invulnerable to attack. That's sort\nof true. It will be invulnerable to valid attacks. But in practice\nthat's little consolation.In fact, the strength component of useful writing will make you\nparticularly vulnerable to misrepresentation. If you've stated an\nidea as strongly as you could without making it false, all anyone\nhas to do is to exaggerate slightly what you said, and now it is\nfalse.Much of the time they're not even doing it deliberately. One of the\nmost surprising things you'll discover, if you start writing essays,\nis that people who disagree with you rarely disagree with what\nyou've actually written. Instead they make up something you said\nand disagree with that.For what it's worth, the countermove is to ask someone who does\nthis to quote a specific sentence or passage you wrote that they\nbelieve is false, and explain why. I say \"for what it's worth\"\nbecause they never do. So although it might seem that this could\nget a broken discussion back on track, the truth is that it was\nnever on track in the first place.Should you explicitly forestall likely misinterpretations? Yes, if\nthey're misinterpretations a reasonably smart and well-intentioned\nperson might make. In fact it's sometimes better to say something\nslightly misleading and then add the correction than to try to get\nan idea right in one shot. That can be more efficient, and can also\nmodel the way such an idea would be discovered.But I don't think you should explicitly forestall intentional\nmisinterpretations in the body of an essay. An essay is a place to\nmeet honest readers. You don't want to spoil your house by putting\nbars on the windows to protect against dishonest ones. The place\nto protect against intentional misinterpretations is in end-notes.\nBut don't think you can predict them all. People are as ingenious\nat misrepresenting you when you say something they don't want to\nhear as they are at coming up with rationalizations for things they\nwant to do but know they shouldn't. I suspect it's the same skill._____\nAs with most other things, the way to get better at writing essays\nis to practice. But how do you start? Now that we've examined the\nstructure of useful writing, we can rephrase that question more\nprecisely. Which constraint do you relax initially? The answer is,\nthe first component of importance: the number of people who care\nabout what you write.If you narrow the topic sufficiently, you can probably find something\nyou're an expert on. Write about that to start with. If you only\nhave ten readers who care, that's fine. You're helping them, and\nyou're writing. Later you can expand the breadth of topics you write\nabout.The other constraint you can relax is a little surprising: publication.\nWriting essays doesn't have to mean publishing them. That may seem\nstrange now that the trend is to publish every random thought, but\nit worked for me. I wrote what amounted to essays in notebooks for\nabout 15 years. I never published any of them and never expected\nto. I wrote them as a way of figuring things out. But when the web\ncame along I'd had a lot of practice.Incidentally, \nSteve \nWozniak did the same thing. In high school he\ndesigned computers on paper for fun. He couldn't build them because\nhe couldn't afford the components. But when Intel launched 4K DRAMs\nin 1975, he was ready._____\nHow many essays are there left to write though? The answer to that\nquestion is probably the most exciting thing I've learned about\nessay writing. Nearly all of them are left to write.Although the essay \nis an old form, it hasn't been assiduously\ncultivated. In the print era, publication was expensive, and there\nwasn't enough demand for essays to publish that many. You could\npublish essays if you were already well known for writing something\nelse, like novels. Or you could write book reviews that you took\nover to express your own ideas. But there was not really a direct\npath to becoming an essayist. Which meant few essays got written,\nand those that did tended to be about a narrow range of subjects.Now, thanks to the internet, there's a path. Anyone can publish\nessays online. You start in obscurity, perhaps, but at least you\ncan start. You don't need anyone's permission.It sometimes happens that an area of knowledge sits quietly for\nyears, till some change makes it explode. Cryptography did this to\nnumber theory. The internet is doing it to the essay.The exciting thing is not that there's a lot left to write, but\nthat there's a lot left to discover. There's a certain kind of idea\nthat's best discovered by writing essays. If most essays are still\nunwritten, most such ideas are still undiscovered.Notes[1] Put railings on the balconies, but don't put bars on the windows.[2] Even now I sometimes write essays that are not meant for\npublication. I wrote several to figure out what Y Combinator should\ndo, and they were really helpful.Thanks to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and\nRobert Morris for reading drafts of this.December 2014If the world were static, we could have monotonically increasing\nconfidence in our beliefs.  The more (and more varied) experience\na belief survived, the less likely it would be false.  Most people\nimplicitly believe something like this about their opinions.  And\nthey're justified in doing so with opinions about things that don't\nchange much, like human nature.  But you can't trust your opinions\nin the same way about things that change, which could include\npractically everything else.When experts are wrong, it's often because they're experts on an\nearlier version of the world.Is it possible to avoid that?  Can you protect yourself against\nobsolete beliefs?  To some extent, yes. I spent almost a decade\ninvesting in early stage startups, and curiously enough protecting\nyourself against obsolete beliefs is exactly what you have to do\nto succeed as a startup investor.  Most really good startup ideas\nlook like bad ideas at first, and many of those look bad specifically\nbecause some change in the world just switched them from bad to\ngood.  I spent a lot of time learning to recognize such ideas, and\nthe techniques I used may be applicable to ideas in general.The first step is to have an explicit belief in change.  People who\nfall victim to a monotonically increasing confidence in their\nopinions are implicitly concluding the world is static.  If you\nconsciously remind yourself it isn't, you start to look for change.Where should one look for it?  Beyond the moderately useful\ngeneralization that human nature doesn't change much, the unfortunate\nfact is that change is hard to predict.  This is largely a tautology\nbut worth remembering all the same: change that matters usually\ncomes from an unforeseen quarter.So I don't even try to predict it.  When I get asked in interviews\nto predict the future, I always have to struggle to come up with\nsomething plausible-sounding on the fly, like a student who hasn't\nprepared for an exam.\n[1]\nBut it's not out of laziness that I haven't\nprepared.  It seems to me that beliefs about the future are so\nrarely correct that they usually aren't worth the extra rigidity\nthey impose, and that the best strategy is simply to be aggressively\nopen-minded.  Instead of trying to point yourself in the right\ndirection, admit you have no idea what the right direction is, and\ntry instead to be super sensitive to the winds of change.It's ok to have working hypotheses, even though they may constrain\nyou a bit, because they also motivate you.  It's exciting to chase\nthings and exciting to try to guess answers.  But you have to be\ndisciplined about not letting your hypotheses harden into anything\nmore.\n[2]I believe this passive m.o. works not just for evaluating new ideas\nbut also for having them.  The way to come up with new ideas is not\nto try explicitly to, but to try to solve problems and simply not\ndiscount weird hunches you have in the process.The winds of change originate in the unconscious minds of domain\nexperts.  If you're sufficiently expert in a field, any weird idea\nor apparently irrelevant question that occurs to you is ipso facto\nworth exploring. \n[3]\n Within Y Combinator, when an idea is described\nas crazy, it's a compliment\u2014in fact, on average probably a\nhigher compliment than when an idea is described as good.Startup investors have extraordinary incentives for correcting\nobsolete beliefs.  If they can realize before other investors that\nsome apparently unpromising startup isn't, they can make a huge\namount of money.  But the incentives are more than just financial.\nInvestors' opinions are explicitly tested: startups come to them\nand they have to say yes or no, and then, fairly quickly, they learn\nwhether they guessed right.  The investors who say no to a Google\n(and there were several) will remember it for the rest of their\nlives.Anyone who must in some sense bet on ideas rather than merely\ncommenting on them has similar incentives.  Which means anyone who\nwants such incentives can have them, by turning their comments into\nbets: if you write about a topic in some fairly durable and public\nform, you'll find you worry much more about getting things right\nthan most people would in a casual conversation.\n[4]Another trick I've found to protect myself against obsolete beliefs\nis to focus initially on people rather than ideas. Though the nature\nof future discoveries is hard to predict, I've found I can predict\nquite well what sort of people will make them.  Good new ideas come\nfrom earnest, energetic, independent-minded people.Betting on people over ideas saved me countless times as an investor.\nWe thought Airbnb was a bad idea, for example. But we could tell\nthe founders were earnest, energetic, and independent-minded.\n(Indeed, almost pathologically so.)  So we suspended disbelief and\nfunded them.This too seems a technique that should be generally applicable.\nSurround yourself with the sort of people new ideas come from.  If\nyou want to notice quickly when your beliefs become obsolete, you\ncan't do better than to be friends with the people whose discoveries\nwill make them so.It's hard enough already not to become the prisoner of your own\nexpertise, but it will only get harder, because change is accelerating.\nThat's not a recent trend; change has been accelerating since the\npaleolithic era.  Ideas beget ideas.  I don't expect that to change.\nBut I could be wrong.\nNotes[1]\nMy usual trick is to talk about aspects of the present that\nmost people haven't noticed yet.[2]\nEspecially if they become well enough known that people start\nto identify them with you.  You have to be extra skeptical about\nthings you want to believe, and once a hypothesis starts to be\nidentified with you, it will almost certainly start to be in that\ncategory.[3]\nIn practice \"sufficiently expert\" doesn't require one to be\nrecognized as an expert\u2014which is a trailing indicator in any\ncase.  In many fields a year of focused work plus caring a lot would\nbe enough.[4]\nThough they are public and persist indefinitely, comments on\ne.g. forums and places like Twitter seem empirically to work like\ncasual conversation.  The threshold may be whether what you write\nhas a title.\nThanks to Sam Altman, Patrick Collison, and Robert Morris\nfor reading drafts of this.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nNovember 2005Does \"Web 2.0\" mean anything?  Till recently I thought it didn't,\nbut the truth turns out to be more complicated.  Originally, yes,\nit was meaningless.  Now it seems to have acquired a meaning.  And\nyet those who dislike the term are probably right, because if it\nmeans what I think it does, we don't need it.I first heard the phrase \"Web 2.0\" in the name of the Web 2.0\nconference in 2004.  At the time it was supposed to mean using \"the\nweb as a platform,\" which I took to refer to web-based applications.\n[1]So I was surprised at a conference this summer when Tim O'Reilly\nled a session intended to figure out a definition of \"Web 2.0.\"\nDidn't it already mean using the web as a platform?  And if it\ndidn't already mean something, why did we need the phrase at all?OriginsTim says the phrase \"Web 2.0\" first\narose in \"a brainstorming session between\nO'Reilly and Medialive International.\" What is Medialive International?\n\"Producers of technology tradeshows and conferences,\" according to\ntheir site.  So presumably that's what this brainstorming session\nwas about.  O'Reilly wanted to organize a conference about the web,\nand they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a\nnew version of the web.  They just wanted to make the point\nthat the web mattered again.  It was a kind of semantic deficit\nspending: they knew new things were coming, and the \"2.0\" referred\nto whatever those might turn out to be.And they were right.  New things were coming.  But the new version\nnumber led to some awkwardness in the short term.  In the process\nof developing the pitch for the first conference, someone must have\ndecided they'd better take a stab at explaining what that \"2.0\"\nreferred to.  Whatever it meant, \"the web as a platform\" was at\nleast not too constricting.The story about \"Web 2.0\" meaning the web as a platform didn't live\nmuch past the first conference.  By the second conference, what\n\"Web 2.0\" seemed to mean was something about democracy.  At least,\nit did when people wrote about it online.  The conference itself\ndidn't seem very grassroots.  It cost $2800, so the only people who\ncould afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article\nabout the conference in Wired News spoke of \"throngs of\ngeeks.\"  When a friend of mine asked Ryan about this, it was news\nto him.  He said he'd originally written something like \"throngs\nof VCs and biz dev guys\" but had later shortened it just to \"throngs,\"\nand that this must have in turn been expanded by the editors into\n\"throngs of geeks.\"  After all, a Web 2.0 conference would presumably\nbe full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a   \nsuit, a sight so alien I couldn't parse it at first.  I saw\nhim walk by and said to one of the O'Reilly people \"that guy looks\njust like Tim.\"\"Oh, that's Tim.  He bought a suit.\"\nI ran after him, and sure enough, it was.  He explained that he'd\njust bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows\nduring the Bubble, full of prowling VCs looking for the next hot\nstartup.  There was that same odd atmosphere created by a large  \nnumber of people determined not to miss out.  Miss out on what?\nThey didn't know.  Whatever was going to happen\u2014whatever Web 2.0\nturned out to be.I wouldn't quite call it \"Bubble 2.0\" just because VCs are eager\nto invest again.  The Internet is a genuinely big deal.  The bust\nwas as much an overreaction as\nthe boom.  It's to be expected that once we started to pull out of\nthe bust, there would be a lot of growth in this area, just as there\nwas in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO\nmarket is gone.  Venture investors\nare driven by exit strategies.  The reason they were funding all  \nthose laughable startups during the late 90s was that they hoped\nto sell them to gullible retail investors; they hoped to be laughing\nall the way to the bank.  Now that route is closed.  Now the default\nexit strategy is to get bought, and acquirers are less prone to\nirrational exuberance than IPO investors.  The closest you'll get \nto Bubble valuations is Rupert Murdoch paying $580 million for   \nMyspace.  That's only off by a factor of 10 or so.1. AjaxDoes \"Web 2.0\" mean anything more than the name of a conference\nyet?  I don't like to admit it, but it's starting to.  When people\nsay \"Web 2.0\" now, I have some idea what they mean.  And the fact\nthat I both despise the phrase and understand it is the surest proof\nthat it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still\nonly just bear to use without scare quotes.  Basically, what \"Ajax\"\nmeans is \"Javascript now works.\"  And that in turn means that\nweb-based applications can now be made to work much more like desktop\nones.As you read this, a whole new generation\nof software is being written to take advantage of Ajax.  There\nhasn't been such a wave of new applications since microcomputers\nfirst appeared.  Even Microsoft sees it, but it's too late for them\nto do anything more than leak \"internal\"  \ndocuments designed to give the impression they're on top of this\nnew trend.In fact the new generation of software is being written way too\nfast for Microsoft even to channel it, let alone write their own\nin house.  Their only hope now is to buy all the best Ajax startups\nbefore Google does.  And even that's going to be hard, because\nGoogle has as big a head start in buying microstartups as it did\nin search a few years ago.  After all, Google Maps, the canonical\nAjax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference\nturned out to be partially right: web-based applications are a big\ncomponent of Web 2.0.  But I'm convinced they got this right by \naccident.  The Ajax boom didn't start till early 2005, when Google\nMaps appeared and the term \"Ajax\" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several\nexamples to prove that amateurs can   \nsurpass professionals, when they have the right kind of system to \nchannel their efforts.  Wikipedia\nmay be the most famous.  Experts have given Wikipedia middling\nreviews, but they miss the critical point: it's good enough.  And   \nit's free, which means people actually read it.  On the web, articles\nyou have to pay for might as well not exist.  Even if you were    \nwilling to pay to read them yourself, you can't link to them.    \nThey're not part of the conversation.Another place democracy seems to win is in deciding what counts as\nnews.  I never look at any news site now except Reddit.\n[2]\n I know if something major\nhappens, or someone writes a particularly interesting article, it   \nwill show up there.  Why bother checking the front page of any\nspecific paper or magazine?  Reddit's like an RSS feed for the whole\nweb, with a filter for quality.  Similar sites include Digg, a technology news site that's\nrapidly approaching Slashdot in popularity, and del.icio.us, the collaborative\nbookmarking network that set off the \"tagging\" movement.  And whereas\nWikipedia's main appeal is that it's good enough and free, these\nsites suggest that voters do a significantly better job than human\neditors.The most dramatic example of Web 2.0 democracy is not in the selection\nof ideas, but their production.  \nI've noticed for a while that the stuff I read on individual people's\nsites is as good as or better than the stuff I read in newspapers\nand magazines.  And now I have independent evidence: the top links\non Reddit are generally links to individual people's sites rather  \nthan to magazine articles or news stories.My experience of writing\nfor magazines suggests an explanation.  Editors.  They control the\ntopics you can write about, and they can generally rewrite whatever\nyou produce.  The result is to damp extremes.  Editing yields 95th\npercentile writing\u201495% of articles are improved by it, but 5% are\ndragged down.  5% of the time you get \"throngs of geeks.\"On the web, people can publish whatever they want.  Nearly all of\nit falls short of the editor-damped writing in print publications.\nBut the pool of writers is very, very large.  If it's large enough,\nthe lack of damping means the best writing online should surpass  \nthe best in print.\n[3]  \nAnd now that the web has evolved mechanisms\nfor selecting good stuff, the web wins net.  Selection beats damping,\nfor the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the  \nstartups of the Bubble what bloggers are to the print media.  During\nthe Bubble, a startup meant a company headed by an MBA that was   \nblowing through several million dollars of VC money to \"get big\nfast\" in the most literal sense.  Now it means a smaller, younger, more technical group that just      \ndecided to make something great.  They'll decide later if they want  \nto raise VC-scale funding, and if they take it, they'll take it on\ntheir terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements\nof \"Web 2.0.\"  I also see a third: not to maltreat users.  During\nthe Bubble a lot of popular sites were quite high-handed with users.\nAnd not just in obvious ways, like making them register, or subjecting\nthem to annoying ads.  The very design of the average site in the   \nlate 90s was an abuse.  Many of the most popular sites were loaded\nwith obtrusive branding that made them slow to load and sent the\nuser the message: this is our site, not yours.  (There's a physical\nanalog in the Intel and Microsoft stickers that come on some\nlaptops.)I think the root of the problem was that sites felt they were giving\nsomething away for free, and till recently a company giving anything\naway for free could be pretty high-handed about it.  Sometimes it\nreached the point of economic sadism: site owners assumed that the\nmore pain they caused the user, the more benefit it must be to them.  \nThe most dramatic remnant of this model may be at salon.com, where   \nyou can read the beginning of a story, but to get the rest you have\nsit through a movie.At Y Combinator we advise all the startups we fund never to lord\nit over users.  Never make users register, unless you need to in\norder to store something for them.  If you do make users register,   \nnever make them wait for a confirmation link in an email; in fact,\ndon't even ask for their email address unless you need it for some\nreason.  Don't ask them any unnecessary questions.  Never send them\nemail unless they explicitly ask for it.  Never frame pages you\nlink to, or open them in new windows.  If you have a free version \nand a pay version, don't make the free version too restricted.  And\nif you find yourself asking \"should we allow users to do x?\" just \nanswer \"yes\" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups\nnever to let anyone fly under them, meaning never to let any other\ncompany offer a cheaper, easier solution.  Another way to fly low \nis to give users more power.  Let users do what they want.  If you \ndon't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual\nsongs instead of having to buy whole albums.  The recording industry\nhated the idea and resisted it as long as possible.  But it was\nobvious what users wanted, so Apple flew under the labels.\n[4]\nThough really it might be better to describe iTunes as Web 1.5.     \nWeb 2.0 applied to music would probably mean individual bands giving\naway DRMless songs for free.The ultimate way to be nice to users is to give them something for\nfree that competitors charge for.  During the 90s a lot of people   \nprobably thought we'd have some working system for micropayments     \nby now.  In fact things have gone in the other direction.  The most   \nsuccessful sites are the ones that figure out new ways to give stuff\naway for free.  Craigslist has largely destroyed the classified ad\nsites of the 90s, and OkCupid looks likely to do the same to the\nprevious generation of dating sites.Serving web pages is very, very cheap.  If you can make even a   \nfraction of a cent per page view, you can make a profit.  And\ntechnology for targeting ads continues to improve.  I wouldn't be\nsurprised if ten years from now eBay had been supplanted by an      \nad-supported freeBay (or, more likely, gBay).Odd as it might sound, we tell startups that they should try to\nmake as little money as possible.  If you can figure out a way to\nturn a billion dollar industry into a fifty million dollar industry,\nso much the better, if all fifty million go to you.  Though indeed,\nmaking things cheaper often turns out to generate more money in the\nend, just as automating things often turns out to generate more\njobs.The ultimate target is Microsoft.  What a bang that balloon is going\nto make when someone pops it by offering a free web-based alternative \nto MS Office.\n[5]\nWho will?  Google?  They seem to be taking their\ntime.  I suspect the pin will be wielded by a couple of 20 year old\nhackers who are too naive to be intimidated by the idea.  (How hard\ncan it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in  \ncommon?  I didn't realize they had anything in common till recently,\nwhich is one of the reasons I disliked the term \"Web 2.0\" so much.\nIt seemed that it was being used as a label for whatever happened\nto be new\u2014that it didn't predict anything.But there is a common thread.  Web 2.0 means using the web the way\nit's meant to be used.  The \"trends\" we're seeing now are simply\nthe inherent nature of the web emerging from under the broken models\nthat got imposed on it during the Bubble.I realized this when I read an  interview with\nJoe Kraus, the co-founder of Excite.\n[6]\n\n  Excite really never got the business model right at all.  We fell \n  into the classic problem of how when a new medium comes out it\n  adopts the practices, the content, the business models of the old\n  medium\u2014which fails, and then the more appropriate models get\n  figured out.\n\nIt may have seemed as if not much was happening during the years\nafter the Bubble burst.  But in retrospect, something was happening:\nthe web was finding its natural angle of repose.  The democracy \ncomponent, for example\u2014that's not an innovation, in the sense of\nsomething someone made happen.  That's what the web naturally tends\nto produce.Ditto for the idea of delivering desktop-like applications over the\nweb.  That idea is almost as old as the web.  But the first time    \naround it was co-opted by Sun, and we got Java applets.  Java has\nsince been remade into a generic replacement for C++, but in 1996\nthe story about Java was that it represented a new model of software.\nInstead of desktop applications, you'd run Java \"applets\" delivered\nfrom a server.This plan collapsed under its own weight. Microsoft helped kill it,\nbut it would have died anyway.  There was no uptake among hackers.\nWhen you find PR firms promoting\nsomething as the next development platform, you can be sure it's\nnot.  If it were, you wouldn't need PR firms to tell you, because   \nhackers would already be writing stuff on top of it, the way sites    \nlike Busmonster used Google Maps as a\nplatform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of  \nhackers have spontaneously started building things on top\nof it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common.\nHere's a clue.  Suppose you approached investors with the following\nidea for a Web 2.0 startup:\n\n  Sites like del.icio.us and flickr allow users to \"tag\" content\n  with descriptive tokens.  But there is also huge source of\n  implicit tags that they ignore: the text within web links.\n  Moreover, these links represent a social network connecting the   \n  individuals and organizations who created the pages, and by using\n  graph theory we can compute from this network an estimate of the\n  reputation of each member.  We plan to mine the web for these \n  implicit tags, and use them together with the reputation hierarchy\n  they embody to enhance web searches.\n\nHow long do you think it would take them on average to realize that\nit was a description of Google?Google was a pioneer in all three components of Web 2.0: their core\nbusiness sounds crushingly hip when described in Web 2.0 terms, \n\"Don't maltreat users\" is a subset of \"Don't be evil,\" and of course\nGoogle set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google\ndoes.  That's their secret.    They're sailing with the wind, instead of sitting  \nbecalmed praying for a business model, like the print media, or   \ntrying to tack upwind by suing their customers, like Microsoft and \nthe record labels.\n[7]Google doesn't try to force things to happen their way.  They try   \nto figure out what's going to happen, and arrange to be standing \nthere when it does.  That's the way to approach technology\u2014and \nas business includes an ever larger technological component, the\nright way to do business.The fact that Google is a \"Web 2.0\" company shows that, while\nmeaningful, the term is also rather bogus.  It's like the word\n\"allopathic.\"  It just means doing things right, and it's a bad   \nsign when you have a special word for that.\nNotes[1]\nFrom the conference\nsite, June 2004: \"While the first wave of the Web was closely  \ntied to the browser, the second wave extends applications across    \nthe web and enables a new generation of services and business\nopportunities.\"  To the extent this means anything, it seems to be\nabout \nweb-based applications.[2]\nDisclosure: Reddit was funded by \nY Combinator.  But although\nI started using it out of loyalty to the home team, I've become a\ngenuine addict.  While we're at it, I'm also an investor in\n!MSFT, having sold all my shares earlier this year.[3]\nI'm not against editing. I spend more time editing than\nwriting, and I have a group of picky friends who proofread almost\neverything I write.  What I dislike is editing done after the fact  \nby someone else.[4]\nObvious is an understatement.  Users had been climbing in through  \nthe window for years before Apple finally moved the door.[5]\nHint: the way to create a web-based alternative to Office may\nnot be to write every component yourself, but to establish a protocol\nfor web-based apps to share a virtual home directory spread across\nmultiple servers.  Or it may be to write it all yourself.[6]\nIn Jessica Livingston's\nFounders at\nWork.[7]\nMicrosoft didn't sue their customers directly, but they seem \nto have done all they could to help SCO sue them.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, Peter\nNorvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the\nguys at O'Reilly and Adaptive Path for answering my questions.April 2012A palliative care nurse called Bronnie Ware made a list of the\nbiggest regrets\nof the dying.  Her list seems plausible.  I could see\nmyself \u2014 can see myself \u2014 making at least 4 of these\n5 mistakes.If you had to compress them into a single piece of advice, it might\nbe: don't be a cog.  The 5 regrets paint a portrait of post-industrial\nman, who shrinks himself into a shape that fits his circumstances,\nthen turns dutifully till he stops.The alarming thing is, the mistakes that produce these regrets are\nall errors of omission.  You forget your dreams, ignore your family,\nsuppress your feelings, neglect your friends, and forget to be\nhappy.  Errors of omission are a particularly dangerous type of\nmistake, because you make them by default.I would like to avoid making these mistakes.  But how do you avoid\nmistakes you make by default?  Ideally you transform your life so\nit has other defaults.  But it may not be possible to do that\ncompletely. As long as these mistakes happen by default, you probably\nhave to be reminded not to make them.  So I inverted the 5 regrets,\nyielding a list of 5 commands\n\n   Don't ignore your dreams; don't work too much; say what you\n   think; cultivate friendships; be happy.\n\nwhich I then put at the top of the file I use as a todo list.December 2014I've read Villehardouin's chronicle of the Fourth Crusade at least\ntwo times, maybe three.  And yet if I had to write down everything\nI remember from it, I doubt it would amount to much more than a\npage.  Multiply this times several hundred, and I get an uneasy\nfeeling when I look at my bookshelves. What use is it to read all\nthese books if I remember so little from them?A few months ago, as I was reading Constance Reid's excellent\nbiography of Hilbert, I figured out if not the answer to this\nquestion, at least something that made me feel better about it.\nShe writes:\n\n  Hilbert had no patience with mathematical lectures which filled\n  the students with facts but did not teach them how to frame a\n  problem and solve it. He often used to tell them that \"a perfect\n  formulation of a problem is already half its solution.\"\n\nThat has always seemed to me an important point, and I was even\nmore convinced of it after hearing it confirmed by Hilbert.But how had I come to believe in this idea in the first place?  A\ncombination of my own experience and other things I'd read.  None\nof which I could at that moment remember!  And eventually I'd forget\nthat Hilbert had confirmed it too.  But my increased belief in the\nimportance of this idea would remain something I'd learned from\nthis book, even after I'd forgotten I'd learned it.Reading and experience train your model of the world.  And even if\nyou forget the experience or what you read, its effect on your model\nof the world persists.  Your mind is like a compiled program you've\nlost the source of.  It works, but you don't know why.The place to look for what I learned from Villehardouin's chronicle\nis not what I remember from it, but my mental models of the crusades,\nVenice, medieval culture, siege warfare, and so on.  Which doesn't\nmean I couldn't have read more attentively, but at least the harvest\nof reading is not so miserably small as it might seem.This is one of those things that seem obvious in retrospect.  But\nit was a surprise to me and presumably would be to anyone else who\nfelt uneasy about (apparently) forgetting so much they'd read.Realizing it does more than make you feel a little better about\nforgetting, though.  There are specific implications.For example, reading and experience are usually \"compiled\" at the\ntime they happen, using the state of your brain at that time.  The\nsame book would get compiled differently at different points in\nyour life.  Which means it is very much worth reading important\nbooks multiple times.  I always used to feel some misgivings about\nrereading books.  I unconsciously lumped reading together with work\nlike carpentry, where having to do something again is a sign you\ndid it wrong the first time.  Whereas now the phrase \"already read\"\nseems almost ill-formed.Intriguingly, this implication isn't limited to books.  Technology\nwill increasingly make it possible to relive our experiences.  When\npeople do that today it's usually to enjoy them again (e.g. when\nlooking at pictures of a trip) or to find the origin of some bug in\ntheir compiled code (e.g. when Stephen Fry succeeded in remembering\nthe childhood trauma that prevented him from singing).  But as\ntechnologies for recording and playing back your life improve, it\nmay become common for people to relive experiences without any goal\nin mind, simply to learn from them again as one might when rereading\na book.Eventually we may be able not just to play back experiences but\nalso to index and even edit them. So although not knowing how you\nknow things may seem part of being human, it may not be.\nThanks to Sam Altman, Jessica Livingston, and Robert Morris for reading \ndrafts of this.September 2007In high school I decided I was going to study philosophy in college.\nI had several motives, some more honorable than others.  One of the\nless honorable was to shock people.  College was regarded as job\ntraining where I grew up, so studying philosophy seemed an impressively\nimpractical thing to do.  Sort of like slashing holes in your clothes\nor putting a safety pin through your ear, which were other forms\nof impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying\nphilosophy would be a shortcut straight to wisdom.  All the people\nmajoring in other things would just end up with a bunch of domain\nknowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you\nwouldn't find those in our high school library.  But I tried to\nread Plato and Aristotle.  I doubt I believed I understood them,\nbut they sounded like they were talking about something important.\nI assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned\na lot in the calculus class, but I didn't learn much in Philosophy\n101.  And yet my plan to study philosophy remained intact.  It was\nmy fault I hadn't learned anything.  I hadn't read the books we\nwere assigned carefully enough.  I'd give Berkeley's Principles\nof Human Knowledge another shot in college.  Anything so admired\nand so difficult to read must have something in it, if one could\nonly figure out what.Twenty-six years later, I still don't understand Berkeley.  I have\na nice edition of his collected works.  Will I ever read it?  Seems\nunlikely.The difference between then and now is that now I understand why\nBerkeley is probably not worth trying to understand.  I think I see\nnow what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It\ndidn't work out as I'd hoped.  I didn't learn any magical truths\ncompared to which everything else was mere domain knowledge.  But\nI do at least know now why I didn't.  Philosophy doesn't really\nhave a subject matter in the way math or history or most other\nuniversity subjects do.  There is no core of knowledge one must\nmaster.  The closest you come to that is a knowledge of what various\nindividual philosophers have said about different topics over the\nyears.  Few were sufficiently correct that people have forgotten\nwho discovered what they discovered.Formal logic has some subject matter. I took several classes in\nlogic.  I don't know if I learned anything from them.\n[1]\nIt does seem to me very important to be able to flip ideas around in\none's head: to see when two ideas don't fully cover the space of\npossibilities, or when one idea is the same as another but with a\ncouple things changed.  But did studying logic teach me the importance\nof thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The\nmost dramatic I learned immediately, in the first semester of\nfreshman year, in a class taught by Sydney Shoemaker.  I learned\nthat I don't exist.  I am (and you are) a collection of cells that\nlurches around driven by various forces, and calls itself I.  But\nthere's no central, indivisible thing that your identity goes with.\nYou could conceivably lose half your brain and live.  Which means\nyour brain could conceivably be split into two halves and each\ntransplanted into different bodies.  Imagine waking up after such\nan operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life\nare fuzzy, and break down if pushed too hard.  Even a concept as\ndear to us as I.  It took me a while to grasp this, but when I\ndid it was fairly sudden, like someone in the nineteenth century\ngrasping evolution and realizing the story of creation they'd been\ntold as a child was all wrong. \n[2]\nOutside of math there's a limit\nto how far you can push words; in fact, it would not be a bad\ndefinition of math to call it the study of terms that have precise\nmeanings.  Everyday words are inherently imprecise.  They work well\nenough in everyday life that you don't notice.  Words seem to work,\njust as Newtonian physics seems to.  But you can always make them\nbreak if you push them far enough.I would say that this has been, unfortunately for philosophy, the\ncentral fact of philosophy.  Most philosophical debates are not\nmerely afflicted by but driven by confusions over words.  Do we\nhave free will?  Depends what you mean by \"free.\" Do abstract ideas\nexist?  Depends what you mean by \"exist.\"Wittgenstein is popularly credited with the idea that most philosophical\ncontroversies are due to confusions over language.  I'm not sure\nhow much credit to give him.  I suspect a lot of people realized\nthis, but reacted simply by not studying philosophy, rather than\nbecoming philosophy professors.How did things get this way?  Can something people have spent\nthousands of years studying really be a waste of time?  Those are\ninteresting questions.  In fact, some of the most interesting\nquestions you can ask about philosophy.  The most valuable way to\napproach the current philosophical tradition may be neither to get\nlost in pointless speculations like Berkeley, nor to shut them down\nlike Wittgenstein, but to study it as an example of reason gone\nwrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle.\nWhat we know of their predecessors comes from fragments and references\nin later works; their doctrines could be described as speculative\ncosmology that occasionally strays into analysis.  Presumably they\nwere driven by whatever makes people in every other society invent\ncosmologies.\n[3]With Socrates, Plato, and particularly Aristotle, this tradition\nturned a corner.  There started to be a lot more analysis.  I suspect\nPlato and Aristotle were encouraged in this by progress in math.\nMathematicians had by then shown that you could figure things out\nin a much more conclusive way than by making up fine sounding stories\nabout them.  \n[4]People talk so much about abstractions now that we don't realize\nwhat a leap it must have been when they first started to.  It was\npresumably many thousands of years between when people first started\ndescribing things as hot or cold and when someone asked \"what is\nheat?\"  No doubt it was a very gradual process.  We don't know if\nPlato or Aristotle were the first to ask any of the questions they\ndid.  But their works are the oldest we have that do this on a large\nscale, and there is a freshness (not to say naivete) about them\nthat suggests some of the questions they asked were new to them,\nat least.Aristotle in particular reminds me of the phenomenon that happens\nwhen people discover something new, and are so excited by it that\nthey race through a huge percentage of the newly discovered territory\nin one lifetime.  If so, that's evidence of how new this kind of\nthinking was. \n[5]This is all to explain how Plato and Aristotle can be very impressive\nand yet naive and mistaken.  It was impressive even to ask the\nquestions they did.  That doesn't mean they always came up with\ngood answers.  It's not considered insulting to say that ancient\nGreek mathematicians were naive in some respects, or at least lacked\nsome concepts that would have made their lives easier.  So I hope\npeople will not be too offended if I propose that ancient philosophers\nwere similarly naive.  In particular, they don't seem to have fully\ngrasped what I earlier called the central fact of philosophy: that\nwords break if you push them too far.\"Much to the surprise of the builders of the first digital computers,\"\nRod Brooks wrote, \"programs written for them usually did not work.\"\n[6]\nSomething similar happened when people first started trying\nto talk about abstractions.  Much to their surprise, they didn't\narrive at answers they agreed upon.  In fact, they rarely seemed\nto arrive at answers at all.They were in effect arguing about artifacts induced by sampling at\ntoo low a resolution.The proof of how useless some of their answers turned out to be is\nhow little effect they have.  No one after reading Aristotle's\nMetaphysics does anything differently as a result.\n[7]Surely I'm not claiming that ideas have to have practical applications\nto be interesting?  No, they may not have to.  Hardy's boast that\nnumber theory had no use whatsoever wouldn't disqualify it.  But\nhe turned out to be mistaken.  In fact, it's suspiciously hard to\nfind a field of math that truly has no practical use.  And Aristotle's\nexplanation of the ultimate goal of philosophy in Book A of the\nMetaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.\nThe examples he gives are convincing: an ordinary worker builds\nthings a certain way out of habit; a master craftsman can do more\nbecause he grasps the underlying principles.  The trend is clear:\nthe more general the knowledge, the more admirable it is.  But then\nhe makes a mistake\u2014possibly the most important mistake in the\nhistory of philosophy.  He has noticed that theoretical knowledge\nis often acquired for its own sake, out of curiosity, rather than\nfor any practical need.  So he proposes there are two kinds of\ntheoretical knowledge: some that's useful in practical matters and\nsome that isn't.  Since people interested in the latter are interested\nin it for its own sake, it must be more noble.  So he sets as his\ngoal in the Metaphysics the exploration of knowledge that has no\npractical use.  Which means no alarms go off when he takes on grand\nbut vaguely understood questions and ends up getting lost in a sea\nof words.His mistake was to confuse motive and result.  Certainly, people\nwho want a deep understanding of something are often driven by\ncuriosity rather than any practical need.  But that doesn't mean\nwhat they end up learning is useless.  It's very valuable in practice\nto have a deep understanding of what you're doing; even if you're\nnever called on to solve advanced problems, you can see shortcuts\nin the solution of simple ones, and your knowledge won't break down\nin edge cases, as it would if you were relying on formulas you\ndidn't understand.  Knowledge is power.  That's what makes theoretical\nknowledge prestigious.  It's also what causes smart people to be\ncurious about certain things and not others; our DNA is not so\ndisinterested as we might think.So while ideas don't have to have immediate practical applications\nto be interesting, the kinds of things we find interesting will\nsurprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was\npartly that he set off with contradictory aims: to explore the most\nabstract ideas, guided by the assumption that they were useless.\nHe was like an explorer looking for a territory to the north of\nhim, starting with the assumption that it was located to the south.And since his work became the map used by generations of future\nexplorers, he sent them off in the wrong direction as well. \n[8]\nPerhaps worst of all, he protected them from both the criticism of\noutsiders and the promptings of their own inner compass by establishing\nthe principle that the most noble sort of theoretical knowledge had\nto be useless.The Metaphysics is mostly a failed experiment.  A few ideas from\nit turned out to be worth keeping; the bulk of it has had no effect\nat all.  The Metaphysics is among the least read of all famous\nbooks.  It's not hard to understand the way Newton's Principia\nis, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately\nthat was not the conclusion Aristotle's successors derived from\nworks like the Metaphysics. \n[9]\nSoon after, the western world\nfell on intellectual hard times.  Instead of version 1s to be\nsuperseded, the works of Plato and Aristotle became revered texts\nto be mastered and discussed.  And so things remained for a shockingly\nlong time.  It was not till around 1600 (in Europe, where the center\nof gravity had shifted by then) that one found people confident\nenough to treat Aristotle's work as a catalog of mistakes.  And\neven then they rarely said so outright.If it seems surprising that the gap was so long, consider how little\nprogress there was in math between Hellenistic times and the\nRenaissance.In the intervening years an unfortunate idea took hold:  that it\nwas not only acceptable to produce works like the Metaphysics,\nbut that it was a particularly prestigious line of work, done by a\nclass of people called philosophers.  No one thought to go back and\ndebug Aristotle's motivating argument.  And so instead of correcting\nthe problem Aristotle discovered by falling into it\u2014that you can\neasily get lost if you talk too loosely about very abstract ideas\u2014they \ncontinued to fall into it.The SingularityCuriously, however, the works they produced continued to attract\nnew readers.  Traditional philosophy occupies a kind of singularity\nin this respect.  If you write in an unclear way about big ideas,\nyou produce something that seems tantalizingly attractive to\ninexperienced but intellectually ambitious students.  Till one knows\nbetter, it's hard to distinguish something that's hard to understand\nbecause the writer was unclear in his own mind from something like\na mathematical proof that's hard to understand because the ideas\nit represents are hard to understand.  To someone who hasn't learned\nthe difference, traditional philosophy seems extremely attractive:\nas hard (and therefore impressive) as math, yet broader in scope.\nThat was what lured me in as a high school student.This singularity is even more singular in having its own defense\nbuilt in.  When things are hard to understand, people who suspect\nthey're nonsense generally keep quiet.  There's no way to prove a\ntext is meaningless.  The closest you can get is to show that the\nofficial judges of some class of texts can't distinguish them from\nplacebos. \n[10]And so instead of denouncing philosophy, most people who suspected\nit was a waste of time just studied other things.  That alone is\nfairly damning evidence, considering philosophy's claims.  It's\nsupposed to be about the ultimate truths. Surely all smart people\nwould be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might\nhave corrected them, they tended to be self-perpetuating.  Bertrand\nRussell wrote in a letter in 1912:\n\n  Hitherto the people attracted to philosophy have been mostly those\n  who loved the big generalizations, which were all wrong, so that\n  few people with exact minds have taken up the subject.\n[11]\n\nHis response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery\nthat most previous philosophy was a waste of time, which judging\nfrom the circumstantial evidence must have been made by every smart\nperson who studied a little philosophy and declined to pursue it\nfurther, but for how he acted in response.\n[12]\nInstead of quietly\nswitching to another field, he made a fuss, from inside.  He was\nGorbachev.The field of philosophy is still shaken from the fright Wittgenstein\ngave it. \n[13]\nLater in life he spent a lot of time talking about\nhow words worked.  Since that seems to be allowed, that's what a\nlot of philosophers do now.  Meanwhile, sensing a vacuum in the\nmetaphysical speculation department, the people who used to do\nliterary criticism have been edging Kantward, under new names like\n\"literary theory,\" \"critical theory,\" and when they're feeling\nambitious, plain \"theory.\"  The writing is the familiar word salad:\n\n  Gender is not like some of the other grammatical modes which\n  express precisely a mode of conception without any reality that\n  corresponds to the conceptual mode, and consequently do not express\n  precisely something in reality by which the intellect could be\n  moved to conceive a thing the way it does, even where that motive\n  is not something in the thing as such.\n  [14]\n\nThe singularity I've described is not going away.  There's a market\nfor writing that sounds impressive and can't be disproven. There\nwill always be both supply and demand.  So if one group abandons\nthis territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility.\nPerhaps we should do what Aristotle meant to do, instead of what\nhe did.  The goal he announces in the Metaphysics seems one worth\npursuing: to discover the most general truths.  That sounds good.\nBut instead of trying to discover them because they're useless,\nlet's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised\ncriterion, applicability, as a guide to keep us from wondering\noff into a swamp of abstractions.  Instead of trying to answer the\nquestion:\n\n  What are the most general truths?\n\nlet's try to answer the question\n\n  Of all the useful things we can say, which are the most general?\n\nThe test of utility I propose is whether we cause people who read\nwhat we've written to do anything differently afterward.  Knowing\nwe have to give definite (if implicit) advice will keep us from\nstraying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a\ndifferent direction.As an example of a useful, general idea, consider that of the\ncontrolled experiment.  There's an idea that has turned out to be\nwidely applicable.  Some might say it's part of science, but it's\nnot part of any specific science; it's literally meta-physics (in\nour sense of \"meta\").   The idea of evolution is another. It turns\nout to have quite broad applications\u2014for example, in genetic\nalgorithms and even product design.  Frankfurt's distinction between\nlying and bullshitting seems a promising recent example.\n[15]These seem to me what philosophy should look like: quite general\nobservations that would cause someone who understood them to do\nsomething differently.Such observations will necessarily be about things that are imprecisely\ndefined.  Once you start using words with precise meanings, you're\ndoing math.  So starting from utility won't entirely solve the\nproblem I described above\u2014it won't flush out the metaphysical\nsingularity.  But it should help.  It gives people with good\nintentions a new roadmap into abstraction.  And they may thereby\nproduce things that make the writing of the people with bad intentions\nlook bad by comparison.One drawback of this approach is that it won't produce the sort of\nwriting that gets you tenure.  And not just because it's not currently\nthe fashion.  In order to get tenure in any field you must not\narrive at conclusions that members of tenure committees can disagree\nwith.  In practice there are two kinds of solutions to this problem.\nIn math and the sciences, you can prove what you're saying, or at\nany rate adjust your conclusions so you're not claiming anything\nfalse (\"6 of 8 subjects had lower blood pressure after the treatment\").\nIn the humanities you can either avoid drawing any definite conclusions\n(e.g. conclude that an issue is a complex one), or draw conclusions\nso narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either\nof these routes.  At best you'll be able to achieve the essayist's\nstandard of proof, not the mathematician's or the experimentalist's.\nAnd yet you won't be able to meet the usefulness test without\nimplying definite and fairly broadly applicable conclusions.  Worse\nstill, the usefulness test will tend to produce results that annoy\npeople: there's no use in telling people things they already believe,\nand people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting\nto general plus useful by starting with useful and cranking up the\ngenerality may be unsuitable for junior professors trying to get\ntenure, but it's better for everyone else, including professors who\nalready have it.  This side of the mountain is a nice gradual slope.\nYou can start by writing things that are useful but very specific,\nand then gradually make them more general.  Joe's has good burritos.\nWhat makes a good burrito?  What makes good food?  What makes\nanything good?  You can take as long as you want.  You don't have\nto get all the way to the top of the mountain.  You don't have to\ntell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an\nencouraging thought.  The field is a lot younger than it seems.\nThough the first philosophers in the western tradition lived about\n2500 years ago, it would be misleading to say the field is 2500\nyears old, because for most of that time the leading practitioners\nweren't doing much more than writing commentaries on Plato or\nAristotle while watching over their shoulders for the next invading\narmy.  In the times when they weren't, philosophy was hopelessly\nintermingled with religion.  It didn't shake itself free till a\ncouple hundred years ago, and even then was afflicted by the\nstructural problems I've described above.  If I say this, some will\nsay it's a ridiculously overbroad and uncharitable generalization,\nand others will say it's old news, but here goes: judging from their\nworks, most philosophers up to the present have been wasting their\ntime.  So in a sense the field is still at the first step. \n[16]That sounds a preposterous claim to make.  It won't seem so\npreposterous in 10,000 years.  Civilization always seems old, because\nit's always the oldest it's ever been.  The only way to say whether\nsomething is really old or not is by looking at structural evidence,\nand structurally philosophy is young; it's still reeling from the\nunexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot\nmore to discover.Notes\n[1]\nIn practice formal logic is not much use, because despite\nsome progress in the last 150 years we're still only able to formalize\na small percentage of statements.  We may never do that much better,\nfor the same reason 1980s-style \"knowledge representation\" could\nnever have worked; many statements may have no representation more\nconcise than a huge, analog brain state.[2]\nIt was harder for Darwin's contemporaries to grasp this than\nwe can easily imagine.  The story of creation in the Bible is not\njust a Judeo-Christian concept; it's roughly what everyone must\nhave believed since before people were people.  The hard part of\ngrasping evolution was to realize that species weren't, as they\nseem to be, unchanging, but had instead evolved from different,\nsimpler organisms over unimaginably long periods of time.Now we don't have to make that leap.  No one in an industrialized\ncountry encounters the idea of evolution for the first time as an\nadult.  Everyone's taught about it as a child, either as truth or\nheresy.[3]\nGreek philosophers before Plato wrote in verse.  This must\nhave affected what they said.  If you try to write about the nature\nof the world in verse, it inevitably turns into incantation.  Prose\nlets you be more precise, and more tentative.[4]\nPhilosophy is like math's\nne'er-do-well brother.  It was born when Plato and Aristotle looked\nat the works of their predecessors and said in effect \"why can't\nyou be more like your brother?\"  Russell was still saying the same\nthing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy\nthe imprecise half.  It's probably inevitable that philosophy will\nsuffer by comparison, because there's no lower bound to its precision.\nBad math is merely boring, whereas bad philosophy is nonsense.  And\nyet there are some good ideas in the imprecise half.[5]\nAristotle's best work was in logic and zoology, both of which\nhe can  be said to have invented.  But the most dramatic departure\nfrom his predecessors was a new, much more analytical style of\nthinking.  He was arguably the first scientist.[6]\nBrooks, Rodney, Programming in Common Lisp, Wiley, 1985, p.\n94.[7]\nSome would say we depend on Aristotle more than we realize,\nbecause his ideas were one of the ingredients in our common culture.\nCertainly a lot of the words we use have a connection with Aristotle,\nbut it seems a bit much to suggest that we wouldn't have the concept\nof the essence of something or the distinction between matter and\nform if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to\ndiff European culture with Chinese: what ideas did European culture\nhave in 1800 that Chinese culture didn't, in virtue of Aristotle's\ncontribution?[8]\nThe meaning of the word \"philosophy\" has changed over time.\nIn ancient times it covered a broad range of topics, comparable in\nscope to our \"scholarship\" (though without the methodological\nimplications).  Even as late as Newton's time it included what we\nnow call \"science.\"  But core of the subject today is still what\nseemed to Aristotle the core: the attempt to discover the most\ngeneral truths.Aristotle didn't call this \"metaphysics.\"  That name got assigned\nto it because the books we now call the Metaphysics came after\n(meta = after) the Physics in the standard edition of Aristotle's\nworks compiled by Andronicus of Rhodes three centuries later.  What\nwe call \"metaphysics\" Aristotle called \"first philosophy.\"[9]\nSome of Aristotle's immediate successors may have realized\nthis, but it's hard to say because most of their works are lost.[10]\nSokal, Alan, \"Transgressing the Boundaries: Toward a Transformative\nHermeneutics of Quantum Gravity,\" Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's\naligned with some axe the audience already has to grind.  If this\nis so we should find it's most popular with groups that are (or\nfeel) weak.  The powerful don't need its reassurance.[11]\nLetter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991,\np. 75.[12]\nA preliminary result, that all metaphysics between Aristotle\nand 1783 had been a waste of time, is due to I. Kant.[13]\nWittgenstein asserted a sort of mastery to which the inhabitants\nof early 20th century Cambridge seem to have been peculiarly\nvulnerable\u2014perhaps partly because so many had been raised religious\nand then stopped believing, so had a vacant space in their heads\nfor someone to tell them what to do (others chose Marx or Cardinal\nNewman), and partly because a quiet, earnest place like Cambridge\nin that era had no natural immunity to messianic figures, just as\nEuropean politics then had no natural immunity to dictators.[14]\nThis is actually from the Ordinatio of Duns Scotus (ca.\n1300), with \"number\" replaced by \"gender.\"  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson,\n1963, p. 92.[15]\nFrankfurt, Harry, On Bullshit,  Princeton University Press,\n2005.[16]\nSome introductions to philosophy now take the line that\nphilosophy is worth studying as a process rather than for any\nparticular truths you'll learn.  The philosophers whose works they\ncover would be rolling in their graves at that.  They hoped they\nwere doing more than serving as examples of how to argue: they hoped\nthey were getting results.  Most were wrong, but it doesn't seem\nan impossible hope.This argument seems to me like someone in 1500 looking at the lack\nof results achieved by alchemy and saying its value was as a process.\nNo, they were going about it wrong.  It turns out it is possible\nto transmute lead into gold (though not economically at current\nenergy prices), but the route to that knowledge was to\nbacktrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston, \nRobert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.April 2005\"Suits make a corporate comeback,\" says the New\nYork Times.  Why does this sound familiar?  Maybe because\nthe suit was also back in February,\n\nSeptember\n2004, June\n2004, March\n2004, September\n2003, \n\nNovember\n2002, \nApril 2002,\nand February\n2002.\n\nWhy do the media keep running stories saying suits are back?  Because\nPR firms tell \nthem to.  One of the most surprising things I discovered\nduring my brief business career was the existence of the PR industry,\nlurking like a huge, quiet submarine beneath the news.  Of the\nstories you read in traditional media that aren't about politics,\ncrimes, or disasters, more than half probably come from PR firms.I know because I spent years hunting such \"press hits.\"  Our startup spent\nits entire marketing budget on PR: at a time when we were assembling\nour own computers to save money, we were paying a PR firm $16,000\na month.  And they were worth it.  PR is the news equivalent of\nsearch engine optimization; instead of buying ads, which readers\nignore, you get yourself inserted directly into the stories.  [1]Our PR firm\nwas one of the best in the business.  In 18 months, they got press\nhits in over 60 different publications.  \nAnd we weren't the only ones they did great things for.  \nIn 1997 I got a call from another\nstartup founder considering hiring them to promote his company.  I\ntold him they were PR gods, worth every penny of their outrageous   \nfees.  But I remember thinking his company's name was odd.\nWhy call an auction site \"eBay\"?\nSymbiosisPR is not dishonest.  Not quite.  In fact, the reason the best PR\nfirms are so effective is precisely that they aren't dishonest.\nThey give reporters genuinely valuable information.  A good PR firm\nwon't bug reporters just because the client tells them to; they've\nworked hard to build their credibility with reporters, and they\ndon't want to destroy it by feeding them mere propaganda.If anyone is dishonest, it's the reporters.  The main reason PR  \nfirms exist is that reporters are lazy.  Or, to put it more nicely,\noverworked.  Really they ought to be out there digging up stories\nfor themselves.  But it's so tempting to sit in their offices and\nlet PR firms bring the stories to them.  After all, they know good\nPR firms won't lie to them.A good flatterer doesn't lie, but tells his victim selective truths\n(what a nice color your eyes are). Good PR firms use the same\nstrategy: they give reporters stories that are true, but whose truth\nfavors their clients.For example, our PR firm often pitched stories about how the Web  \nlet small merchants compete with big ones.  This was perfectly true.\nBut the reason reporters ended up writing stories about this\nparticular truth, rather than some other one, was that small merchants\nwere our target market, and we were paying the piper.Different publications vary greatly in their reliance on PR firms.\nAt the bottom of the heap are the trade press, who make most of\ntheir money from advertising and would give the magazines away for\nfree if advertisers would let them.  [2] The average\ntrade publication is a  bunch of ads, glued together by just enough\narticles to make it look like a magazine.  They're so desperate for\n\"content\" that some will print your press releases almost verbatim,\nif you take the trouble to write them to read like articles.At the other extreme are publications like the New York Times\nand the Wall Street Journal.  Their reporters do go out and\nfind their own stories, at least some of the time.  They'll listen \nto PR firms, but briefly and skeptically.  We managed to get press   \nhits in almost every publication we wanted, but we never managed \nto crack the print edition of the Times.  [3]The weak point of the top reporters is not laziness, but vanity.\nYou don't pitch stories to them.  You have to approach them as if\nyou were a specimen under their all-seeing microscope, and make it\nseem as if the story you want them to run is something they thought \nof themselves.Our greatest PR coup was a two-part one.  We estimated, based on\nsome fairly informal math, that there were about 5000 stores on the\nWeb.  We got one paper to print this number, which seemed neutral   \nenough.  But once this \"fact\" was out there in print, we could quote\nit to other publications, and claim that with 1000 users we had 20%\nof the online store market.This was roughly true.  We really did have the biggest share of the\nonline store market, and 5000 was our best guess at its size.  But\nthe way the story appeared in the press sounded a lot more definite.Reporters like definitive statements.  For example, many of the\nstories about Jeremy Jaynes's conviction say that he was one of the\n10 worst spammers.  This \"fact\" originated in Spamhaus's ROKSO list,\nwhich I think even Spamhaus would admit is a rough guess at the top\nspammers.  The first stories about Jaynes cited this source, but\nnow it's simply repeated as if it were part of the indictment.   \n[4]All you can say with certainty about Jaynes is that he was a fairly\nbig spammer.  But reporters don't want to print vague stuff like\n\"fairly big.\"  They want statements with punch, like \"top ten.\" And\nPR firms give them what they want.\nWearing suits, we're told, will make us \n3.6\npercent more productive.BuzzWhere the work of PR firms really does get deliberately misleading is in\nthe generation of \"buzz.\"  They usually feed the same story to    \nseveral different publications at once.  And when readers see similar\nstories in multiple places, they think there is some important trend\nafoot.  Which is exactly what they're supposed to think.When Windows 95 was launched, people waited outside stores\nat midnight to buy the first copies.  None"}, {"role": "user", "content": "What is the best thing to do in San Francisco? Don't give information outside the document or repeat your findings"}]