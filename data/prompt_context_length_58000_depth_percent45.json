[{"role": "system", "content": "You are a helpful AI bot that answers questions for a user. Keep your response short and direct"}, {"role": "user", "content": "July 2006I've discovered a handy test for figuring out what you're addicted\nto.  Imagine you were going to spend the weekend at a friend's house\non a little island off the coast of Maine.  There are no shops on\nthe island and you won't be able to leave while you're there.  Also,\nyou've never been to this house before, so you can't assume it will\nhave more than any house might.What, besides clothes and toiletries, do you make a point of packing?\nThat's what you're addicted to.  For example, if you find yourself\npacking a bottle of vodka (just in case), you may want to stop and\nthink about that.For me the list is four things: books, earplugs, a notebook, and a\npen.There are other things I might bring if I thought of it, like music,\nor tea, but I can live without them.  I'm not so addicted to caffeine\nthat I wouldn't risk the house not having any tea, just for a\nweekend.Quiet is another matter.  I realize it seems a bit eccentric to\ntake earplugs on a trip to an island off the coast of Maine.  If\nanywhere should be quiet, that should.  But what if the person in\nthe next room snored?  What if there was a kid playing basketball?\n(Thump, thump, thump... thump.)  Why risk it?  Earplugs are small.Sometimes I can think with noise.  If I already have momentum on\nsome project, I can work in noisy places.  I can edit an essay or\ndebug code in an airport.  But airports are not so bad: most of the\nnoise is whitish.  I couldn't work with the sound of a sitcom coming\nthrough the wall, or a car in the street playing thump-thump music.And of course there's another kind of thinking, when you're starting\nsomething new, that requires complete quiet.   You never\nknow when this will strike. It's just as well to carry plugs.The notebook and pen are professional equipment, as it were.  Though\nactually there is something druglike about them, in the sense that\ntheir main purpose is to make me feel better.  I hardly ever go\nback and read stuff I write down in notebooks.  It's just that if\nI can't write things down, worrying about remembering one idea gets\nin the way of having the next.  Pen and paper wick ideas.The best notebooks I've found are made by a company called Miquelrius.\nI use their smallest size, which is about 2.5 x 4 in.\nThe secret to writing on such\nnarrow pages is to break words only when you run out of space, like\na Latin inscription.  I use the cheapest plastic Bic ballpoints,\npartly because their gluey ink doesn't seep through pages, and\npartly so I don't worry about losing them.I only started carrying a notebook about three years ago.  Before\nthat I used whatever scraps of paper I could find.  But the problem\nwith scraps of paper is that they're not ordered.  In a notebook\nyou can guess what a scribble means by looking at the pages\naround it.  In the scrap era I was constantly finding notes I'd\nwritten years before that might say something I needed to remember,\nif I could only figure out what.As for books, I know the house would probably have something to\nread.  On the average trip I bring four books and only read one of\nthem, because I find new books to read en route.  Really bringing\nbooks is insurance.I realize this dependence on books is not entirely good\u2014that what\nI need them for is distraction.  The books I bring on trips are\noften quite virtuous, the sort of stuff that might be assigned\nreading in a college class.  But I know my motives aren't virtuous.\nI bring books because if the world gets boring I need to be able\nto slip into another distilled by some writer.  It's like eating\njam when you know you should be eating fruit.There is a point where I'll do without books.  I was walking in\nsome steep mountains once, and decided I'd rather just think, if I\nwas bored, rather than carry a single unnecessary ounce.  It wasn't\nso bad.  I found I could entertain myself by having ideas instead\nof reading other people's.  If you stop eating jam, fruit starts\nto taste better.So maybe I'll try not bringing books on some future trip.  They're\ngoing to have to pry the plugs out of my cold, dead ears, however.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nMarch 2008, rev. June 2008Technology tends to separate normal from natural.  Our bodies\nweren't designed to eat the foods that people in rich countries eat, or\nto get so little exercise.  \nThere may be a similar problem with the way we work: \na normal job may be as bad for us intellectually as white flour\nor sugar is for us physically.I began to suspect this after spending several years working \nwith startup founders.  I've now worked with over 200 of them, and I've\nnoticed a definite difference between programmers working on their\nown startups and those working for large organizations.\nI wouldn't say founders seem happier, necessarily;\nstarting a startup can be very stressful. Maybe the best way to put\nit is to say that they're happier in the sense that your body is\nhappier during a long run than sitting on a sofa eating\ndoughnuts.Though they're statistically abnormal, startup founders seem to be\nworking in a way that's more natural for humans.I was in Africa last year and saw a lot of animals in the wild that\nI'd only seen in zoos before. It was remarkable how different they\nseemed. Particularly lions. Lions in the wild seem about ten times\nmore alive. They're like different animals. I suspect that working\nfor oneself feels better to humans in much the same way that living\nin the wild must feel better to a wide-ranging predator like a lion.\nLife in a zoo is easier, but it isn't the life they were designed\nfor.\nTreesWhat's so unnatural about working for a big company?  The root of\nthe problem is that humans weren't meant to work in such large\ngroups.Another thing you notice when you see animals in the wild is that\neach species thrives in groups of a certain size.  A herd of impalas\nmight have 100 adults; baboons maybe 20; lions rarely 10.  Humans\nalso seem designed to work in groups, and what I've read about\nhunter-gatherers accords with research on organizations and my own\nexperience to suggest roughly what the ideal size is: groups of 8\nwork well; by 20 they're getting hard to manage; and a group of 50\nis really unwieldy.\n[1]\nWhatever the upper limit is, we are clearly not meant to work in\ngroups of several hundred.  And yet\u2014for reasons having more\nto do with technology than human nature\u2014a great many people\nwork for companies with hundreds or thousands of employees.Companies know groups that large wouldn't work, so they divide\nthemselves into units small enough to work together.  But to\ncoordinate these they have to introduce something new: bosses.These smaller groups are always arranged in a tree structure.  Your\nboss is the point where your group attaches to the tree.  But when\nyou use this trick for dividing a large group into smaller ones,\nsomething strange happens that I've never heard anyone mention\nexplicitly.  In the group one level up from yours, your boss\nrepresents your entire group.  A group of 10 managers is not merely\na group of 10 people working together in the usual way.  It's really\na group of groups.  Which means for a group of 10 managers to work\ntogether as if they were simply a group of 10 individuals, the group\nworking for each manager would have to work as if they were a single\nperson\u2014the workers and manager would each share only one\nperson's worth of freedom between them.In practice a group of people are never able to act as if they were\none person.  But in a large organization divided into groups in\nthis way, the pressure is always in that direction.  Each group\ntries its best to work as if it were the small group of individuals\nthat humans were designed to work in.  That was the point of creating\nit.  And when you propagate that constraint, the result is that\neach person gets freedom of action in inverse proportion to the\nsize of the entire tree.\n[2]Anyone who's worked for a large organization has felt this.  You\ncan feel the difference between working for a company with 100\nemployees and one with 10,000, even if your group has only 10 people.\nCorn SyrupA group of 10 people within a large organization is a kind of fake\ntribe.  The number of people you interact with is about right.  But\nsomething is missing: individual initiative.  Tribes of hunter-gatherers\nhave much more freedom.  The leaders have a little more power than other\nmembers of the tribe, but they don't generally tell them what to\ndo and when the way a boss can.It's not your boss's fault.  The real problem is that in the group\nabove you in the hierarchy, your entire group is one virtual person.\nYour boss is just the way that constraint is imparted to you.So working in a group of 10 people within a large organization feels\nboth right and wrong at the same time.   On the surface it feels\nlike the kind of group you're meant to work in, but something major\nis missing.  A job at a big company is like high fructose corn\nsyrup: it has some of the qualities of things you're meant to like,\nbut is disastrously lacking in others.Indeed, food is an excellent metaphor to explain what's wrong with\nthe usual sort of job.For example, working for a big company is the default thing to do,\nat least for programmers.  How bad could it be?  Well, food shows\nthat pretty clearly.  If you were dropped at a random point in\nAmerica today, nearly all the food around you would be bad for you.\nHumans were not designed to eat white flour, refined sugar, high\nfructose corn syrup, and hydrogenated vegetable oil.  And yet if\nyou analyzed the contents of the average grocery store you'd probably\nfind these four ingredients accounted for most of the calories.\n\"Normal\" food is terribly bad for you.  The only people who eat\nwhat humans were actually designed to eat are a few Birkenstock-wearing\nweirdos in Berkeley.If \"normal\" food is so bad for us, why is it so common?  There are\ntwo main reasons. One is that it has more immediate appeal.  You\nmay feel lousy an hour after eating that pizza, but eating the first\ncouple bites feels great.  The other is economies of scale.\nProducing junk food scales; producing fresh vegetables doesn't.\nWhich means (a) junk food can be very cheap, and (b) it's worth\nspending a lot to market it.If people have to choose between something that's cheap, heavily\nmarketed, and appealing in the short term, and something that's\nexpensive, obscure, and appealing in the long term, which do you\nthink most will choose?It's the same with work.  The average MIT graduate wants to work\nat Google or Microsoft, because it's a recognized brand, it's safe,\nand they'll get paid a good salary right away.  It's the job\nequivalent of the pizza they had for lunch.  The drawbacks will\nonly become apparent later, and then only in a vague sense of\nmalaise.And founders and early employees of startups, meanwhile, are like\nthe Birkenstock-wearing weirdos of Berkeley:  though a tiny minority\nof the population, they're the ones living as humans are meant to.\nIn an artificial world, only extremists live naturally.\nProgrammersThe restrictiveness of big company jobs is particularly hard on\nprogrammers, because the essence of programming is to build new\nthings.  Sales people make much the same pitches every day; support\npeople answer much the same questions; but once you've written a\npiece of code you don't need to write it again.  So a programmer\nworking as programmers are meant to is always making new things.\nAnd when you're part of an organization whose structure gives each\nperson freedom in inverse proportion to the size of the tree, you're\ngoing to face resistance when you do something new.This seems an inevitable consequence of bigness.  It's true even\nin the smartest companies.  I was talking recently to a founder who\nconsidered starting a startup right out of college, but went to\nwork for Google instead because he thought he'd learn more there.\nHe didn't learn as much as he expected.  Programmers learn by doing,\nand most of the things he wanted to do, he couldn't\u2014sometimes\nbecause the company wouldn't let him, but often because the company's\ncode wouldn't let him.  Between the drag of legacy code, the overhead\nof doing development in such a large organization, and the restrictions\nimposed by interfaces owned by other groups, he could only try a\nfraction of the things he would have liked to.  He said he has\nlearned much more in his own startup, despite the fact that he has\nto do all the company's errands as well as programming, because at\nleast when he's programming he can do whatever he wants.An obstacle downstream propagates upstream.  If you're not allowed\nto implement new ideas, you stop having them.  And vice versa: when\nyou can do whatever you want, you have more ideas about what to do.\nSo working for yourself makes your brain more powerful in the same\nway a low-restriction exhaust system makes an engine more powerful.Working for yourself doesn't have to mean starting a startup, of\ncourse.  But a programmer deciding between a regular job at a big\ncompany and their own startup is probably going to learn more doing\nthe startup.You can adjust the amount of freedom you get by scaling the size\nof company you work for.  If you start the company, you'll have the\nmost freedom.  If you become one of the first 10 employees you'll\nhave almost as much freedom as the founders.  Even a company with\n100 people will feel different from one with 1000.Working for a small company doesn't ensure freedom.  The tree\nstructure of large organizations sets an upper bound on freedom,\nnot a lower bound.  The head of a small company may still choose\nto be a tyrant.  The point is that a large organization is compelled\nby its structure to be one.\nConsequencesThat has real consequences for both organizations and individuals.\nOne is that companies will inevitably slow down as they grow larger,\nno matter how hard they try to keep their startup mojo.  It's a\nconsequence of the tree structure that every large organization is\nforced to adopt.Or rather, a large organization could only avoid slowing down if\nthey avoided tree structure.  And since human nature limits the\nsize of group that can work together, the only way I can imagine\nfor larger groups to avoid tree structure would be to have no\nstructure: to have each group actually be independent, and to work\ntogether the way components of a market economy do.That might be worth exploring.  I suspect there are already some\nhighly partitionable businesses that lean this way.  But I don't\nknow any technology companies that have done it.There is one thing companies can do short of structuring themselves\nas sponges:  they can stay small.  If I'm right, then it really\npays to keep a company as small as it can be at every stage.\nParticularly a technology company.  Which means it's doubly important\nto hire the best people.  Mediocre hires hurt you twice: they get\nless done, but they also make you big, because you need more of\nthem to solve a given problem.For individuals the upshot is the same: aim small.  It will always\nsuck to work for large organizations, and the larger the organization,\nthe more it will suck.In an essay I wrote a couple years ago \nI advised graduating seniors\nto work for a couple years for another company before starting their\nown.  I'd modify that now.  Work for another company if you want\nto, but only for a small one, and if you want to start your own\nstartup, go ahead.The reason I suggested college graduates not start startups immediately\nwas that I felt most would fail.  And they will.  But ambitious\nprogrammers are better off doing their own thing and failing than\ngoing to work at a big company.  Certainly they'll learn more.  They\nmight even be better off financially.  A lot of people in their\nearly twenties get into debt, because their expenses grow even\nfaster than the salary that seemed so high when they left school.\nAt least if you start a startup and fail your net worth will be\nzero rather than negative.  \n[3]We've now funded so many different types of founders that we have\nenough data to see patterns, and there seems to be no benefit from\nworking for a big company.  The people who've worked for a few years\ndo seem better than the ones straight out of college, but only\nbecause they're that much older.The people who come to us from big companies often seem kind of\nconservative.  It's hard to say how much is because big companies\nmade them that way, and how much is the natural conservatism that\nmade them work for the big companies in the first place.  But\ncertainly a large part of it is learned.  I know because I've seen\nit burn off.Having seen that happen so many times is one of the things that\nconvinces me that working for oneself, or at least for a small\ngroup, is the natural way for programmers to live.  Founders arriving\nat Y Combinator often have the downtrodden air of refugees.  Three\nmonths later they're transformed: they have so much more \nconfidence\nthat they seem as if they've grown several inches taller. \n[4]\nStrange as this sounds, they seem both more worried and happier at the same\ntime.  Which is exactly how I'd describe the way lions seem in the\nwild.Watching employees get transformed into founders makes it clear\nthat the difference between the two is due mostly to environment\u2014and\nin particular that the environment in big companies is toxic to\nprogrammers.   In the first couple weeks of working on their own\nstartup they seem to come to life, because finally they're working\nthe way people are meant to.Notes[1]\nWhen I talk about humans being meant or designed to live a\ncertain way, I mean by evolution.[2]\nIt's not only the leaves who suffer.  The constraint propagates\nup as well as down.  So managers are constrained too; instead of\njust doing things, they have to act through subordinates.[3]\nDo not finance your startup with credit cards.  Financing a\nstartup with debt is usually a stupid move, and credit card debt\nstupidest of all.  Credit card debt is a bad idea, period.  It is\na trap set by evil companies for the desperate and the foolish.[4]\nThe founders we fund used to be younger (initially we encouraged\nundergrads to apply), and the first couple times I saw this I used\nto wonder if they were actually getting physically taller.Thanks to Trevor Blackwell, Ross Boucher, Aaron Iba, Abby\nKirigin, Ivan Kirigin, Jessica Livingston, and Robert Morris for\nreading drafts of this.April 2004To the popular press, \"hacker\" means someone who breaks\ninto computers.  Among programmers it means a good programmer.\nBut the two meanings are connected.  To programmers,\n\"hacker\" connotes mastery in the most literal sense: someone\nwho can make a computer do what he wants\u2014whether the computer\nwants to or not.To add to the confusion, the noun \"hack\" also has two senses.  It can\nbe either a compliment or an insult.  It's called a hack when\nyou do something in an ugly way.  But when you do something\nso clever that you somehow beat the system, that's also\ncalled a hack.  The word is used more often in the former than\nthe latter sense, probably because ugly solutions are more\ncommon than brilliant ones.Believe it or not, the two senses of \"hack\" are also\nconnected.  Ugly and imaginative solutions have something in\ncommon: they both break the rules.  And there is a gradual\ncontinuum between rule breaking that's merely ugly (using\nduct tape to attach something to your bike) and rule breaking\nthat is brilliantly imaginative (discarding Euclidean space).Hacking predates computers.  When he\nwas working on the Manhattan Project, Richard Feynman used to\namuse himself by breaking into safes containing secret documents.\nThis tradition continues today.\nWhen we were in grad school, a hacker friend of mine who spent too much\ntime around MIT had\nhis own lock picking kit.\n(He now runs a hedge fund, a not unrelated enterprise.)It is sometimes hard to explain to authorities why one would\nwant to do such things.\nAnother friend of mine once got in trouble with the government for\nbreaking into computers.  This had only recently been declared\na crime, and the FBI found that their usual investigative\ntechnique didn't work.  Police investigation apparently begins with\na motive.  The usual motives are few: drugs, money, sex,\nrevenge.  Intellectual curiosity was not one of the motives on\nthe FBI's list.  Indeed, the whole concept seemed foreign to\nthem.Those in authority tend to be annoyed by hackers'\ngeneral attitude of disobedience.  But that disobedience is\na byproduct of the qualities that make them good programmers.\nThey may laugh at the CEO when he talks in generic corporate\nnewspeech, but they also laugh at someone who tells them\na certain problem can't be solved.\nSuppress one, and you suppress the other.This attitude is sometimes affected.  Sometimes young programmers\nnotice the eccentricities of eminent hackers and decide to\nadopt some of their own in order to seem smarter.\nThe fake version is not merely\nannoying; the prickly attitude of these posers\ncan actually slow the process of innovation.But even factoring in their annoying eccentricities,\nthe disobedient attitude of hackers is a net win.  I wish its\nadvantages were better understood.For example, I suspect people in Hollywood are\nsimply mystified by\nhackers' attitudes toward copyrights.  They are a perennial\ntopic of heated discussion on Slashdot.\nBut why should people who program computers\nbe so concerned about copyrights, of all things?Partly because some companies use mechanisms to prevent\ncopying.  Show any hacker a lock and his first thought is\nhow to pick it.  But there is a deeper reason that\nhackers are alarmed by measures like copyrights and patents.\nThey see increasingly aggressive measures to protect\n\"intellectual property\"\nas a threat to the intellectual\nfreedom they need to do their job.\nAnd they are right.It is by poking about inside current technology that\nhackers get ideas for the next generation.  No thanks,\nintellectual homeowners may say, we don't need any\noutside help.  But they're wrong.\nThe next generation of computer technology has\noften\u2014perhaps more often than not\u2014been developed by outsiders.In 1977 there was no doubt some group within IBM developing\nwhat they expected to be\nthe next generation of business computer.  They were mistaken.\nThe next generation of business computer was\nbeing developed on entirely different lines by two long-haired\nguys called Steve in a garage in Los Altos.  At about the\nsame time, the powers that be\nwere cooperating to develop the\nofficial next generation operating system, Multics.\nBut two guys who thought Multics excessively complex went off\nand wrote their own.  They gave it a name that\nwas a joking reference to Multics: Unix.The latest intellectual property laws impose\nunprecedented restrictions on the sort of poking around that\nleads to new ideas. In the past, a competitor might use patents\nto prevent you from selling a copy of something they\nmade, but they couldn't prevent you from\ntaking one apart to see how it worked.   The latest\nlaws make this a crime.  How are we\nto develop new technology if we can't study current\ntechnology to figure out how to improve it?Ironically, hackers have brought this on themselves.\nComputers are responsible for the problem.  The control systems\ninside machines used to be physical: gears and levers and cams.\nIncreasingly, the brains (and thus the value) of products is\nin software. And by this I mean software in the general sense:\ni.e. data.  A song on an LP is physically stamped into the\nplastic.  A song on an iPod's disk is merely stored on it.Data is by definition easy to copy.  And the Internet\nmakes copies easy to distribute.  So it is no wonder\ncompanies are afraid.  But, as so often happens, fear has\nclouded their judgement.  The government has responded\nwith draconian laws to protect intellectual property.\nThey probably mean well. But\nthey may not realize that such laws will do more harm\nthan good.Why are programmers so violently opposed to these laws?\nIf I were a legislator, I'd be interested in this\nmystery\u2014for the same reason that, if I were a farmer and suddenly\nheard a lot of squawking coming from my hen house one night,\nI'd want to go out and investigate.  Hackers are not stupid,\nand unanimity is very rare in this world.\nSo if they're all squawking,   \nperhaps there is something amiss.Could it be that such laws, though intended to protect America,\nwill actually harm it?  Think about it.  There is something\nvery American about Feynman breaking into safes during\nthe Manhattan Project.  It's hard to imagine the authorities\nhaving a sense of humor about such things over\nin Germany at that time.  Maybe it's not a coincidence.Hackers are unruly.  That is the essence of hacking.  And it\nis also the essence of Americanness.  It is no accident\nthat Silicon Valley\nis in America, and not France, or Germany,\nor England, or Japan. In those countries, people color inside\nthe lines.I lived for a while in Florence.  But after I'd been there\na few months I realized that what I'd been unconsciously hoping\nto find there was back in the place I'd just left.\nThe reason Florence is famous is that in 1450, it was New York.\nIn 1450 it was filled with the kind of turbulent and ambitious\npeople you find now in America.  (So I went back to America.)It is greatly to America's advantage that it is\na congenial atmosphere for the right sort of unruliness\u2014that\nit is a home not just for the smart, but for smart-alecks.\nAnd hackers are invariably smart-alecks.  If we had a national\nholiday, it would be April 1st.  It says a great deal about\nour work that we use the same word for a brilliant or a\nhorribly cheesy solution.   When we cook one up we're not\nalways 100% sure which kind it is.  But as long as it has\nthe right sort of wrongness, that's a promising sign.\nIt's odd that people\nthink of programming as precise and methodical.  Computers\nare precise and methodical.  Hacking is something you do\nwith a gleeful laugh.In our world some of the most characteristic solutions\nare not far removed from practical\njokes.  IBM was no doubt rather surprised by the consequences\nof the licensing deal for DOS, just as the hypothetical\n\"adversary\" must be when Michael Rabin solves a problem by\nredefining it as one that's easier to solve.Smart-alecks have to develop a keen sense of how much they\ncan get away with.  And lately hackers \nhave sensed a change\nin the atmosphere.\nLately hackerliness seems rather frowned upon.To hackers the recent contraction in civil liberties seems\nespecially ominous.  That must also mystify outsiders. \nWhy should we care especially about civil\nliberties?  Why programmers, more than\ndentists or salesmen or landscapers?Let me put the case in terms a government official would appreciate.\nCivil liberties are not just an ornament, or a quaint\nAmerican tradition.  Civil liberties make countries rich.\nIf you made a graph of\nGNP per capita vs. civil liberties, you'd notice a definite\ntrend.  Could civil liberties really be a cause, rather\nthan just an effect?  I think so.  I think a society in which\npeople can do and say what they want will also tend to\nbe one in which the most efficient solutions win, rather than\nthose sponsored by the most influential people.\nAuthoritarian countries become corrupt;\ncorrupt countries become poor; and poor countries are weak. \nIt seems to me there is\na Laffer curve for government power, just as for\ntax revenues.  At least, it seems likely enough that it\nwould be stupid to try the experiment and find out.  Unlike\nhigh tax rates, you can't repeal totalitarianism if it\nturns out to be a mistake.This is why hackers worry.  The government spying on people doesn't\nliterally make programmers write worse code.  It just leads\neventually to a world in which bad ideas win.  And because\nthis is so important to hackers, they're especially sensitive\nto it.  They can sense totalitarianism approaching from a\ndistance, as animals can sense an approaching  \nthunderstorm.It would be ironic if, as hackers fear, recent measures\nintended to protect national security and intellectual property\nturned out to be a missile aimed right at what makes   \nAmerica successful.  But it would not be the first time that\nmeasures taken in an atmosphere of panic had\nthe opposite of the intended effect.There is such a thing as Americanness.\nThere's nothing like living abroad to teach you that.   \nAnd if you want to know whether something will nurture or squash\nthis quality, it would be hard to find a better focus\ngroup than hackers, because they come closest of any group\nI know to embodying it.  Closer, probably,  than\nthe men running our government,\nwho for all their talk of patriotism\nremind me more of Richelieu or Mazarin\nthan Thomas Jefferson or George Washington.When you read what the founding fathers had to say for\nthemselves, they sound more like hackers.\n\"The spirit of resistance to government,\"\nJefferson wrote, \"is so valuable on certain occasions, that I wish\nit always to be kept alive.\"Imagine an American president saying that today.\nLike the remarks of an outspoken old grandmother, the sayings of\nthe founding fathers have embarrassed generations of\ntheir less confident successors.  They remind us where we come from.\nThey remind us that it is the people who break rules that are\nthe source of America's wealth and power.Those in a position to impose rules naturally want them to be\nobeyed.  But be careful what you ask for. You might get it.Thanks to Ken Anderson, Trevor Blackwell, Daniel Giffin, \nSarah Harlin,  Shiro Kawai, Jessica Livingston, Matz, \nJackie McDonough, Robert Morris, Eric Raymond, Guido van Rossum,\nDavid Weinberger, and\nSteven Wolfram for reading drafts of this essay.\n(The image shows Steves Jobs and Wozniak \nwith a \"blue box.\"\nPhoto by Margret Wozniak. Reproduced by permission of Steve\nWozniak.)May 2021Noora Health, a nonprofit I've \nsupported for years, just launched\na new NFT. It has a dramatic name, Save Thousands of Lives,\nbecause that's what the proceeds will do.Noora has been saving lives for 7 years. They run programs in\nhospitals in South Asia to teach new mothers how to take care of\ntheir babies once they get home. They're in 165 hospitals now. And\nbecause they know the numbers before and after they start at a new\nhospital, they can measure the impact they have. It is massive.\nFor every 1000 live births, they save 9 babies.This number comes from a study\nof 133,733 families at 28 different\nhospitals that Noora conducted in collaboration with the Better\nBirth team at Ariadne Labs, a joint center for health systems\ninnovation at Brigham and Women\u0092s Hospital and Harvard T.H. Chan\nSchool of Public Health.Noora is so effective that even if you measure their costs in the\nmost conservative way, by dividing their entire budget by the number\nof lives saved, the cost of saving a life is the lowest I've seen.\n$1,235.For this NFT, they're going to issue a public report tracking how\nthis specific tranche of money is spent, and estimating the number\nof lives saved as a result.NFTs are a new territory, and this way of using them is especially\nnew, but I'm excited about its potential. And I'm excited to see\nwhat happens with this particular auction, because unlike an NFT\nrepresenting something that has already happened,\nthis NFT gets better as the price gets higher.The reserve price was about $2.5 million, because that's what it\ntakes for the name to be accurate: that's what it costs to save\n2000 lives. But the higher the price of this NFT goes, the more\nlives will be saved. What a sentence to be able to write.February 2020What should an essay be? Many people would say persuasive. That's\nwhat a lot of us were taught essays should be. But I think we can\naim for something more ambitious: that an essay should be useful.To start with, that means it should be correct. But it's not enough\nmerely to be correct. It's easy to make a statement correct by\nmaking it vague. That's a common flaw in academic writing, for\nexample. If you know nothing at all about an issue, you can't go\nwrong by saying that the issue is a complex one, that there are\nmany factors to be considered, that it's a mistake to take too\nsimplistic a view of it, and so on.Though no doubt correct, such statements tell the reader nothing.\nUseful writing makes claims that are as strong as they can be made\nwithout becoming false.For example, it's more useful to say that Pike's Peak is near the\nmiddle of Colorado than merely somewhere in Colorado. But if I say\nit's in the exact middle of Colorado, I've now gone too far, because\nit's a bit east of the middle.Precision and correctness are like opposing forces. It's easy to\nsatisfy one if you ignore the other. The converse of vaporous\nacademic writing is the bold, but false, rhetoric of demagogues.\nUseful writing is bold, but true.It's also two other things: it tells people something important,\nand that at least some of them didn't already know.Telling people something they didn't know doesn't always mean\nsurprising them. Sometimes it means telling them something they\nknew unconsciously but had never put into words. In fact those may\nbe the more valuable insights, because they tend to be more\nfundamental.Let's put them all together. Useful writing tells people something\ntrue and important that they didn't already know, and tells them\nas unequivocally as possible.Notice these are all a matter of degree. For example, you can't\nexpect an idea to be novel to everyone. Any insight that you have\nwill probably have already been had by at least one of the world's\n7 billion people. But it's sufficient if an idea is novel to a lot\nof readers.Ditto for correctness, importance, and strength. In effect the four\ncomponents are like numbers you can multiply together to get a score\nfor usefulness. Which I realize is almost awkwardly reductive, but\nnonetheless true._____\nHow can you ensure that the things you say are true and novel and\nimportant? Believe it or not, there is a trick for doing this. I\nlearned it from my friend Robert Morris, who has a horror of saying\nanything dumb. His trick is not to say anything unless he's sure\nit's worth hearing. This makes it hard to get opinions out of him,\nbut when you do, they're usually right.Translated into essay writing, what this means is that if you write\na bad sentence, you don't publish it. You delete it and try again.\nOften you abandon whole branches of four or five paragraphs. Sometimes\na whole essay.You can't ensure that every idea you have is good, but you can\nensure that every one you publish is, by simply not publishing the\nones that aren't.In the sciences, this is called publication bias, and is considered\nbad. When some hypothesis you're exploring gets inconclusive results,\nyou're supposed to tell people about that too. But with essay\nwriting, publication bias is the way to go.My strategy is loose, then tight. I write the first draft of an\nessay fast, trying out all kinds of ideas. Then I spend days rewriting\nit very carefully.I've never tried to count how many times I proofread essays, but\nI'm sure there are sentences I've read 100 times before publishing\nthem. When I proofread an essay, there are usually passages that\nstick out in an annoying way, sometimes because they're clumsily\nwritten, and sometimes because I'm not sure they're true. The\nannoyance starts out unconscious, but after the tenth reading or\nso I'm saying \"Ugh, that part\" each time I hit it. They become like\nbriars that catch your sleeve as you walk past. Usually I won't\npublish an essay till they're all gone \u0097 till I can read through\nthe whole thing without the feeling of anything catching.I'll sometimes let through a sentence that seems clumsy, if I can't\nthink of a way to rephrase it, but I will never knowingly let through\none that doesn't seem correct. You never have to. If a sentence\ndoesn't seem right, all you have to do is ask why it doesn't, and\nyou've usually got the replacement right there in your head.This is where essayists have an advantage over journalists. You\ndon't have a deadline. You can work for as long on an essay as you\nneed to get it right. You don't have to publish the essay at all,\nif you can't get it right. Mistakes seem to lose courage in the\nface of an enemy with unlimited resources. Or that's what it feels\nlike. What's really going on is that you have different expectations\nfor yourself. You're like a parent saying to a child \"we can sit\nhere all night till you eat your vegetables.\" Except you're the\nchild too.I'm not saying no mistake gets through. For example, I added condition\n(c) in \"A Way to Detect Bias\" \nafter readers pointed out that I'd\nomitted it. But in practice you can catch nearly all of them.There's a trick for getting importance too. It's like the trick I\nsuggest to young founders for getting startup ideas: to make something\nyou yourself want. You can use yourself as a proxy for the reader.\nThe reader is not completely unlike you, so if you write about\ntopics that seem important to you, they'll probably seem important\nto a significant number of readers as well.Importance has two factors. It's the number of people something\nmatters to, times how much it matters to them. Which means of course\nthat it's not a rectangle, but a sort of ragged comb, like a Riemann\nsum.The way to get novelty is to write about topics you've thought about\na lot. Then you can use yourself as a proxy for the reader in this\ndepartment too. Anything you notice that surprises you, who've\nthought about the topic a lot, will probably also surprise a\nsignificant number of readers. And here, as with correctness and\nimportance, you can use the Morris technique to ensure that you\nwill. If you don't learn anything from writing an essay, don't\npublish it.You need humility to measure novelty, because acknowledging the\nnovelty of an idea means acknowledging your previous ignorance of\nit. Confidence and humility are often seen as opposites, but in\nthis case, as in many others, confidence helps you to be humble.\nIf you know you're an expert on some topic, you can freely admit\nwhen you learn something you didn't know, because you can be confident\nthat most other people wouldn't know it either.The fourth component of useful writing, strength, comes from two\nthings: thinking well, and the skillful use of qualification. These\ntwo counterbalance each other, like the accelerator and clutch in\na car with a manual transmission. As you try to refine the expression\nof an idea, you adjust the qualification accordingly. Something\nyou're sure of, you can state baldly with no qualification at all,\nas I did the four components of useful writing. Whereas points that\nseem dubious have to be held at arm's length with perhapses.As you refine an idea, you're pushing in the direction of less\nqualification. But you can rarely get it down to zero. Sometimes\nyou don't even want to, if it's a side point and a fully refined\nversion would be too long.Some say that qualifications weaken writing. For example, that you\nshould never begin a sentence in an essay with \"I think,\" because\nif you're saying it, then of course you think it. And it's true\nthat \"I think x\" is a weaker statement than simply \"x.\" Which is\nexactly why you need \"I think.\" You need it to express your degree\nof certainty.But qualifications are not scalars. They're not just experimental\nerror. There must be 50 things they can express: how broadly something\napplies, how you know it, how happy you are it's so, even how it\ncould be falsified. I'm not going to try to explore the structure\nof qualification here. It's probably more complex than the whole\ntopic of writing usefully. Instead I'll just give you a practical\ntip: Don't underestimate qualification. It's an important skill in\nits own right, not just a sort of tax you have to pay in order to\navoid saying things that are false. So learn and use its full range.\nIt may not be fully half of having good ideas, but it's part of\nhaving them.There's one other quality I aim for in essays: to say things as\nsimply as possible. But I don't think this is a component of\nusefulness. It's more a matter of consideration for the reader. And\nit's a practical aid in getting things right; a mistake is more\nobvious when expressed in simple language. But I'll admit that the\nmain reason I write simply is not for the reader's sake or because\nit helps get things right, but because it bothers me to use more\nor fancier words than I need to. It seems inelegant, like a program\nthat's too long.I realize florid writing works for some people. But unless you're\nsure you're one of them, the best advice is to write as simply as\nyou can._____\nI believe the formula I've given you, importance + novelty +\ncorrectness + strength, is the recipe for a good essay. But I should\nwarn you that it's also a recipe for making people mad.The root of the problem is novelty. When you tell people something\nthey didn't know, they don't always thank you for it. Sometimes the\nreason people don't know something is because they don't want to\nknow it. Usually because it contradicts some cherished belief. And\nindeed, if you're looking for novel ideas, popular but mistaken\nbeliefs are a good place to find them. Every popular mistaken belief\ncreates a dead zone of ideas around \nit that are relatively unexplored because they contradict it.The strength component just makes things worse. If there's anything\nthat annoys people more than having their cherished assumptions\ncontradicted, it's having them flatly contradicted.Plus if you've used the Morris technique, your writing will seem\nquite confident. Perhaps offensively confident, to people who\ndisagree with you. The reason you'll seem confident is that you are\nconfident: you've cheated, by only publishing the things you're\nsure of.  It will seem to people who try to disagree with you that\nyou never admit you're wrong. In fact you constantly admit you're\nwrong. You just do it before publishing instead of after.And if your writing is as simple as possible, that just makes things\nworse. Brevity is the diction of command. If you watch someone\ndelivering unwelcome news from a position of inferiority, you'll\nnotice they tend to use lots of words, to soften the blow. Whereas\nto be short with someone is more or less to be rude to them.It can sometimes work to deliberately phrase statements more weakly\nthan you mean. To put \"perhaps\" in front of something you're actually\nquite sure of. But you'll notice that when writers do this, they\nusually do it with a wink.I don't like to do this too much. It's cheesy to adopt an ironic\ntone for a whole essay. I think we just have to face the fact that\nelegance and curtness are two names for the same thing.You might think that if you work sufficiently hard to ensure that\nan essay is correct, it will be invulnerable to attack. That's sort\nof true. It will be invulnerable to valid attacks. But in practice\nthat's little consolation.In fact, the strength component of useful writing will make you\nparticularly vulnerable to misrepresentation. If you've stated an\nidea as strongly as you could without making it false, all anyone\nhas to do is to exaggerate slightly what you said, and now it is\nfalse.Much of the time they're not even doing it deliberately. One of the\nmost surprising things you'll discover, if you start writing essays,\nis that people who disagree with you rarely disagree with what\nyou've actually written. Instead they make up something you said\nand disagree with that.For what it's worth, the countermove is to ask someone who does\nthis to quote a specific sentence or passage you wrote that they\nbelieve is false, and explain why. I say \"for what it's worth\"\nbecause they never do. So although it might seem that this could\nget a broken discussion back on track, the truth is that it was\nnever on track in the first place.Should you explicitly forestall likely misinterpretations? Yes, if\nthey're misinterpretations a reasonably smart and well-intentioned\nperson might make. In fact it's sometimes better to say something\nslightly misleading and then add the correction than to try to get\nan idea right in one shot. That can be more efficient, and can also\nmodel the way such an idea would be discovered.But I don't think you should explicitly forestall intentional\nmisinterpretations in the body of an essay. An essay is a place to\nmeet honest readers. You don't want to spoil your house by putting\nbars on the windows to protect against dishonest ones. The place\nto protect against intentional misinterpretations is in end-notes.\nBut don't think you can predict them all. People are as ingenious\nat misrepresenting you when you say something they don't want to\nhear as they are at coming up with rationalizations for things they\nwant to do but know they shouldn't. I suspect it's the same skill._____\nAs with most other things, the way to get better at writing essays\nis to practice. But how do you start? Now that we've examined the\nstructure of useful writing, we can rephrase that question more\nprecisely. Which constraint do you relax initially? The answer is,\nthe first component of importance: the number of people who care\nabout what you write.If you narrow the topic sufficiently, you can probably find something\nyou're an expert on. Write about that to start with. If you only\nhave ten readers who care, that's fine. You're helping them, and\nyou're writing. Later you can expand the breadth of topics you write\nabout.The other constraint you can relax is a little surprising: publication.\nWriting essays doesn't have to mean publishing them. That may seem\nstrange now that the trend is to publish every random thought, but\nit worked for me. I wrote what amounted to essays in notebooks for\nabout 15 years. I never published any of them and never expected\nto. I wrote them as a way of figuring things out. But when the web\ncame along I'd had a lot of practice.Incidentally, \nSteve \nWozniak did the same thing. In high school he\ndesigned computers on paper for fun. He couldn't build them because\nhe couldn't afford the components. But when Intel launched 4K DRAMs\nin 1975, he was ready._____\nHow many essays are there left to write though? The answer to that\nquestion is probably the most exciting thing I've learned about\nessay writing. Nearly all of them are left to write.Although the essay \nis an old form, it hasn't been assiduously\ncultivated. In the print era, publication was expensive, and there\nwasn't enough demand for essays to publish that many. You could\npublish essays if you were already well known for writing something\nelse, like novels. Or you could write book reviews that you took\nover to express your own ideas. But there was not really a direct\npath to becoming an essayist. Which meant few essays got written,\nand those that did tended to be about a narrow range of subjects.Now, thanks to the internet, there's a path. Anyone can publish\nessays online. You start in obscurity, perhaps, but at least you\ncan start. You don't need anyone's permission.It sometimes happens that an area of knowledge sits quietly for\nyears, till some change makes it explode. Cryptography did this to\nnumber theory. The internet is doing it to the essay.The exciting thing is not that there's a lot left to write, but\nthat there's a lot left to discover. There's a certain kind of idea\nthat's best discovered by writing essays. If most essays are still\nunwritten, most such ideas are still undiscovered.Notes[1] Put railings on the balconies, but don't put bars on the windows.[2] Even now I sometimes write essays that are not meant for\npublication. I wrote several to figure out what Y Combinator should\ndo, and they were really helpful.Thanks to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and\nRobert Morris for reading drafts of this.December 2014If the world were static, we could have monotonically increasing\nconfidence in our beliefs.  The more (and more varied) experience\na belief survived, the less likely it would be false.  Most people\nimplicitly believe something like this about their opinions.  And\nthey're justified in doing so with opinions about things that don't\nchange much, like human nature.  But you can't trust your opinions\nin the same way about things that change, which could include\npractically everything else.When experts are wrong, it's often because they're experts on an\nearlier version of the world.Is it possible to avoid that?  Can you protect yourself against\nobsolete beliefs?  To some extent, yes. I spent almost a decade\ninvesting in early stage startups, and curiously enough protecting\nyourself against obsolete beliefs is exactly what you have to do\nto succeed as a startup investor.  Most really good startup ideas\nlook like bad ideas at first, and many of those look bad specifically\nbecause some change in the world just switched them from bad to\ngood.  I spent a lot of time learning to recognize such ideas, and\nthe techniques I used may be applicable to ideas in general.The first step is to have an explicit belief in change.  People who\nfall victim to a monotonically increasing confidence in their\nopinions are implicitly concluding the world is static.  If you\nconsciously remind yourself it isn't, you start to look for change.Where should one look for it?  Beyond the moderately useful\ngeneralization that human nature doesn't change much, the unfortunate\nfact is that change is hard to predict.  This is largely a tautology\nbut worth remembering all the same: change that matters usually\ncomes from an unforeseen quarter.So I don't even try to predict it.  When I get asked in interviews\nto predict the future, I always have to struggle to come up with\nsomething plausible-sounding on the fly, like a student who hasn't\nprepared for an exam.\n[1]\nBut it's not out of laziness that I haven't\nprepared.  It seems to me that beliefs about the future are so\nrarely correct that they usually aren't worth the extra rigidity\nthey impose, and that the best strategy is simply to be aggressively\nopen-minded.  Instead of trying to point yourself in the right\ndirection, admit you have no idea what the right direction is, and\ntry instead to be super sensitive to the winds of change.It's ok to have working hypotheses, even though they may constrain\nyou a bit, because they also motivate you.  It's exciting to chase\nthings and exciting to try to guess answers.  But you have to be\ndisciplined about not letting your hypotheses harden into anything\nmore.\n[2]I believe this passive m.o. works not just for evaluating new ideas\nbut also for having them.  The way to come up with new ideas is not\nto try explicitly to, but to try to solve problems and simply not\ndiscount weird hunches you have in the process.The winds of change originate in the unconscious minds of domain\nexperts.  If you're sufficiently expert in a field, any weird idea\nor apparently irrelevant question that occurs to you is ipso facto\nworth exploring. \n[3]\n Within Y Combinator, when an idea is described\nas crazy, it's a compliment\u2014in fact, on average probably a\nhigher compliment than when an idea is described as good.Startup investors have extraordinary incentives for correcting\nobsolete beliefs.  If they can realize before other investors that\nsome apparently unpromising startup isn't, they can make a huge\namount of money.  But the incentives are more than just financial.\nInvestors' opinions are explicitly tested: startups come to them\nand they have to say yes or no, and then, fairly quickly, they learn\nwhether they guessed right.  The investors who say no to a Google\n(and there were several) will remember it for the rest of their\nlives.Anyone who must in some sense bet on ideas rather than merely\ncommenting on them has similar incentives.  Which means anyone who\nwants such incentives can have them, by turning their comments into\nbets: if you write about a topic in some fairly durable and public\nform, you'll find you worry much more about getting things right\nthan most people would in a casual conversation.\n[4]Another trick I've found to protect myself against obsolete beliefs\nis to focus initially on people rather than ideas. Though the nature\nof future discoveries is hard to predict, I've found I can predict\nquite well what sort of people will make them.  Good new ideas come\nfrom earnest, energetic, independent-minded people.Betting on people over ideas saved me countless times as an investor.\nWe thought Airbnb was a bad idea, for example. But we could tell\nthe founders were earnest, energetic, and independent-minded.\n(Indeed, almost pathologically so.)  So we suspended disbelief and\nfunded them.This too seems a technique that should be generally applicable.\nSurround yourself with the sort of people new ideas come from.  If\nyou want to notice quickly when your beliefs become obsolete, you\ncan't do better than to be friends with the people whose discoveries\nwill make them so.It's hard enough already not to become the prisoner of your own\nexpertise, but it will only get harder, because change is accelerating.\nThat's not a recent trend; change has been accelerating since the\npaleolithic era.  Ideas beget ideas.  I don't expect that to change.\nBut I could be wrong.\nNotes[1]\nMy usual trick is to talk about aspects of the present that\nmost people haven't noticed yet.[2]\nEspecially if they become well enough known that people start\nto identify them with you.  You have to be extra skeptical about\nthings you want to believe, and once a hypothesis starts to be\nidentified with you, it will almost certainly start to be in that\ncategory.[3]\nIn practice \"sufficiently expert\" doesn't require one to be\nrecognized as an expert\u2014which is a trailing indicator in any\ncase.  In many fields a year of focused work plus caring a lot would\nbe enough.[4]\nThough they are public and persist indefinitely, comments on\ne.g. forums and places like Twitter seem empirically to work like\ncasual conversation.  The threshold may be whether what you write\nhas a title.\nThanks to Sam Altman, Patrick Collison, and Robert Morris\nfor reading drafts of this.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nNovember 2005Does \"Web 2.0\" mean anything?  Till recently I thought it didn't,\nbut the truth turns out to be more complicated.  Originally, yes,\nit was meaningless.  Now it seems to have acquired a meaning.  And\nyet those who dislike the term are probably right, because if it\nmeans what I think it does, we don't need it.I first heard the phrase \"Web 2.0\" in the name of the Web 2.0\nconference in 2004.  At the time it was supposed to mean using \"the\nweb as a platform,\" which I took to refer to web-based applications.\n[1]So I was surprised at a conference this summer when Tim O'Reilly\nled a session intended to figure out a definition of \"Web 2.0.\"\nDidn't it already mean using the web as a platform?  And if it\ndidn't already mean something, why did we need the phrase at all?OriginsTim says the phrase \"Web 2.0\" first\narose in \"a brainstorming session between\nO'Reilly and Medialive International.\" What is Medialive International?\n\"Producers of technology tradeshows and conferences,\" according to\ntheir site.  So presumably that's what this brainstorming session\nwas about.  O'Reilly wanted to organize a conference about the web,\nand they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a\nnew version of the web.  They just wanted to make the point\nthat the web mattered again.  It was a kind of semantic deficit\nspending: they knew new things were coming, and the \"2.0\" referred\nto whatever those might turn out to be.And they were right.  New things were coming.  But the new version\nnumber led to some awkwardness in the short term.  In the process\nof developing the pitch for the first conference, someone must have\ndecided they'd better take a stab at explaining what that \"2.0\"\nreferred to.  Whatever it meant, \"the web as a platform\" was at\nleast not too constricting.The story about \"Web 2.0\" meaning the web as a platform didn't live\nmuch past the first conference.  By the second conference, what\n\"Web 2.0\" seemed to mean was something about democracy.  At least,\nit did when people wrote about it online.  The conference itself\ndidn't seem very grassroots.  It cost $2800, so the only people who\ncould afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article\nabout the conference in Wired News spoke of \"throngs of\ngeeks.\"  When a friend of mine asked Ryan about this, it was news\nto him.  He said he'd originally written something like \"throngs\nof VCs and biz dev guys\" but had later shortened it just to \"throngs,\"\nand that this must have in turn been expanded by the editors into\n\"throngs of geeks.\"  After all, a Web 2.0 conference would presumably\nbe full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a   \nsuit, a sight so alien I couldn't parse it at first.  I saw\nhim walk by and said to one of the O'Reilly people \"that guy looks\njust like Tim.\"\"Oh, that's Tim.  He bought a suit.\"\nI ran after him, and sure enough, it was.  He explained that he'd\njust bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows\nduring the Bubble, full of prowling VCs looking for the next hot\nstartup.  There was that same odd atmosphere created by a large  \nnumber of people determined not to miss out.  Miss out on what?\nThey didn't know.  Whatever was going to happen\u2014whatever Web 2.0\nturned out to be.I wouldn't quite call it \"Bubble 2.0\" just because VCs are eager\nto invest again.  The Internet is a genuinely big deal.  The bust\nwas as much an overreaction as\nthe boom.  It's to be expected that once we started to pull out of\nthe bust, there would be a lot of growth in this area, just as there\nwas in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO\nmarket is gone.  Venture investors\nare driven by exit strategies.  The reason they were funding all  \nthose laughable startups during the late 90s was that they hoped\nto sell them to gullible retail investors; they hoped to be laughing\nall the way to the bank.  Now that route is closed.  Now the default\nexit strategy is to get bought, and acquirers are less prone to\nirrational exuberance than IPO investors.  The closest you'll get \nto Bubble valuations is Rupert Murdoch paying $580 million for   \nMyspace.  That's only off by a factor of 10 or so.1. AjaxDoes \"Web 2.0\" mean anything more than the name of a conference\nyet?  I don't like to admit it, but it's starting to.  When people\nsay \"Web 2.0\" now, I have some idea what they mean.  And the fact\nthat I both despise the phrase and understand it is the surest proof\nthat it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still\nonly just bear to use without scare quotes.  Basically, what \"Ajax\"\nmeans is \"Javascript now works.\"  And that in turn means that\nweb-based applications can now be made to work much more like desktop\nones.As you read this, a whole new generation\nof software is being written to take advantage of Ajax.  There\nhasn't been such a wave of new applications since microcomputers\nfirst appeared.  Even Microsoft sees it, but it's too late for them\nto do anything more than leak \"internal\"  \ndocuments designed to give the impression they're on top of this\nnew trend.In fact the new generation of software is being written way too\nfast for Microsoft even to channel it, let alone write their own\nin house.  Their only hope now is to buy all the best Ajax startups\nbefore Google does.  And even that's going to be hard, because\nGoogle has as big a head start in buying microstartups as it did\nin search a few years ago.  After all, Google Maps, the canonical\nAjax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference\nturned out to be partially right: web-based applications are a big\ncomponent of Web 2.0.  But I'm convinced they got this right by \naccident.  The Ajax boom didn't start till early 2005, when Google\nMaps appeared and the term \"Ajax\" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several\nexamples to prove that amateurs can   \nsurpass professionals, when they have the right kind of system to \nchannel their efforts.  Wikipedia\nmay be the most famous.  Experts have given Wikipedia middling\nreviews, but they miss the critical point: it's good enough.  And   \nit's free, which means people actually read it.  On the web, articles\nyou have to pay for might as well not exist.  Even if you were    \nwilling to pay to read them yourself, you can't link to them.    \nThey're not part of the conversation.Another place democracy seems to win is in deciding what counts as\nnews.  I never look at any news site now except Reddit.\n[2]\n I know if something major\nhappens, or someone writes a particularly interesting article, it   \nwill show up there.  Why bother checking the front page of any\nspecific paper or magazine?  Reddit's like an RSS feed for the whole\nweb, with a filter for quality.  Similar sites include Digg, a technology news site that's\nrapidly approaching Slashdot in popularity, and del.icio.us, the collaborative\nbookmarking network that set off the \"tagging\" movement.  And whereas\nWikipedia's main appeal is that it's good enough and free, these\nsites suggest that voters do a significantly better job than human\neditors.The most dramatic example of Web 2.0 democracy is not in the selection\nof ideas, but their production.  \nI've noticed for a while that the stuff I read on individual people's\nsites is as good as or better than the stuff I read in newspapers\nand magazines.  And now I have independent evidence: the top links\non Reddit are generally links to individual people's sites rather  \nthan to magazine articles or news stories.My experience of writing\nfor magazines suggests an explanation.  Editors.  They control the\ntopics you can write about, and they can generally rewrite whatever\nyou produce.  The result is to damp extremes.  Editing yields 95th\npercentile writing\u201495% of articles are improved by it, but 5% are\ndragged down.  5% of the time you get \"throngs of geeks.\"On the web, people can publish whatever they want.  Nearly all of\nit falls short of the editor-damped writing in print publications.\nBut the pool of writers is very, very large.  If it's large enough,\nthe lack of damping means the best writing online should surpass  \nthe best in print.\n[3]  \nAnd now that the web has evolved mechanisms\nfor selecting good stuff, the web wins net.  Selection beats damping,\nfor the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the  \nstartups of the Bubble what bloggers are to the print media.  During\nthe Bubble, a startup meant a company headed by an MBA that was   \nblowing through several million dollars of VC money to \"get big\nfast\" in the most literal sense.  Now it means a smaller, younger, more technical group that just      \ndecided to make something great.  They'll decide later if they want  \nto raise VC-scale funding, and if they take it, they'll take it on\ntheir terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements\nof \"Web 2.0.\"  I also see a third: not to maltreat users.  During\nthe Bubble a lot of popular sites were quite high-handed with users.\nAnd not just in obvious ways, like making them register, or subjecting\nthem to annoying ads.  The very design of the average site in the   \nlate 90s was an abuse.  Many of the most popular sites were loaded\nwith obtrusive branding that made them slow to load and sent the\nuser the message: this is our site, not yours.  (There's a physical\nanalog in the Intel and Microsoft stickers that come on some\nlaptops.)I think the root of the problem was that sites felt they were giving\nsomething away for free, and till recently a company giving anything\naway for free could be pretty high-handed about it.  Sometimes it\nreached the point of economic sadism: site owners assumed that the\nmore pain they caused the user, the more benefit it must be to them.  \nThe most dramatic remnant of this model may be at salon.com, where   \nyou can read the beginning of a story, but to get the rest you have\nsit through a movie.At Y Combinator we advise all the startups we fund never to lord\nit over users.  Never make users register, unless you need to in\norder to store something for them.  If you do make users register,   \nnever make them wait for a confirmation link in an email; in fact,\ndon't even ask for their email address unless you need it for some\nreason.  Don't ask them any unnecessary questions.  Never send them\nemail unless they explicitly ask for it.  Never frame pages you\nlink to, or open them in new windows.  If you have a free version \nand a pay version, don't make the free version too restricted.  And\nif you find yourself asking \"should we allow users to do x?\" just \nanswer \"yes\" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups\nnever to let anyone fly under them, meaning never to let any other\ncompany offer a cheaper, easier solution.  Another way to fly low \nis to give users more power.  Let users do what they want.  If you \ndon't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual\nsongs instead of having to buy whole albums.  The recording industry\nhated the idea and resisted it as long as possible.  But it was\nobvious what users wanted, so Apple flew under the labels.\n[4]\nThough really it might be better to describe iTunes as Web 1.5.     \nWeb 2.0 applied to music would probably mean individual bands giving\naway DRMless songs for free.The ultimate way to be nice to users is to give them something for\nfree that competitors charge for.  During the 90s a lot of people   \nprobably thought we'd have some working system for micropayments     \nby now.  In fact things have gone in the other direction.  The most   \nsuccessful sites are the ones that figure out new ways to give stuff\naway for free.  Craigslist has largely destroyed the classified ad\nsites of the 90s, and OkCupid looks likely to do the same to the\nprevious generation of dating sites.Serving web pages is very, very cheap.  If you can make even a   \nfraction of a cent per page view, you can make a profit.  And\ntechnology for targeting ads continues to improve.  I wouldn't be\nsurprised if ten years from now eBay had been supplanted by an      \nad-supported freeBay (or, more likely, gBay).Odd as it might sound, we tell startups that they should try to\nmake as little money as possible.  If you can figure out a way to\nturn a billion dollar industry into a fifty million dollar industry,\nso much the better, if all fifty million go to you.  Though indeed,\nmaking things cheaper often turns out to generate more money in the\nend, just as automating things often turns out to generate more\njobs.The ultimate target is Microsoft.  What a bang that balloon is going\nto make when someone pops it by offering a free web-based alternative \nto MS Office.\n[5]\nWho will?  Google?  They seem to be taking their\ntime.  I suspect the pin will be wielded by a couple of 20 year old\nhackers who are too naive to be intimidated by the idea.  (How hard\ncan it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in  \ncommon?  I didn't realize they had anything in common till recently,\nwhich is one of the reasons I disliked the term \"Web 2.0\" so much.\nIt seemed that it was being used as a label for whatever happened\nto be new\u2014that it didn't predict anything.But there is a common thread.  Web 2.0 means using the web the way\nit's meant to be used.  The \"trends\" we're seeing now are simply\nthe inherent nature of the web emerging from under the broken models\nthat got imposed on it during the Bubble.I realized this when I read an  interview with\nJoe Kraus, the co-founder of Excite.\n[6]\n\n  Excite really never got the business model right at all.  We fell \n  into the classic problem of how when a new medium comes out it\n  adopts the practices, the content, the business models of the old\n  medium\u2014which fails, and then the more appropriate models get\n  figured out.\n\nIt may have seemed as if not much was happening during the years\nafter the Bubble burst.  But in retrospect, something was happening:\nthe web was finding its natural angle of repose.  The democracy \ncomponent, for example\u2014that's not an innovation, in the sense of\nsomething someone made happen.  That's what the web naturally tends\nto produce.Ditto for the idea of delivering desktop-like applications over the\nweb.  That idea is almost as old as the web.  But the first time    \naround it was co-opted by Sun, and we got Java applets.  Java has\nsince been remade into a generic replacement for C++, but in 1996\nthe story about Java was that it represented a new model of software.\nInstead of desktop applications, you'd run Java \"applets\" delivered\nfrom a server.This plan collapsed under its own weight. Microsoft helped kill it,\nbut it would have died anyway.  There was no uptake among hackers.\nWhen you find PR firms promoting\nsomething as the next development platform, you can be sure it's\nnot.  If it were, you wouldn't need PR firms to tell you, because   \nhackers would already be writing stuff on top of it, the way sites    \nlike Busmonster used Google Maps as a\nplatform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of  \nhackers have spontaneously started building things on top\nof it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common.\nHere's a clue.  Suppose you approached investors with the following\nidea for a Web 2.0 startup:\n\n  Sites like del.icio.us and flickr allow users to \"tag\" content\n  with descriptive tokens.  But there is also huge source of\n  implicit tags that they ignore: the text within web links.\n  Moreover, these links represent a social network connecting the   \n  individuals and organizations who created the pages, and by using\n  graph theory we can compute from this network an estimate of the\n  reputation of each member.  We plan to mine the web for these \n  implicit tags, and use them together with the reputation hierarchy\n  they embody to enhance web searches.\n\nHow long do you think it would take them on average to realize that\nit was a description of Google?Google was a pioneer in all three components of Web 2.0: their core\nbusiness sounds crushingly hip when described in Web 2.0 terms, \n\"Don't maltreat users\" is a subset of \"Don't be evil,\" and of course\nGoogle set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google\ndoes.  That's their secret.    They're sailing with the wind, instead of sitting  \nbecalmed praying for a business model, like the print media, or   \ntrying to tack upwind by suing their customers, like Microsoft and \nthe record labels.\n[7]Google doesn't try to force things to happen their way.  They try   \nto figure out what's going to happen, and arrange to be standing \nthere when it does.  That's the way to approach technology\u2014and \nas business includes an ever larger technological component, the\nright way to do business.The fact that Google is a \"Web 2.0\" company shows that, while\nmeaningful, the term is also rather bogus.  It's like the word\n\"allopathic.\"  It just means doing things right, and it's a bad   \nsign when you have a special word for that.\nNotes[1]\nFrom the conference\nsite, June 2004: \"While the first wave of the Web was closely  \ntied to the browser, the second wave extends applications across    \nthe web and enables a new generation of services and business\nopportunities.\"  To the extent this means anything, it seems to be\nabout \nweb-based applications.[2]\nDisclosure: Reddit was funded by \nY Combinator.  But although\nI started using it out of loyalty to the home team, I've become a\ngenuine addict.  While we're at it, I'm also an investor in\n!MSFT, having sold all my shares earlier this year.[3]\nI'm not against editing. I spend more time editing than\nwriting, and I have a group of picky friends who proofread almost\neverything I write.  What I dislike is editing done after the fact  \nby someone else.[4]\nObvious is an understatement.  Users had been climbing in through  \nthe window for years before Apple finally moved the door.[5]\nHint: the way to create a web-based alternative to Office may\nnot be to write every component yourself, but to establish a protocol\nfor web-based apps to share a virtual home directory spread across\nmultiple servers.  Or it may be to write it all yourself.[6]\nIn Jessica Livingston's\nFounders at\nWork.[7]\nMicrosoft didn't sue their customers directly, but they seem \nto have done all they could to help SCO sue them.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, Peter\nNorvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the\nguys at O'Reilly and Adaptive Path for answering my questions.April 2012A palliative care nurse called Bronnie Ware made a list of the\nbiggest regrets\nof the dying.  Her list seems plausible.  I could see\nmyself \u2014 can see myself \u2014 making at least 4 of these\n5 mistakes.If you had to compress them into a single piece of advice, it might\nbe: don't be a cog.  The 5 regrets paint a portrait of post-industrial\nman, who shrinks himself into a shape that fits his circumstances,\nthen turns dutifully till he stops.The alarming thing is, the mistakes that produce these regrets are\nall errors of omission.  You forget your dreams, ignore your family,\nsuppress your feelings, neglect your friends, and forget to be\nhappy.  Errors of omission are a particularly dangerous type of\nmistake, because you make them by default.I would like to avoid making these mistakes.  But how do you avoid\nmistakes you make by default?  Ideally you transform your life so\nit has other defaults.  But it may not be possible to do that\ncompletely. As long as these mistakes happen by default, you probably\nhave to be reminded not to make them.  So I inverted the 5 regrets,\nyielding a list of 5 commands\n\n   Don't ignore your dreams; don't work too much; say what you\n   think; cultivate friendships; be happy.\n\nwhich I then put at the top of the file I use as a todo list.December 2014I've read Villehardouin's chronicle of the Fourth Crusade at least\ntwo times, maybe three.  And yet if I had to write down everything\nI remember from it, I doubt it would amount to much more than a\npage.  Multiply this times several hundred, and I get an uneasy\nfeeling when I look at my bookshelves. What use is it to read all\nthese books if I remember so little from them?A few months ago, as I was reading Constance Reid's excellent\nbiography of Hilbert, I figured out if not the answer to this\nquestion, at least something that made me feel better about it.\nShe writes:\n\n  Hilbert had no patience with mathematical lectures which filled\n  the students with facts but did not teach them how to frame a\n  problem and solve it. He often used to tell them that \"a perfect\n  formulation of a problem is already half its solution.\"\n\nThat has always seemed to me an important point, and I was even\nmore convinced of it after hearing it confirmed by Hilbert.But how had I come to believe in this idea in the first place?  A\ncombination of my own experience and other things I'd read.  None\nof which I could at that moment remember!  And eventually I'd forget\nthat Hilbert had confirmed it too.  But my increased belief in the\nimportance of this idea would remain something I'd learned from\nthis book, even after I'd forgotten I'd learned it.Reading and experience train your model of the world.  And even if\nyou forget the experience or what you read, its effect on your model\nof the world persists.  Your mind is like a compiled program you've\nlost the source of.  It works, but you don't know why.The place to look for what I learned from Villehardouin's chronicle\nis not what I remember from it, but my mental models of the crusades,\nVenice, medieval culture, siege warfare, and so on.  Which doesn't\nmean I couldn't have read more attentively, but at least the harvest\nof reading is not so miserably small as it might seem.This is one of those things that seem obvious in retrospect.  But\nit was a surprise to me and presumably would be to anyone else who\nfelt uneasy about (apparently) forgetting so much they'd read.Realizing it does more than make you feel a little better about\nforgetting, though.  There are specific implications.For example, reading and experience are usually \"compiled\" at the\ntime they happen, using the state of your brain at that time.  The\nsame book would get compiled differently at different points in\nyour life.  Which means it is very much worth reading important\nbooks multiple times.  I always used to feel some misgivings about\nrereading books.  I unconsciously lumped reading together with work\nlike carpentry, where having to do something again is a sign you\ndid it wrong the first time.  Whereas now the phrase \"already read\"\nseems almost ill-formed.Intriguingly, this implication isn't limited to books.  Technology\nwill increasingly make it possible to relive our experiences.  When\npeople do that today it's usually to enjoy them again (e.g. when\nlooking at pictures of a trip) or to find the origin of some bug in\ntheir compiled code (e.g. when Stephen Fry succeeded in remembering\nthe childhood trauma that prevented him from singing).  But as\ntechnologies for recording and playing back your life improve, it\nmay become common for people to relive experiences without any goal\nin mind, simply to learn from them again as one might when rereading\na book.Eventually we may be able not just to play back experiences but\nalso to index and even edit them. So although not knowing how you\nknow things may seem part of being human, it may not be.\nThanks to Sam Altman, Jessica Livingston, and Robert Morris for reading \ndrafts of this.September 2007In high school I decided I was going to study philosophy in college.\nI had several motives, some more honorable than others.  One of the\nless honorable was to shock people.  College was regarded as job\ntraining where I grew up, so studying philosophy seemed an impressively\nimpractical thing to do.  Sort of like slashing holes in your clothes\nor putting a safety pin through your ear, which were other forms\nof impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying\nphilosophy would be a shortcut straight to wisdom.  All the people\nmajoring in other things would just end up with a bunch of domain\nknowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you\nwouldn't find those in our high school library.  But I tried to\nread Plato and Aristotle.  I doubt I believed I understood them,\nbut they sounded like they were talking about something important.\nI assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned\na lot in the calculus class, but I didn't learn much in Philosophy\n101.  And yet my plan to study philosophy remained intact.  It was\nmy fault I hadn't learned anything.  I hadn't read the books we\nwere assigned carefully enough.  I'd give Berkeley's Principles\nof Human Knowledge another shot in college.  Anything so admired\nand so difficult to read must have something in it, if one could\nonly figure out what.Twenty-six years later, I still don't understand Berkeley.  I have\na nice edition of his collected works.  Will I ever read it?  Seems\nunlikely.The difference between then and now is that now I understand why\nBerkeley is probably not worth trying to understand.  I think I see\nnow what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It\ndidn't work out as I'd hoped.  I didn't learn any magical truths\ncompared to which everything else was mere domain knowledge.  But\nI do at least know now why I didn't.  Philosophy doesn't really\nhave a subject matter in the way math or history or most other\nuniversity subjects do.  There is no core of knowledge one must\nmaster.  The closest you come to that is a knowledge of what various\nindividual philosophers have said about different topics over the\nyears.  Few were sufficiently correct that people have forgotten\nwho discovered what they discovered.Formal logic has some subject matter. I took several classes in\nlogic.  I don't know if I learned anything from them.\n[1]\nIt does seem to me very important to be able to flip ideas around in\none's head: to see when two ideas don't fully cover the space of\npossibilities, or when one idea is the same as another but with a\ncouple things changed.  But did studying logic teach me the importance\nof thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The\nmost dramatic I learned immediately, in the first semester of\nfreshman year, in a class taught by Sydney Shoemaker.  I learned\nthat I don't exist.  I am (and you are) a collection of cells that\nlurches around driven by various forces, and calls itself I.  But\nthere's no central, indivisible thing that your identity goes with.\nYou could conceivably lose half your brain and live.  Which means\nyour brain could conceivably be split into two halves and each\ntransplanted into different bodies.  Imagine waking up after such\nan operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life\nare fuzzy, and break down if pushed too hard.  Even a concept as\ndear to us as I.  It took me a while to grasp this, but when I\ndid it was fairly sudden, like someone in the nineteenth century\ngrasping evolution and realizing the story of creation they'd been\ntold as a child was all wrong. \n[2]\nOutside of math there's a limit\nto how far you can push words; in fact, it would not be a bad\ndefinition of math to call it the study of terms that have precise\nmeanings.  Everyday words are inherently imprecise.  They work well\nenough in everyday life that you don't notice.  Words seem to work,\njust as Newtonian physics seems to.  But you can always make them\nbreak if you push them far enough.I would say that this has been, unfortunately for philosophy, the\ncentral fact of philosophy.  Most philosophical debates are not\nmerely afflicted by but driven by confusions over words.  Do we\nhave free will?  Depends what you mean by \"free.\" Do abstract ideas\nexist?  Depends what you mean by \"exist.\"Wittgenstein is popularly credited with the idea that most philosophical\ncontroversies are due to confusions over language.  I'm not sure\nhow much credit to give him.  I suspect a lot of people realized\nthis, but reacted simply by not studying philosophy, rather than\nbecoming philosophy professors.How did things get this way?  Can something people have spent\nthousands of years studying really be a waste of time?  Those are\ninteresting questions.  In fact, some of the most interesting\nquestions you can ask about philosophy.  The most valuable way to\napproach the current philosophical tradition may be neither to get\nlost in pointless speculations like Berkeley, nor to shut them down\nlike Wittgenstein, but to study it as an example of reason gone\nwrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle.\nWhat we know of their predecessors comes from fragments and references\nin later works; their doctrines could be described as speculative\ncosmology that occasionally strays into analysis.  Presumably they\nwere driven by whatever makes people in every other society invent\ncosmologies.\n[3]With Socrates, Plato, and particularly Aristotle, this tradition\nturned a corner.  There started to be a lot more analysis.  I suspect\nPlato and Aristotle were encouraged in this by progress in math.\nMathematicians had by then shown that you could figure things out\nin a much more conclusive way than by making up fine sounding stories\nabout them.  \n[4]People talk so much about abstractions now that we don't realize\nwhat a leap it must have been when they first started to.  It was\npresumably many thousands of years between when people first started\ndescribing things as hot or cold and when someone asked \"what is\nheat?\"  No doubt it was a very gradual process.  We don't know if\nPlato or Aristotle were the first to ask any of the questions they\ndid.  But their works are the oldest we have that do this on a large\nscale, and there is a freshness (not to say naivete) about them\nthat suggests some of the questions they asked were new to them,\nat least.Aristotle in particular reminds me of the phenomenon that happens\nwhen people discover something new, and are so excited by it that\nthey race through a huge percentage of the newly discovered territory\nin one lifetime.  If so, that's evidence of how new this kind of\nthinking was. \n[5]This is all to explain how Plato and Aristotle can be very impressive\nand yet naive and mistaken.  It was impressive even to ask the\nquestions they did.  That doesn't mean they always came up with\ngood answers.  It's not considered insulting to say that ancient\nGreek mathematicians were naive in some respects, or at least lacked\nsome concepts that would have made their lives easier.  So I hope\npeople will not be too offended if I propose that ancient philosophers\nwere similarly naive.  In particular, they don't seem to have fully\ngrasped what I earlier called the central fact of philosophy: that\nwords break if you push them too far.\"Much to the surprise of the builders of the first digital computers,\"\nRod Brooks wrote, \"programs written for them usually did not work.\"\n[6]\nSomething similar happened when people first started trying\nto talk about abstractions.  Much to their surprise, they didn't\narrive at answers they agreed upon.  In fact, they rarely seemed\nto arrive at answers at all.They were in effect arguing about artifacts induced by sampling at\ntoo low a resolution.The proof of how useless some of their answers turned out to be is\nhow little effect they have.  No one after reading Aristotle's\nMetaphysics does anything differently as a result.\n[7]Surely I'm not claiming that ideas have to have practical applications\nto be interesting?  No, they may not have to.  Hardy's boast that\nnumber theory had no use whatsoever wouldn't disqualify it.  But\nhe turned out to be mistaken.  In fact, it's suspiciously hard to\nfind a field of math that truly has no practical use.  And Aristotle's\nexplanation of the ultimate goal of philosophy in Book A of the\nMetaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.\nThe examples he gives are convincing: an ordinary worker builds\nthings a certain way out of habit; a master craftsman can do more\nbecause he grasps the underlying principles.  The trend is clear:\nthe more general the knowledge, the more admirable it is.  But then\nhe makes a mistake\u2014possibly the most important mistake in the\nhistory of philosophy.  He has noticed that theoretical knowledge\nis often acquired for its own sake, out of curiosity, rather than\nfor any practical need.  So he proposes there are two kinds of\ntheoretical knowledge: some that's useful in practical matters and\nsome that isn't.  Since people interested in the latter are interested\nin it for its own sake, it must be more noble.  So he sets as his\ngoal in the Metaphysics the exploration of knowledge that has no\npractical use.  Which means no alarms go off when he takes on grand\nbut vaguely understood questions and ends up getting lost in a sea\nof words.His mistake was to confuse motive and result.  Certainly, people\nwho want a deep understanding of something are often driven by\ncuriosity rather than any practical need.  But that doesn't mean\nwhat they end up learning is useless.  It's very valuable in practice\nto have a deep understanding of what you're doing; even if you're\nnever called on to solve advanced problems, you can see shortcuts\nin the solution of simple ones, and your knowledge won't break down\nin edge cases, as it would if you were relying on formulas you\ndidn't understand.  Knowledge is power.  That's what makes theoretical\nknowledge prestigious.  It's also what causes smart people to be\ncurious about certain things and not others; our DNA is not so\ndisinterested as we might think.So while ideas don't have to have immediate practical applications\nto be interesting, the kinds of things we find interesting will\nsurprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was\npartly that he set off with contradictory aims: to explore the most\nabstract ideas, guided by the assumption that they were useless.\nHe was like an explorer looking for a territory to the north of\nhim, starting with the assumption that it was located to the south.And since his work became the map used by generations of future\nexplorers, he sent them off in the wrong direction as well. \n[8]\nPerhaps worst of all, he protected them from both the criticism of\noutsiders and the promptings of their own inner compass by establishing\nthe principle that the most noble sort of theoretical knowledge had\nto be useless.The Metaphysics is mostly a failed experiment.  A few ideas from\nit turned out to be worth keeping; the bulk of it has had no effect\nat all.  The Metaphysics is among the least read of all famous\nbooks.  It's not hard to understand the way Newton's Principia\nis, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately\nthat was not the conclusion Aristotle's successors derived from\nworks like the Metaphysics. \n[9]\nSoon after, the western world\nfell on intellectual hard times.  Instead of version 1s to be\nsuperseded, the works of Plato and Aristotle became revered texts\nto be mastered and discussed.  And so things remained for a shockingly\nlong time.  It was not till around 1600 (in Europe, where the center\nof gravity had shifted by then) that one found people confident\nenough to treat Aristotle's work as a catalog of mistakes.  And\neven then they rarely said so outright.If it seems surprising that the gap was so long, consider how little\nprogress there was in math between Hellenistic times and the\nRenaissance.In the intervening years an unfortunate idea took hold:  that it\nwas not only acceptable to produce works like the Metaphysics,\nbut that it was a particularly prestigious line of work, done by a\nclass of people called philosophers.  No one thought to go back and\ndebug Aristotle's motivating argument.  And so instead of correcting\nthe problem Aristotle discovered by falling into it\u2014that you can\neasily get lost if you talk too loosely about very abstract ideas\u2014they \ncontinued to fall into it.The SingularityCuriously, however, the works they produced continued to attract\nnew readers.  Traditional philosophy occupies a kind of singularity\nin this respect.  If you write in an unclear way about big ideas,\nyou produce something that seems tantalizingly attractive to\ninexperienced but intellectually ambitious students.  Till one knows\nbetter, it's hard to distinguish something that's hard to understand\nbecause the writer was unclear in his own mind from something like\na mathematical proof that's hard to understand because the ideas\nit represents are hard to understand.  To someone who hasn't learned\nthe difference, traditional philosophy seems extremely attractive:\nas hard (and therefore impressive) as math, yet broader in scope.\nThat was what lured me in as a high school student.This singularity is even more singular in having its own defense\nbuilt in.  When things are hard to understand, people who suspect\nthey're nonsense generally keep quiet.  There's no way to prove a\ntext is meaningless.  The closest you can get is to show that the\nofficial judges of some class of texts can't distinguish them from\nplacebos. \n[10]And so instead of denouncing philosophy, most people who suspected\nit was a waste of time just studied other things.  That alone is\nfairly damning evidence, considering philosophy's claims.  It's\nsupposed to be about the ultimate truths. Surely all smart people\nwould be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might\nhave corrected them, they tended to be self-perpetuating.  Bertrand\nRussell wrote in a letter in 1912:\n\n  Hitherto the people attracted to philosophy have been mostly those\n  who loved the big generalizations, which were all wrong, so that\n  few people with exact minds have taken up the subject.\n[11]\n\nHis response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery\nthat most previous philosophy was a waste of time, which judging\nfrom the circumstantial evidence must have been made by every smart\nperson who studied a little philosophy and declined to pursue it\nfurther, but for how he acted in response.\n[12]\nInstead of quietly\nswitching to another field, he made a fuss, from inside.  He was\nGorbachev.The field of philosophy is still shaken from the fright Wittgenstein\ngave it. \n[13]\nLater in life he spent a lot of time talking about\nhow words worked.  Since that seems to be allowed, that's what a\nlot of philosophers do now.  Meanwhile, sensing a vacuum in the\nmetaphysical speculation department, the people who used to do\nliterary criticism have been edging Kantward, under new names like\n\"literary theory,\" \"critical theory,\" and when they're feeling\nambitious, plain \"theory.\"  The writing is the familiar word salad:\n\n  Gender is not like some of the other grammatical modes which\n  express precisely a mode of conception without any reality that\n  corresponds to the conceptual mode, and consequently do not express\n  precisely something in reality by which the intellect could be\n  moved to conceive a thing the way it does, even where that motive\n  is not something in the thing as such.\n  [14]\n\nThe singularity I've described is not going away.  There's a market\nfor writing that sounds impressive and can't be disproven. There\nwill always be both supply and demand.  So if one group abandons\nthis territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility.\nPerhaps we should do what Aristotle meant to do, instead of what\nhe did.  The goal he announces in the Metaphysics seems one worth\npursuing: to discover the most general truths.  That sounds good.\nBut instead of trying to discover them because they're useless,\nlet's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised\ncriterion, applicability, as a guide to keep us from wondering\noff into a swamp of abstractions.  Instead of trying to answer the\nquestion:\n\n  What are the most general truths?\n\nlet's try to answer the question\n\n  Of all the useful things we can say, which are the most general?\n\nThe test of utility I propose is whether we cause people who read\nwhat we've written to do anything differently afterward.  Knowing\nwe have to give definite (if implicit) advice will keep us from\nstraying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a\ndifferent direction.As an example of a useful, general idea, consider that of the\ncontrolled experiment.  There's an idea that has turned out to be\nwidely applicable.  Some might say it's part of science, but it's\nnot part of any specific science; it's literally meta-physics (in\nour sense of \"meta\").   The idea of evolution is another. It turns\nout to have quite broad applications\u2014for example, in genetic\nalgorithms and even product design.  Frankfurt's distinction between\nlying and bullshitting seems a promising recent example.\n[15]These seem to me what philosophy should look like: quite general\nobservations that would cause someone who understood them to do\nsomething differently.Such observations will necessarily be about things that are imprecisely\ndefined.  Once you start using words with precise meanings, you're\ndoing math.  So starting from utility won't entirely solve the\nproblem I described above\u2014it won't flush out the metaphysical\nsingularity.  But it should help.  It gives people with good\nintentions a new roadmap into abstraction.  And they may thereby\nproduce things that make the writing of the people with bad intentions\nlook bad by comparison.One drawback of this approach is that it won't produce the sort of\nwriting that gets you tenure.  And not just because it's not currently\nthe fashion.  In order to get tenure in any field you must not\narrive at conclusions that members of tenure committees can disagree\nwith.  In practice there are two kinds of solutions to this problem.\nIn math and the sciences, you can prove what you're saying, or at\nany rate adjust your conclusions so you're not claiming anything\nfalse (\"6 of 8 subjects had lower blood pressure after the treatment\").\nIn the humanities you can either avoid drawing any definite conclusions\n(e.g. conclude that an issue is a complex one), or draw conclusions\nso narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either\nof these routes.  At best you'll be able to achieve the essayist's\nstandard of proof, not the mathematician's or the experimentalist's.\nAnd yet you won't be able to meet the usefulness test without\nimplying definite and fairly broadly applicable conclusions.  Worse\nstill, the usefulness test will tend to produce results that annoy\npeople: there's no use in telling people things they already believe,\nand people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting\nto general plus useful by starting with useful and cranking up the\ngenerality may be unsuitable for junior professors trying to get\ntenure, but it's better for everyone else, including professors who\nalready have it.  This side of the mountain is a nice gradual slope.\nYou can start by writing things that are useful but very specific,\nand then gradually make them more general.  Joe's has good burritos.\nWhat makes a good burrito?  What makes good food?  What makes\nanything good?  You can take as long as you want.  You don't have\nto get all the way to the top of the mountain.  You don't have to\ntell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an\nencouraging thought.  The field is a lot younger than it seems.\nThough the first philosophers in the western tradition lived about\n2500 years ago, it would be misleading to say the field is 2500\nyears old, because for most of that time the leading practitioners\nweren't doing much more than writing commentaries on Plato or\nAristotle while watching over their shoulders for the next invading\narmy.  In the times when they weren't, philosophy was hopelessly\nintermingled with religion.  It didn't shake itself free till a\ncouple hundred years ago, and even then was afflicted by the\nstructural problems I've described above.  If I say this, some will\nsay it's a ridiculously overbroad and uncharitable generalization,\nand others will say it's old news, but here goes: judging from their\nworks, most philosophers up to the present have been wasting their\ntime.  So in a sense the field is still at the first step. \n[16]That sounds a preposterous claim to make.  It won't seem so\npreposterous in 10,000 years.  Civilization always seems old, because\nit's always the oldest it's ever been.  The only way to say whether\nsomething is really old or not is by looking at structural evidence,\nand structurally philosophy is young; it's still reeling from the\nunexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot\nmore to discover.Notes\n[1]\nIn practice formal logic is not much use, because despite\nsome progress in the last 150 years we're still only able to formalize\na small percentage of statements.  We may never do that much better,\nfor the same reason 1980s-style \"knowledge representation\" could\nnever have worked; many statements may have no representation more\nconcise than a huge, analog brain state.[2]\nIt was harder for Darwin's contemporaries to grasp this than\nwe can easily imagine.  The story of creation in the Bible is not\njust a Judeo-Christian concept; it's roughly what everyone must\nhave believed since before people were people.  The hard part of\ngrasping evolution was to realize that species weren't, as they\nseem to be, unchanging, but had instead evolved from different,\nsimpler organisms over unimaginably long periods of time.Now we don't have to make that leap.  No one in an industrialized\ncountry encounters the idea of evolution for the first time as an\nadult.  Everyone's taught about it as a child, either as truth or\nheresy.[3]\nGreek philosophers before Plato wrote in verse.  This must\nhave affected what they said.  If you try to write about the nature\nof the world in verse, it inevitably turns into incantation.  Prose\nlets you be more precise, and more tentative.[4]\nPhilosophy is like math's\nne'er-do-well brother.  It was born when Plato and Aristotle looked\nat the works of their predecessors and said in effect \"why can't\nyou be more like your brother?\"  Russell was still saying the same\nthing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy\nthe imprecise half.  It's probably inevitable that philosophy will\nsuffer by comparison, because there's no lower bound to its precision.\nBad math is merely boring, whereas bad philosophy is nonsense.  And\nyet there are some good ideas in the imprecise half.[5]\nAristotle's best work was in logic and zoology, both of which\nhe can  be said to have invented.  But the most dramatic departure\nfrom his predecessors was a new, much more analytical style of\nthinking.  He was arguably the first scientist.[6]\nBrooks, Rodney, Programming in Common Lisp, Wiley, 1985, p.\n94.[7]\nSome would say we depend on Aristotle more than we realize,\nbecause his ideas were one of the ingredients in our common culture.\nCertainly a lot of the words we use have a connection with Aristotle,\nbut it seems a bit much to suggest that we wouldn't have the concept\nof the essence of something or the distinction between matter and\nform if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to\ndiff European culture with Chinese: what ideas did European culture\nhave in 1800 that Chinese culture didn't, in virtue of Aristotle's\ncontribution?[8]\nThe meaning of the word \"philosophy\" has changed over time.\nIn ancient times it covered a broad range of topics, comparable in\nscope to our \"scholarship\" (though without the methodological\nimplications).  Even as late as Newton's time it included what we\nnow call \"science.\"  But core of the subject today is still what\nseemed to Aristotle the core: the attempt to discover the most\ngeneral truths.Aristotle didn't call this \"metaphysics.\"  That name got assigned\nto it because the books we now call the Metaphysics came after\n(meta = after) the Physics in the standard edition of Aristotle's\nworks compiled by Andronicus of Rhodes three centuries later.  What\nwe call \"metaphysics\" Aristotle called \"first philosophy.\"[9]\nSome of Aristotle's immediate successors may have realized\nthis, but it's hard to say because most of their works are lost.[10]\nSokal, Alan, \"Transgressing the Boundaries: Toward a Transformative\nHermeneutics of Quantum Gravity,\" Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's\naligned with some axe the audience already has to grind.  If this\nis so we should find it's most popular with groups that are (or\nfeel) weak.  The powerful don't need its reassurance.[11]\nLetter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991,\np. 75.[12]\nA preliminary result, that all metaphysics between Aristotle\nand 1783 had been a waste of time, is due to I. Kant.[13]\nWittgenstein asserted a sort of mastery to which the inhabitants\nof early 20th century Cambridge seem to have been peculiarly\nvulnerable\u2014perhaps partly because so many had been raised religious\nand then stopped believing, so had a vacant space in their heads\nfor someone to tell them what to do (others chose Marx or Cardinal\nNewman), and partly because a quiet, earnest place like Cambridge\nin that era had no natural immunity to messianic figures, just as\nEuropean politics then had no natural immunity to dictators.[14]\nThis is actually from the Ordinatio of Duns Scotus (ca.\n1300), with \"number\" replaced by \"gender.\"  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson,\n1963, p. 92.[15]\nFrankfurt, Harry, On Bullshit,  Princeton University Press,\n2005.[16]\nSome introductions to philosophy now take the line that\nphilosophy is worth studying as a process rather than for any\nparticular truths you'll learn.  The philosophers whose works they\ncover would be rolling in their graves at that.  They hoped they\nwere doing more than serving as examples of how to argue: they hoped\nthey were getting results.  Most were wrong, but it doesn't seem\nan impossible hope.This argument seems to me like someone in 1500 looking at the lack\nof results achieved by alchemy and saying its value was as a process.\nNo, they were going about it wrong.  It turns out it is possible\nto transmute lead into gold (though not economically at current\nenergy prices), but the route to that knowledge was to\nbacktrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston, \nRobert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.April 2005\"Suits make a corporate comeback,\" says the New\nYork Times.  Why does this sound familiar?  Maybe because\nthe suit was also back in February,\n\nSeptember\n2004, June\n2004, March\n2004, September\n2003, \n\nNovember\n2002, \nApril 2002,\nand February\n2002.\n\nWhy do the media keep running stories saying suits are back?  Because\nPR firms tell \nthem to.  One of the most surprising things I discovered\nduring my brief business career was the existence of the PR industry,\nlurking like a huge, quiet submarine beneath the news.  Of the\nstories you read in traditional media that aren't about politics,\ncrimes, or disasters, more than half probably come from PR firms.I know because I spent years hunting such \"press hits.\"  Our startup spent\nits entire marketing budget on PR: at a time when we were assembling\nour own computers to save money, we were paying a PR firm $16,000\na month.  And they were worth it.  PR is the news equivalent of\nsearch engine optimization; instead of buying ads, which readers\nignore, you get yourself inserted directly into the stories.  [1]Our PR firm\nwas one of the best in the business.  In 18 months, they got press\nhits in over 60 different publications.  \nAnd we weren't the only ones they did great things for.  \nIn 1997 I got a call from another\nstartup founder considering hiring them to promote his company.  I\ntold him they were PR gods, worth every penny of their outrageous   \nfees.  But I remember thinking his company's name was odd.\nWhy call an auction site \"eBay\"?\nSymbiosisPR is not dishonest.  Not quite.  In fact, the reason the best PR\nfirms are so effective is precisely that they aren't dishonest.\nThey give reporters genuinely valuable information.  A good PR firm\nwon't bug reporters just because the client tells them to; they've\nworked hard to build their credibility with reporters, and they\ndon't want to destroy it by feeding them mere propaganda.If anyone is dishonest, it's the reporters.  The main reason PR  \nfirms exist is that reporters are lazy.  Or, to put it more nicely,\noverworked.  Really they ought to be out there digging up stories\nfor themselves.  But it's so tempting to sit in their offices and\nlet PR firms bring the stories to them.  After all, they know good\nPR firms won't lie to them.A good flatterer doesn't lie, but tells his victim selective truths\n(what a nice color your eyes are). Good PR firms use the same\nstrategy: they give reporters stories that are true, but whose truth\nfavors their clients.For example, our PR firm often pitched stories about how the Web  \nlet small merchants compete with big ones.  This was perfectly true.\nBut the reason reporters ended up writing stories about this\nparticular truth, rather than some other one, was that small merchants\nwere our target market, and we were paying the piper.Different publications vary greatly in their reliance on PR firms.\nAt the bottom of the heap are the trade press, who make most of\ntheir money from advertising and would give the magazines away for\nfree if advertisers would let them.  [2] The average\ntrade publication is a  bunch of ads, glued together by just enough\narticles to make it look like a magazine.  They're so desperate for\n\"content\" that some will print your press releases almost verbatim,\nif you take the trouble to write them to read like articles.At the other extreme are publications like the New York Times\nand the Wall Street Journal.  Their reporters do go out and\nfind their own stories, at least some of the time.  They'll listen \nto PR firms, but briefly and skeptically.  We managed to get press   \nhits in almost every publication we wanted, but we never managed \nto crack the print edition of the Times.  [3]The weak point of the top reporters is not laziness, but vanity.\nYou don't pitch stories to them.  You have to approach them as if\nyou were a specimen under their all-seeing microscope, and make it\nseem as if the story you want them to run is something they thought \nof themselves.Our greatest PR coup was a two-part one.  We estimated, based on\nsome fairly informal math, that there were about 5000 stores on the\nWeb.  We got one paper to print this number, which seemed neutral   \nenough.  But once this \"fact\" was out there in print, we could quote\nit to other publications, and claim that with 1000 users we had 20%\nof the online store market.This was roughly true.  We really did have the biggest share of the\nonline store market, and 5000 was our best guess at its size.  But\nthe way the story appeared in the press sounded a lot more definite.Reporters like definitive statements.  For example, many of the\nstories about Jeremy Jaynes's conviction say that he was one of the\n10 worst spammers.  This \"fact\" originated in Spamhaus's ROKSO list,\nwhich I think even Spamhaus would admit is a rough guess at the top\nspammers.  The first stories about Jaynes cited this source, but\nnow it's simply repeated as if it were part of the indictment.   \n[4]All you can say with certainty about Jaynes is that he was a fairly\nbig spammer.  But reporters don't want to print vague stuff like\n\"fairly big.\"  They want statements with punch, like \"top ten.\" And\nPR firms give them what they want.\nWearing suits, we're told, will make us \n3.6\npercent more productive.BuzzWhere the work of PR firms really does get deliberately misleading is in\nthe generation of \"buzz.\"  They usually feed the same story to    \nseveral different publications at once.  And when readers see similar\nstories in multiple places, they think there is some important trend\nafoot.  Which is exactly what they're supposed to think.When Windows 95 was launched, people waited outside stores\nat midnight to buy the first copies.  None of them would have been\nthere without PR firms, who generated such a buzz in\nthe news media that it became self-reinforcing, like a nuclear chain\nreaction.I doubt PR firms realize it yet, but the Web makes it possible to  \ntrack them at work.  If you search for the obvious phrases, you\nturn up several efforts over the years to place stories about the  \nreturn of the suit.  For example, the Reuters article \n\nthat got picked up by USA\nToday in September 2004.  \"The suit is back,\" it begins.Trend articles like this are almost always the work of\nPR firms.  Once you know how to read them, it's straightforward to\nfigure out who the client is.  With trend stories, PR firms usually\nline up one or more \"experts\" to talk about the industry generally. \nIn this case we get three: the NPD Group, the creative director of\nGQ, and a research director at Smith Barney.\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n  [5] When\nyou get to the end of the experts, look for the client. And bingo, \nthere it is: The Men's Wearhouse.Not surprising, considering The Men's Wearhouse was at that moment \nrunning ads saying \"The Suit is Back.\"  Talk about a successful\npress hit-- a wire service article whose first sentence is your own\nad copy.The secret to finding other press hits from a given pitch\nis to realize that they all started from the same document back at\nthe PR firm.  Search for a few key phrases and the names of the\nclients and the experts, and you'll turn up other variants of this \nstory.Casual\nfridays are out and dress codes are in writes Diane E. Lewis\nin The Boston Globe.  In a remarkable coincidence, Ms. Lewis's\nindustry contacts also include the creative director of GQ.Ripped jeans and T-shirts are out, writes Mary Kathleen Flynn in\nUS News & World Report.  And she too knows the \ncreative director of GQ.Men's suits\nare back writes Nicole Ford in Sexbuzz.Com (\"the ultimate men's\nentertainment magazine\").Dressing\ndown loses appeal as men suit up at the office writes Tenisha\nMercer of The Detroit News.\nNow that so many news articles are online, I suspect you could find\na similar pattern for most trend stories placed by PR firms.  I\npropose we call this new sport \"PR diving,\" and I'm sure there are\nfar more striking examples out there than this clump of five stories.OnlineAfter spending years chasing them, it's now second nature\nto me to recognize press hits for what they are.  But before we\nhired a PR firm I had no idea where articles in the mainstream media\ncame from.  I could tell a lot of them were crap, but I didn't\nrealize why.Remember the exercises in critical reading you did in school, where\nyou had to look at a piece of writing and step back and ask whether\nthe author was telling the whole truth?  If you really want to be\na critical reader, it turns out you have to step back one step\nfurther, and ask not just whether the author is telling the truth,\nbut why he's writing about this subject at all.Online, the answer tends to be a lot simpler.  Most people who\npublish online write what they write for the simple reason that\nthey want to.  You\ncan't see the fingerprints of PR firms all over the articles, as\nyou can in so many print publications-- which is one of the reasons,\nthough they may not consciously realize it, that readers trust\nbloggers more than Business Week.I was talking recently to a friend who works for a\nbig newspaper.  He thought the print media were in serious trouble,\nand that they were still mostly in denial about it.  \"They think\nthe decline is cyclic,\" he said.  \"Actually it's structural.\"In other words, the readers are leaving, and they're not coming\nback.\nWhy? I think the main reason is that the writing online is more honest.\nImagine how incongruous the New York Times article about\nsuits would sound if you read it in a blog:\n   The urge to look corporate-- sleek, commanding,\n  prudent, yet with just a touch of hubris on your well-cut sleeve--\n  is an unexpected development in a time of business disgrace.\n   \nThe problem\nwith this article is not just that it originated in a PR firm.\nThe whole tone is bogus.  This is the tone of someone writing down\nto their audience.Whatever its flaws, the writing you find online\nis authentic.  It's not mystery meat cooked up\nout of scraps of pitch letters and press releases, and pressed into \nmolds of zippy\njournalese.  It's people writing what they think.I didn't realize, till there was an alternative, just how artificial\nmost of the writing in the mainstream media was.  I'm not saying\nI used to believe what I read in Time and Newsweek.  Since high\nschool, at least, I've thought of magazines like that more as\nguides to what ordinary people were being\ntold to think than as  \nsources of information.  But I didn't realize till the last  \nfew years that writing for publication didn't have to mean writing\nthat way.  I didn't realize you could write as candidly and\ninformally as you would if you were writing to a friend.Readers aren't the only ones who've noticed the\nchange.  The PR industry has too.\nA hilarious article\non the site of the PR Society of America gets to the heart of the   \nmatter:\n   Bloggers are sensitive about becoming mouthpieces\n  for other organizations and companies, which is the reason they\n  began blogging in the first place.  \nPR people fear bloggers for the same reason readers\nlike them.  And that means there may be a struggle ahead.  As\nthis new kind of writing draws readers away from traditional media, we\nshould be prepared for whatever PR mutates into to compensate.  \nWhen I think   \nhow hard PR firms work to score press hits in the traditional   \nmedia, I can't imagine they'll work any less hard to feed stories\nto bloggers, if they can figure out how.\nNotes[1] PR has at least   \none beneficial feature: it favors small companies.  If PR didn't  \nwork, the only alternative would be to advertise, and only big\ncompanies can afford that.[2] Advertisers pay \nless for ads in free publications, because they assume readers \nignore something they get for free.  This is why so many trade\npublications nominally have a cover price and yet give away free\nsubscriptions with such abandon.[3] Different sections\nof the Times vary so much in their standards that they're\npractically different papers.  Whoever fed the style section reporter\nthis story about suits coming back would have been sent packing by\nthe regular news reporters.[4] The most striking\nexample I know of this type is the \"fact\" that the Internet worm   \nof 1988 infected 6000 computers. I was there when it was cooked up,\nand this was the recipe: someone guessed that there were about\n60,000 computers attached to the Internet, and that the worm might\nhave infected ten percent of them.Actually no one knows how many computers the worm infected, because\nthe remedy was to reboot them, and this destroyed all traces.  But\npeople like numbers.  And so this one is now replicated\nall over the Internet, like a little worm of its own.[5] Not all were\nnecessarily supplied by the PR firm. Reporters sometimes call a few\nadditional sources on their own, like someone adding a few fresh \nvegetables to a can of soup.\nThanks to Ingrid Basset, Trevor Blackwell, Sarah Harlin, Jessica \nLivingston, Jackie McDonough, Robert Morris, and Aaron Swartz (who\nalso found the PRSA article) for reading drafts of this.Correction: Earlier versions used a recent\nBusiness Week article mentioning del.icio.us as an example\nof a press hit, but Joshua Schachter tells me \nit was spontaneous.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nApril 2001, rev. April 2003(This article is derived from a talk given at the 2001 Franz\nDeveloper Symposium.)\nIn the summer of 1995, my friend Robert Morris and I\nstarted a startup called \nViaweb.  \nOur plan was to write\nsoftware that would let end users build online stores.\nWhat was novel about this software, at the time, was\nthat it ran on our server, using ordinary Web pages\nas the interface.A lot of people could have been having this idea at the\nsame time, of course, but as far as I know, Viaweb was\nthe first Web-based application.  It seemed such\na novel idea to us that we named the company after it:\nViaweb, because our software worked via the Web,\ninstead of running on your desktop computer.Another unusual thing about this software was that it\nwas written primarily in a programming language called\nLisp. It was one of the first big end-user\napplications to be written in Lisp, which up till then\nhad been used mostly in universities and research labs. [1]The Secret WeaponEric Raymond has written an essay called \"How to Become a Hacker,\"\nand in it, among other things, he tells would-be hackers what\nlanguages they should learn.  He suggests starting with Python and\nJava, because they are easy to learn.  The serious hacker will also\nwant to learn C, in order to hack Unix, and Perl for system\nadministration and cgi scripts.  Finally, the truly serious hacker\nshould consider learning Lisp:\n\n  Lisp is worth learning for the profound enlightenment experience\n  you will have when you finally get it; that experience will make\n  you a better programmer for the rest of your days, even if you\n  never actually use Lisp itself a lot.\n\nThis is the same argument you tend to hear for learning Latin.  It\nwon't get you a job, except perhaps as a classics professor, but\nit will improve your mind, and make you a better writer in languages\nyou do want to use, like English.But wait a minute.  This metaphor doesn't stretch that far.  The\nreason Latin won't get you a job is that no one speaks it.  If you\nwrite in Latin, no one can understand you.  But Lisp is a computer\nlanguage, and computers speak whatever language you, the programmer,\ntell them to.So if Lisp makes you a better programmer, like he says, why wouldn't\nyou want to use it? If a painter were offered a brush that would\nmake him a better painter, it seems to me that he would want to\nuse it in all his paintings, wouldn't he? I'm not trying to make\nfun of Eric Raymond here.  On the whole, his advice is good.  What\nhe says about Lisp is pretty much the conventional wisdom.  But\nthere is a contradiction in the conventional wisdom:  Lisp will\nmake you a better programmer, and yet you won't use it.Why not?  Programming languages are just tools, after all.  If Lisp\nreally does yield better programs, you should use it.  And if it\ndoesn't, then who needs it?This is not just a theoretical question.  Software is a very\ncompetitive business, prone to natural monopolies.  A company that\ngets software written faster and better will, all other things\nbeing equal, put its competitors out of business.  And when you're\nstarting a startup, you feel this very keenly.  Startups tend to\nbe an all or nothing proposition.  You either get rich, or you get\nnothing.  In a startup, if you bet on the wrong technology, your\ncompetitors will crush you.Robert and I both knew Lisp well, and we couldn't see any reason\nnot to trust our instincts and go with Lisp.  We knew that everyone\nelse was writing their software in C++ or Perl.  But we also knew\nthat that didn't mean anything.  If you chose technology that way,\nyou'd be running Windows.  When you choose technology, you have to\nignore what other people are doing, and consider only what will\nwork the best.This is especially true in a startup.  In a big company, you can\ndo what all the other big companies are doing.  But a startup can't\ndo what all the other startups do.  I don't think a lot of people\nrealize this, even in startups.The average big company grows at about ten percent a year.  So if\nyou're running a big company and you do everything the way the\naverage big company does it, you can expect to do as well as the\naverage big company-- that is, to grow about ten percent a year.The same thing will happen if you're running a startup, of course.\nIf you do everything the way the average startup does it, you should\nexpect average performance.  The problem here is, average performance\nmeans that you'll go out of business.  The survival rate for startups\nis way less than fifty percent.  So if you're running a startup,\nyou had better be doing something odd.  If not, you're in trouble.Back in 1995, we knew something that I don't think our competitors\nunderstood, and few understand even now:  when you're writing\nsoftware that only has to run on your own servers, you can use\nany language you want.  When you're writing desktop software,\nthere's a strong bias toward writing applications in the same\nlanguage as the operating system.  Ten years ago, writing applications\nmeant writing applications in C.  But with Web-based software,\nespecially when you have the source code of both the language and\nthe operating system, you can use whatever language you want.This new freedom is a double-edged sword, however.  Now that you\ncan use any language, you have to think about which one to use.\nCompanies that try to pretend nothing has changed risk finding that\ntheir competitors do not.If you can use any language, which do you use?  We chose Lisp.\nFor one thing, it was obvious that rapid development would be\nimportant in this market.  We were all starting from scratch, so\na company that could get new features done before its competitors\nwould have a big advantage.  We knew Lisp was a really good language\nfor writing software quickly, and server-based applications magnify\nthe effect of rapid development, because you can release software\nthe minute it's done.If other companies didn't want to use Lisp, so much the better.\nIt might give us a technological edge, and we needed all the help\nwe could get.  When we started Viaweb, we had no experience in\nbusiness.  We didn't know anything about marketing, or hiring\npeople, or raising money, or getting customers.  Neither of us had\never even had what you would call a real job.  The only thing we\nwere good at was writing software.  We hoped that would save us.\nAny advantage we could get in the software department, we would\ntake.So you could say that using Lisp was an experiment.  Our hypothesis\nwas that if we wrote our software in Lisp, we'd be able to get\nfeatures done faster than our competitors, and also to do things\nin our software that they couldn't do.  And because Lisp was so\nhigh-level, we wouldn't need a big development team, so our costs\nwould be lower.  If this were so, we could offer a better product\nfor less money, and still make a profit.  We would end up getting\nall the users, and our competitors would get none, and eventually\ngo out of business.  That was what we hoped would happen, anyway.What were the results of this experiment?  Somewhat surprisingly,\nit worked.  We eventually had many competitors, on the order of\ntwenty to thirty of them, but none of their software could compete\nwith ours.  We had a wysiwyg online store builder that ran on the\nserver and yet felt like a desktop application.  Our competitors\nhad cgi scripts.  And we were always far ahead of them in features.\nSometimes, in desperation, competitors would try to introduce\nfeatures that we didn't have.  But with Lisp our development cycle\nwas so fast that we could sometimes duplicate a new feature within\na day or two of a competitor announcing it in a press release.  By\nthe time journalists covering the press release got round to calling\nus, we would have the new feature too.It must have seemed to our competitors that we had some kind of\nsecret weapon-- that we were decoding their Enigma traffic or\nsomething.  In fact we did have a secret weapon, but it was simpler\nthan they realized.  No one was leaking news of their features to\nus.   We were just able to develop software faster than anyone\nthought possible.When I was about nine I happened to get hold of a copy of The Day\nof the Jackal, by Frederick Forsyth.  The main character is an\nassassin who is hired to kill the president of France.  The assassin\nhas to get past the police to get up to an apartment that overlooks\nthe president's route.  He walks right by them, dressed up as an\nold man on crutches, and they never suspect him.Our secret weapon was similar.  We wrote our software in a weird\nAI language, with a bizarre syntax full of parentheses.  For years\nit had annoyed me to hear Lisp described that way.  But now it\nworked to our advantage.  In business, there is nothing more valuable\nthan a technical advantage your competitors don't understand.  In\nbusiness, as in war, surprise is worth as much as force.And so, I'm a little embarrassed to say, I never said anything\npublicly about Lisp while we were working on Viaweb.  We never\nmentioned it to the press, and if you searched for Lisp on our Web\nsite, all you'd find were the titles of two books in my bio.  This\nwas no accident.  A startup should give its competitors as little\ninformation as possible.  If they didn't know what language our\nsoftware was written in, or didn't care, I wanted to keep it that\nway.[2]The people who understood our technology best were the customers.\nThey didn't care what language Viaweb was written in either, but\nthey noticed that it worked really well.  It let them build great\nlooking online stores literally in minutes.  And so, by word of\nmouth mostly, we got more and more users.  By the end of 1996 we\nhad about 70 stores online.  At the end of 1997 we had 500.  Six\nmonths later, when Yahoo bought us, we had 1070 users.  Today, as\nYahoo Store, this software continues to dominate its market.  It's\none of the more profitable pieces of Yahoo, and the stores built\nwith it are the foundation of Yahoo Shopping.  I left Yahoo in\n1999, so I don't know exactly how many users they have now, but\nthe last I heard there were about 20,000.\nThe Blub ParadoxWhat's so great about Lisp?  And if Lisp is so great, why doesn't\neveryone use it?  These sound like rhetorical questions, but actually\nthey have straightforward answers.  Lisp is so great not because\nof some magic quality visible only to devotees, but because it is\nsimply the most powerful language available.  And the reason everyone\ndoesn't use it is that programming languages are not merely\ntechnologies, but habits of mind as well, and nothing changes\nslower.  Of course, both these answers need explaining.I'll begin with a shockingly controversial statement:  programming\nlanguages vary in power.Few would dispute, at least, that high level languages are more\npowerful than machine language.  Most programmers today would agree\nthat you do not, ordinarily, want to program in machine language.\nInstead, you should program in a high-level language, and have a\ncompiler translate it into machine language for you.  This idea is\neven built into the hardware now: since the 1980s, instruction sets\nhave been designed for compilers rather than human programmers.Everyone knows it's a mistake to write your whole program by hand\nin machine language.  What's less often understood is that there\nis a more general principle here: that if you have a choice of\nseveral languages, it is, all other things being equal, a mistake\nto program in anything but the most powerful one. [3]There are many exceptions to this rule.  If you're writing a program\nthat has to work very closely with a program written in a certain\nlanguage, it might be a good idea to write the new program in the\nsame language.  If you're writing a program that only has to do\nsomething very simple, like number crunching or bit manipulation,\nyou may as well use a less abstract language, especially since it\nmay be slightly faster.  And if you're writing a short, throwaway\nprogram, you may be better off just using whatever language has\nthe best library functions for the task.  But in general, for\napplication software, you want to be using the most powerful\n(reasonably efficient) language you can get, and using anything\nelse is a mistake, of exactly the same kind, though possibly in a\nlesser degree, as programming in machine language.You can see that machine language is very low level.  But, at least\nas a kind of social convention, high-level languages are often all\ntreated as equivalent.  They're not.  Technically the term \"high-level\nlanguage\" doesn't mean anything very definite.  There's no dividing\nline with machine languages on one side and all the high-level\nlanguages on the other.  Languages fall along a continuum [4] of\nabstractness, from the most powerful all the way down to machine\nlanguages, which themselves vary in power.Consider Cobol.  Cobol is a high-level language, in the sense that\nit gets compiled into machine language.  Would anyone seriously\nargue that Cobol is equivalent in power to, say, Python?  It's\nprobably closer to machine language than Python.Or how about Perl 4?  Between Perl 4 and Perl 5, lexical closures\ngot added to the language.  Most Perl hackers would agree that Perl\n5 is more powerful than Perl 4.  But once you've admitted that,\nyou've admitted that one high level language can be more powerful\nthan another.  And it follows inexorably that, except in special\ncases, you ought to use the most powerful you can get.This idea is rarely followed to its conclusion, though.  After a\ncertain age, programmers rarely switch languages voluntarily.\nWhatever language people happen to be used to, they tend to consider\njust good enough.Programmers get very attached to their favorite languages, and I\ndon't want to hurt anyone's feelings, so to explain this point I'm\ngoing to use a hypothetical language called Blub.  Blub falls right\nin the middle of the abstractness continuum.  It is not the most\npowerful language, but it is more powerful than Cobol or machine\nlanguage.And in fact, our hypothetical Blub programmer wouldn't use either\nof them.  Of course he wouldn't program in machine language.  That's\nwhat compilers are for.  And as for Cobol, he doesn't know how\nanyone can get anything done with it.  It doesn't even have x (Blub\nfeature of your choice).As long as our hypothetical Blub programmer is looking down the\npower continuum, he knows he's looking down.  Languages less powerful\nthan Blub are obviously less powerful, because they're missing some\nfeature he's used to.  But when our hypothetical Blub programmer\nlooks in the other direction, up the power continuum, he doesn't\nrealize he's looking up.  What he sees are merely weird languages.\nHe probably considers them about equivalent in power to Blub, but\nwith all this other hairy stuff thrown in as well.  Blub is good\nenough for him, because he thinks in Blub.When we switch to the point of view of a programmer using any of\nthe languages higher up the power continuum, however, we find that\nhe in turn looks down upon Blub.  How can you get anything done in\nBlub? It doesn't even have y.By induction, the only programmers in a position to see all the\ndifferences in power between the various languages are those who\nunderstand the most powerful one.  (This is probably what Eric\nRaymond meant about Lisp making you a better programmer.) You can't\ntrust the opinions of the others, because of the Blub paradox:\nthey're satisfied with whatever language they happen to use, because\nit dictates the way they think about programs.I know this from my own experience, as a high school kid writing\nprograms in Basic.  That language didn't even support recursion.\nIt's hard to imagine writing programs without using recursion, but\nI didn't miss it at the time.  I thought in Basic.  And I was a\nwhiz at it.  Master of all I surveyed.The five languages that Eric Raymond recommends to hackers fall at\nvarious points on the power continuum.  Where they fall relative\nto one another is a sensitive topic.  What I will say is that I\nthink Lisp is at the top.  And to support this claim I'll tell you\nabout one of the things I find missing when I look at the other\nfour languages.  How can you get anything done in them, I think,\nwithout macros? [5]Many languages have something called a macro.  But Lisp macros are\nunique.  And believe it or not, what they do is related to the\nparentheses.  The designers of Lisp didn't put all those parentheses\nin the language just to be different.  To the Blub programmer, Lisp\ncode looks weird.  But those parentheses are there for a reason.\nThey are the outward evidence of a fundamental difference between\nLisp and other languages.Lisp code is made out of Lisp data objects.  And not in the trivial\nsense that the source files contain characters, and strings are\none of the data types supported by the language.  Lisp code, after\nit's read by the parser, is made of data structures that you can\ntraverse.If you understand how compilers work, what's really going on is\nnot so much that Lisp has a strange syntax as that Lisp has no\nsyntax.  You write programs in the parse trees that get generated\nwithin the compiler when other languages are parsed.  But these\nparse trees are fully accessible to your programs.  You can write\nprograms that manipulate them.  In Lisp, these programs are called\nmacros.  They are programs that write programs.Programs that write programs?  When would you ever want to do that?\nNot very often, if you think in Cobol.  All the time, if you think\nin Lisp.  It would be convenient here if I could give an example\nof a powerful macro, and say there! how about that?  But if I did,\nit would just look like gibberish to someone who didn't know Lisp;\nthere isn't room here to explain everything you'd need to know to\nunderstand what it meant.  In \nAnsi Common Lisp I tried to move\nthings along as fast as I could, and even so I didn't get to macros\nuntil page 160.But I think I can give a kind of argument that might be convincing.\nThe source code of the Viaweb editor was probably about 20-25%\nmacros.  Macros are harder to write than ordinary Lisp functions,\nand it's considered to be bad style to use them when they're not\nnecessary.  So every macro in that code is there because it has to\nbe.  What that means is that at least 20-25% of the code in this\nprogram is doing things that you can't easily do in any other\nlanguage.  However skeptical the Blub programmer might be about my\nclaims for the mysterious powers of Lisp, this ought to make him\ncurious.  We weren't writing this code for our own amusement.  We\nwere a tiny startup, programming as hard as we could in order to\nput technical barriers between us and our competitors.A suspicious person might begin to wonder if there was some\ncorrelation here.  A big chunk of our code was doing things that\nare very hard to do in other languages.  The resulting software\ndid things our competitors' software couldn't do.  Maybe there was\nsome kind of connection.  I encourage you to follow that thread.\nThere may be more to that old man hobbling along on his crutches\nthan meets the eye.Aikido for StartupsBut I don't expect to convince anyone \n(over 25) \nto go out and learn\nLisp.  The purpose of this article is not to change anyone's mind,\nbut to reassure people already interested in using Lisp-- people\nwho know that Lisp is a powerful language, but worry because it\nisn't widely used.  In a competitive situation, that's an advantage.\nLisp's power is multiplied by the fact that your competitors don't\nget it.If you think of using Lisp in a startup, you shouldn't worry that\nit isn't widely understood.  You should hope that it stays that\nway. And it's likely to.  It's the nature of programming languages\nto make most people satisfied with whatever they currently use.\nComputer hardware changes so much faster than personal habits that\nprogramming practice is usually ten to twenty years behind the\nprocessor.  At places like MIT they were writing programs in\nhigh-level languages in the early 1960s, but many companies continued\nto write code in machine language well into the 1980s.  I bet a\nlot of people continued to write machine language until the processor,\nlike a bartender eager to close up and go home, finally kicked them\nout by switching to a risc instruction set.Ordinarily technology changes fast.  But programming languages are\ndifferent: programming languages are not just technology, but what\nprogrammers think in.  They're half technology and half religion.[6]\nAnd so the median language, meaning whatever language the median\nprogrammer uses, moves as slow as an iceberg.  Garbage collection,\nintroduced by Lisp in about 1960, is now widely considered to be\na good thing.  Runtime typing, ditto, is growing in popularity.\nLexical closures, introduced by Lisp in the early 1970s, are now,\njust barely, on the radar screen.  Macros, introduced by Lisp in the\nmid 1960s, are still terra incognita.Obviously, the median language has enormous momentum.  I'm not\nproposing that you can fight this powerful force.  What I'm proposing\nis exactly the opposite: that, like a practitioner of Aikido, you\ncan use it against your opponents.If you work for a big company, this may not be easy.  You will have\na hard time convincing the pointy-haired boss to let you build\nthings in Lisp, when he has just read in the paper that some other\nlanguage is poised, like Ada was twenty years ago, to take over\nthe world.  But if you work for a startup that doesn't have\npointy-haired bosses yet, you can, like we did, turn the Blub\nparadox to your advantage:  you can use technology that your\ncompetitors, glued immovably to the median language, will never be\nable to match.If you ever do find yourself working for a startup, here's a handy\ntip for evaluating competitors.  Read their job listings.  Everything\nelse on their site may be stock photos or the prose equivalent,\nbut the job listings have to be specific about what they want, or\nthey'll get the wrong candidates.During the years we worked on Viaweb I read a lot of job descriptions.\nA new competitor seemed to emerge out of the woodwork every month\nor so.  The first thing I would do, after checking to see if they\nhad a live online demo, was look at their job listings.  After a\ncouple years of this I could tell which companies to worry about\nand which not to.  The more of an IT flavor the job descriptions\nhad, the less dangerous the company was.  The safest kind were the\nones that wanted Oracle experience.  You never had to worry about\nthose.  You were also safe if they said they wanted C++ or Java\ndevelopers.  If they wanted Perl or Python programmers, that would\nbe a bit frightening-- that's starting to sound like a company\nwhere the technical side, at least, is run by real hackers.  If I\nhad ever seen a job posting looking for Lisp hackers, I would have\nbeen really worried.\nNotes[1] Viaweb at first had two parts: the editor, written in Lisp,\nwhich people used to build their sites, and the ordering system,\nwritten in C, which handled orders.  The first version was mostly\nLisp, because the ordering system was small.  Later we added two\nmore modules, an image generator written in C, and a back-office\nmanager written mostly in Perl.In January 2003, Yahoo released a new version of the editor \nwritten in C++ and Perl.  It's hard to say whether the program is no\nlonger written in Lisp, though, because to translate this program\ninto C++ they literally had to write a Lisp interpreter: the source\nfiles of all the page-generating templates are still, as far as I\nknow,  Lisp code.  (See Greenspun's Tenth Rule.)[2] Robert Morris says that I didn't need to be secretive, because\neven if our competitors had known we were using Lisp, they wouldn't\nhave understood why:  \"If they were that smart they'd already be\nprogramming in Lisp.\"[3] All languages are equally powerful in the sense of being Turing\nequivalent, but that's not the sense of the word programmers care\nabout. (No one wants to program a Turing machine.)  The kind of\npower programmers care about may not be formally definable, but\none way to explain it would be to say that it refers to features\nyou could only get in the less powerful language by writing an\ninterpreter for the more powerful language in it. If language A\nhas an operator for removing spaces from strings and language B\ndoesn't, that probably doesn't make A more powerful, because you\ncan probably write a subroutine to do it in B.  But if A supports,\nsay, recursion, and B doesn't, that's not likely to be something\nyou can fix by writing library functions.[4] Note to nerds: or possibly a lattice, narrowing toward the top;\nit's not the shape that matters here but the idea that there is at\nleast a partial order.[5] It is a bit misleading to treat macros as a separate feature.\nIn practice their usefulness is greatly enhanced by other Lisp\nfeatures like lexical closures and rest parameters.[6] As a result, comparisons of programming languages either take\nthe form of religious wars or undergraduate textbooks so determinedly\nneutral that they're really works of anthropology.  People who\nvalue their peace, or want tenure, avoid the topic.  But the question\nis only half a religious one; there is something there worth\nstudying, especially if you want to design new languages.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nOctober 2014(This essay is derived from a guest lecture in Sam Altman's startup class at\nStanford.  It's intended for college students, but much of it is\napplicable to potential founders at other ages.)One of the advantages of having kids is that when you have to give\nadvice, you can ask yourself \"what would I tell my own kids?\"  My\nkids are little, but I can imagine what I'd tell them about startups\nif they were in college, and that's what I'm going to tell you.Startups are very counterintuitive.  I'm not sure why.  Maybe it's\njust because knowledge about them hasn't permeated our culture yet.\nBut whatever the reason, starting a startup is a task where you\ncan't always trust your instincts.It's like skiing in that way.  When you first try skiing and you\nwant to slow down, your instinct is to lean back.  But if you lean\nback on skis you fly down the hill out of control.  So part of\nlearning to ski is learning to suppress that impulse.  Eventually\nyou get new habits, but at first it takes a conscious effort.  At\nfirst there's a list of things you're trying to remember as you\nstart down the hill.Startups are as unnatural as skiing, so there's a similar list for\nstartups. Here I'm going to give you the first part of it \u2014 the things\nto remember if you want to prepare yourself to start a startup.\nCounterintuitiveThe first item on it is the fact I already mentioned: that startups\nare so weird that if you trust your instincts, you'll make a lot\nof mistakes.  If you know nothing more than this, you may at least\npause before making them.When I was running Y Combinator I used to joke that our function\nwas to tell founders things they would ignore.  It's really true.\nBatch after batch, the YC partners warn founders about mistakes\nthey're about to make, and the founders ignore them, and then come\nback a year later and say \"I wish we'd listened.\"Why do the founders ignore the partners' advice?  Well, that's the\nthing about counterintuitive ideas: they contradict your intuitions.\nThey seem wrong.  So of course your first impulse is to disregard\nthem.  And in fact my joking description is not merely the curse\nof Y Combinator but part of its raison d'etre. If founders' instincts\nalready gave them the right answers, they wouldn't need us.  You\nonly need other people to give you advice that surprises you. That's\nwhy there are a lot of ski instructors and not many running\ninstructors.\n[1]You can, however, trust your instincts about people.  And in fact\none of the most common mistakes young founders make is not to\ndo that enough.  They get involved with people who seem impressive,\nbut about whom they feel some misgivings personally.  Later when\nthings blow up they say \"I knew there was something off about him,\nbut I ignored it because he seemed so impressive.\"If you're thinking about getting involved with someone \u2014 as a\ncofounder, an employee, an investor, or an acquirer \u2014 and you\nhave misgivings about them, trust your gut.  If someone seems\nslippery, or bogus, or a jerk, don't ignore it.This is one case where it pays to be self-indulgent. Work with\npeople you genuinely like, and you've known long enough to be sure.\nExpertiseThe second counterintuitive point is that it's not that important\nto know a lot about startups.  The way to succeed in a startup is\nnot to be an expert on startups, but to be an expert on your users\nand the problem you're solving for them.\nMark Zuckerberg didn't succeed because he was an expert on startups.\nHe succeeded despite being a complete noob at startups, because he\nunderstood his users really well.If you don't know anything about, say, how to raise an angel round,\ndon't feel bad on that account.  That sort of thing you can learn\nwhen you need to, and forget after you've done it.In fact, I worry it's not merely unnecessary to learn in great\ndetail about the mechanics of startups, but possibly somewhat\ndangerous.  If I met an undergrad who knew all about convertible\nnotes and employee agreements and (God forbid) class FF stock, I\nwouldn't think \"here is someone who is way ahead of their peers.\"\nIt would set off alarms.  Because another of the characteristic\nmistakes of young founders is to go through the motions of starting\na startup.  They make up some plausible-sounding idea, raise money\nat a good valuation, rent a cool office, hire a bunch of people.\nFrom the outside that seems like what startups do.  But the next\nstep after rent a cool office and hire a bunch of people is: gradually\nrealize how completely fucked they are, because while imitating all\nthe outward forms of a startup they have neglected the one thing\nthat's actually essential: making something people want.\nGameWe saw this happen so often that we made up a name for it: playing\nhouse.  Eventually I realized why it was happening.  The reason\nyoung founders go through the motions of starting a startup is\nbecause that's what they've been trained to do for their whole lives\nup to that point.  Think about what you have to do to get into\ncollege, for example.  Extracurricular activities, check.  Even in\ncollege classes most of the work is as artificial as running laps.I'm not attacking the educational system for being this way. There\nwill always be a certain amount of fakeness in the work you do when\nyou're being taught something, and if you measure their performance\nit's inevitable that people will exploit the difference to the point\nwhere much of what you're measuring is artifacts of the fakeness.I confess I did it myself in college. I found that in a lot of\nclasses there might only be 20 or 30 ideas that were the right shape\nto make good exam questions.  The way I studied for exams in these\nclasses was not (except incidentally) to master the material taught\nin the class, but to make a list of potential exam questions and\nwork out the answers in advance. When I walked into the final, the\nmain thing I'd be feeling was curiosity about which of my questions\nwould turn up on the exam.  It was like a game.It's not surprising that after being trained for their whole lives\nto play such games, young founders' first impulse on starting a\nstartup is to try to figure out the tricks for winning at this new\ngame. Since fundraising appears to be the measure of success for\nstartups (another classic noob mistake), they always want to know what the\ntricks are for convincing investors.  We tell them the best way to\nconvince investors is to make a startup\nthat's actually doing well, meaning growing fast, and then simply\ntell investors so.  Then they want to know what the tricks are for\ngrowing fast.  And we have to tell them the best way to do that is\nsimply to make something people want.So many of the conversations YC partners have with young founders\nbegin with the founder asking \"How do we...\" and the partner replying\n\"Just...\"Why do the founders always make things so complicated?  The reason,\nI realized, is that they're looking for the trick.So this is the third counterintuitive thing to remember about\nstartups: starting a startup is where gaming the system stops\nworking.  Gaming the system may continue to work if you go to work\nfor a big company. Depending on how broken the company is, you can\nsucceed by sucking up to the right people, giving the impression\nof productivity, and so on. \n[2]\nBut that doesn't work with startups.\nThere is no boss to trick, only users, and all users care about is\nwhether your product does what they want. Startups are as impersonal\nas physics.  You have to make something people want, and you prosper\nonly to the extent you do.The dangerous thing is, faking does work to some degree on investors.\nIf you're super good at sounding like you know what you're talking\nabout, you can fool investors for at least one and perhaps even two\nrounds of funding.  But it's not in your interest to.  The company\nis ultimately doomed.  All you're doing is wasting your own time\nriding it down.So stop looking for the trick. There are tricks in startups, as\nthere are in any domain, but they are an order of magnitude less\nimportant than solving the real problem. A founder who knows nothing\nabout fundraising but has made something users love will have an\neasier time raising money than one who knows every trick in the\nbook but has a flat usage graph. And more importantly, the founder\nwho has made something users love is the one who will go on to\nsucceed after raising the money.Though in a sense it's bad news in that you're deprived of one of\nyour most powerful weapons, I think it's exciting that gaming the\nsystem stops working when you start a startup.  It's exciting that\nthere even exist parts of the world where you win by doing good\nwork.  Imagine how depressing the world would be if it were all\nlike school and big companies, where you either have to spend a lot\nof time on bullshit things or lose to people who do.\n[3]\nI would\nhave been delighted if I'd realized in college that there were parts\nof the real world where gaming the system mattered less than others,\nand a few where it hardly mattered at all.  But there are, and this\nvariation is one of the most important things to consider when\nyou're thinking about your future.  How do you win in each type of\nwork, and what would you like to win by doing?\n[4]\nAll-ConsumingThat brings us to our fourth counterintuitive point: startups are\nall-consuming.  If you start a startup, it will take over your life\nto a degree you cannot imagine.  And if your startup succeeds, it\nwill take over your life for a long time: for several years at the\nvery least, maybe for a decade, maybe for the rest of your working\nlife.  So there is a real opportunity cost here.Larry Page may seem to have an enviable life, but there are aspects\nof it that are unenviable.  Basically at 25 he started running as\nfast as he could and it must seem to him that he hasn't stopped to\ncatch his breath since.  Every day new shit happens in the Google\nempire that only the CEO can deal with, and he, as CEO, has to deal\nwith it.  If he goes on vacation for even a week, a whole week's\nbacklog of shit accumulates.  And he has to bear this uncomplainingly,\npartly because as the company's daddy he can never show fear or\nweakness, and partly because billionaires get less than zero sympathy\nif they talk about having difficult lives.  Which has the strange\nside effect that the difficulty of being a successful startup founder\nis concealed from almost everyone except those who've done it.Y Combinator has now funded several companies that can be called\nbig successes, and in every single case the founders say the same\nthing.  It never gets any easier.  The nature of the problems change.\nYou're worrying about construction delays at your London office\ninstead of the broken air conditioner in your studio apartment.\nBut the total volume of worry never decreases; if anything it\nincreases.Starting a successful startup is similar to having kids in that\nit's like a button you push that changes your life irrevocably.\nAnd while it's truly wonderful having kids, there are a lot of\nthings that are easier to do before you have them than after.  Many\nof which will make you a better parent when you do have kids. And\nsince you can delay pushing the button for a while, most people in\nrich countries do.Yet when it comes to startups, a lot of people seem to think they're\nsupposed to start them while they're still in college.  Are you\ncrazy?  And what are the universities thinking?  They go out of\ntheir way to ensure their students are well supplied with contraceptives,\nand yet they're setting up entrepreneurship programs and startup\nincubators left and right.To be fair, the universities have their hand forced here.  A lot\nof incoming students are interested in startups.  Universities are,\nat least de facto, expected to prepare them for their careers.  So\nstudents who want to start startups hope universities can teach\nthem about startups.  And whether universities can do this or not,\nthere's some pressure to claim they can, lest they lose applicants\nto other universities that do.Can universities teach students about startups?  Yes and no.  They\ncan teach students about startups, but as I explained before, this\nis not what you need to know.  What you need to learn about are the\nneeds of your own users, and you can't do that until you actually\nstart the company.\n[5]\nSo starting a startup is intrinsically\nsomething you can only really learn by doing it.  And it's impossible\nto do that in college, for the reason I just explained: startups\ntake over your life.  You can't start a startup for real as a\nstudent, because if you start a startup for real you're not a student\nanymore. You may be nominally a student for a bit, but you won't even\nbe that for long.\n[6]Given this dichotomy, which of the two paths should you take?  Be\na real student and not start a startup, or start a real startup and\nnot be a student?  I can answer that one for you. Do not start a\nstartup in college.  How to start a startup is just a subset of a\nbigger problem you're trying to solve: how to have a good life.\nAnd though starting a startup can be part of a good life for a lot\nof ambitious people, age 20 is not the optimal time to do it.\nStarting a startup is like a brutally fast depth-first search.  Most\npeople should still be searching breadth-first at 20.You can do things in your early 20s that you can't do as well before\nor after, like plunge deeply into projects on a whim and travel\nsuper cheaply with no sense of a deadline.  For unambitious people,\nthis sort of thing is the dreaded \"failure to launch,\" but for the\nambitious ones it can be an incomparably valuable sort of exploration.\nIf you start a startup at 20 and you're sufficiently successful,\nyou'll never get to do it.\n[7]Mark Zuckerberg will never get to bum around a foreign country.  He\ncan do other things most people can't, like charter jets to fly him\nto foreign countries. But success has taken a lot of the serendipity\nout of his life. Facebook is running him as much as he's running\nFacebook. And while it can be very cool to be in the grip of a\nproject you consider your life's work, there are advantages to\nserendipity too, especially early in life.  Among other things it\ngives you more options to choose your life's work from.There's not even a tradeoff here. You're not sacrificing anything\nif you forgo starting a startup at 20, because you're more likely\nto succeed if you wait.  In the unlikely case that you're 20 and\none of your side projects takes off like Facebook did, you'll face\na choice of running with it or not, and it may be reasonable to run\nwith it.  But the usual way startups take off is for the founders\nto make them take off, and it's gratuitously\nstupid to do that at 20.\nTryShould you do it at any age?  I realize I've made startups sound\npretty hard.  If I haven't, let me try again: starting a startup\nis really hard.  What if it's too hard?  How can you tell if you're\nup to this challenge?The answer is the fifth counterintuitive point: you can't tell. Your\nlife so far may have given you some idea what your prospects might\nbe if you tried to become a mathematician, or a professional football\nplayer.  But unless you've had a very strange life you haven't done\nmuch that was like being a startup founder.\nStarting a startup will change you a lot.  So what you're trying\nto estimate is not just what you are, but what you could grow into,\nand who can do that?For the past 9 years it was my job to predict whether people would\nhave what it took to start successful startups.  It was easy to\ntell how smart they were, and most people reading this will be over\nthat threshold.  The hard part was predicting how tough and ambitious they would become.  There\nmay be no one who has more experience at trying to predict that,\nso I can tell you how much an expert can know about it, and the\nanswer is: not much.  I learned to keep a completely open mind about\nwhich of the startups in each batch would turn out to be the stars.The founders sometimes think they know. Some arrive feeling sure\nthey will ace Y Combinator just as they've aced every one of the (few,\nartificial, easy) tests they've faced in life so far.  Others arrive\nwondering how they got in, and hoping YC doesn't discover whatever\nmistake caused it to accept them.  But there is little correlation\nbetween founders' initial attitudes and how well their companies\ndo.I've read that the same is true in the military \u2014 that the\nswaggering recruits are no more likely to turn out to be really\ntough than the quiet ones. And probably for the same reason: that\nthe tests involved are so different from the ones in their previous\nlives.If you're absolutely terrified of starting a startup, you probably\nshouldn't do it.  But if you're merely unsure whether you're up to\nit, the only way to find out is to try.  Just not now.\nIdeasSo if you want to start a startup one day, what should you do in\ncollege?  There are only two things you need initially: an idea and\ncofounders.  And the m.o. for getting both is the same.  Which leads\nto our sixth and last counterintuitive point: that the way to get\nstartup ideas is not to try to think of startup ideas.I've written a whole essay on this,\nso I won't repeat it all here.  But the short version is that if\nyou make a conscious effort to think of startup ideas, the ideas\nyou come up with will not merely be bad, but bad and plausible-sounding,\nmeaning you'll waste a lot of time on them before realizing they're\nbad.The way to come up with good startup ideas is to take a step back.\nInstead of making a conscious effort to think of startup ideas,\nturn your mind into the type that startup ideas form in without any\nconscious effort.  In fact, so unconsciously that you don't even\nrealize at first that they're startup ideas.This is not only possible, it's how Apple, Yahoo, Google, and\nFacebook all got started.  None of these companies were even meant\nto be companies at first.  They were all just side projects.  The\nbest startups almost have to start as side projects, because great\nideas tend to be such outliers that your conscious mind would reject\nthem as ideas for companies.Ok, so how do you turn your mind into the type that startup ideas\nform in unconsciously?  (1) Learn a lot about things that matter,\nthen (2) work on problems that interest you (3) with people you\nlike and respect.  The third part, incidentally, is how you get\ncofounders at the same time as the idea.The first time I wrote that paragraph, instead of \"learn a lot about\nthings that matter,\" I wrote \"become good at some technology.\" But\nthat prescription, though sufficient, is too narrow.  What was\nspecial about Brian Chesky and Joe Gebbia was not that they were\nexperts in technology.  They were good at design, and perhaps even\nmore importantly, they were good at organizing groups and making\nprojects happen.  So you don't have to work on technology per se,\nso long as you work on problems demanding enough to stretch you.What kind of problems are those?  That is very hard to answer in\nthe general case.  History is full of examples of young people who\nwere working on important problems that no\none else at the time thought were important, and in particular\nthat their parents didn't think were important.  On the other hand,\nhistory is even fuller of examples of parents who thought their\nkids were wasting their time and who were right.  So how do you\nknow when you're working on real stuff?\n[8]I know how I know.  Real problems are interesting, and I am\nself-indulgent in the sense that I always want to work on interesting\nthings, even if no one else cares about them (in fact, especially\nif no one else cares about them), and find it very hard to make\nmyself work on boring things, even if they're supposed to be\nimportant.My life is full of case after case where I worked on something just\nbecause it seemed interesting, and it turned out later to be useful\nin some worldly way.  Y\nCombinator itself was something I only did because it seemed\ninteresting. So I seem to have some sort of internal compass that\nhelps me out.  But I don't know what other people have in their\nheads. Maybe if I think more about this I can come up with heuristics\nfor recognizing genuinely interesting problems, but for the moment\nthe best I can offer is the hopelessly question-begging advice that\nif you have a taste for genuinely interesting problems, indulging\nit energetically is the best way to prepare yourself for a startup.\nAnd indeed, probably also the best way to live.\n[9]But although I can't explain in the general case what counts as an\ninteresting problem, I can tell you about a large subset of them.\nIf you think of technology as something that's spreading like a\nsort of fractal stain, every moving point on the edge represents\nan interesting problem.  So one guaranteed way to turn your mind\ninto the type that has good startup ideas is to get yourself to the\nleading edge of some technology \u2014 to cause yourself, as Paul\nBuchheit put it, to \"live in the future.\" When you reach that point,\nideas that will seem to other people uncannily prescient will seem\nobvious to you.  You may not realize they're startup ideas, but\nyou'll know they're something that ought to exist.For example, back at Harvard in the mid 90s a fellow grad student\nof my friends Robert and Trevor wrote his own voice over IP software.\nHe didn't mean it to be a startup, and he never tried to turn it\ninto one.  He just wanted to talk to his girlfriend in Taiwan without\npaying for long distance calls, and since he was an expert on\nnetworks it seemed obvious to him that the way to do it was turn\nthe sound into packets and ship it over the Internet. He never did\nany more with his software than talk to his girlfriend, but this\nis exactly the way the best startups get started.So strangely enough the optimal thing to do in college if you want\nto be a successful startup founder is not some sort of new, vocational\nversion of college focused on \"entrepreneurship.\" It's the classic\nversion of college as education for its own sake. If you want to\nstart a startup after college, what you should do in college is\nlearn powerful things.  And if you have genuine intellectual\ncuriosity, that's what you'll naturally tend to do if you just\nfollow your own inclinations.\n[10]The component of entrepreneurship that really matters is domain\nexpertise.  The way to become Larry Page was to become an expert\non search. And the way to become an expert on search was to be\ndriven by genuine curiosity, not some ulterior motive.At its best, starting a startup is merely an ulterior motive for\ncuriosity.  And you'll do it best if you introduce the ulterior\nmotive toward the end of the process.So here is the ultimate advice for young would-be startup founders,\nboiled down to two words: just learn.\nNotes[1]\nSome founders listen more than others, and this tends to be a\npredictor of success. One of the things I\nremember about the Airbnbs during YC is how intently they listened.[2]\nIn fact, this is one of the reasons startups are possible.  If\nbig companies weren't plagued by internal inefficiencies, they'd\nbe proportionately more effective, leaving less room for startups.[3]\nIn a startup you have to spend a lot of time on schleps, but this sort of work is merely\nunglamorous, not bogus.[4]\nWhat should you do if your true calling is gaming the system?\nManagement consulting.[5]\nThe company may not be incorporated, but if you start to get\nsignificant numbers of users, you've started it, whether you realize\nit yet or not.[6]\nIt shouldn't be that surprising that colleges can't teach\nstudents how to be good startup founders, because they can't teach\nthem how to be good employees either.The way universities \"teach\" students how to be employees is to\nhand off the task to companies via internship programs.  But you\ncouldn't do the equivalent thing for startups, because by definition\nif the students did well they would never come back.[7]\nCharles Darwin was 22 when he received an invitation to travel\naboard the HMS Beagle as a naturalist.  It was only because he was\notherwise unoccupied, to a degree that alarmed his family, that he\ncould accept it. And yet if he hadn't we probably would not know\nhis name.[8]\nParents can sometimes be especially conservative in this\ndepartment.  There are some whose definition of important problems\nincludes only those on the critical path to med school.[9]\nI did manage to think of a heuristic for detecting whether you\nhave a taste for interesting ideas: whether you find known boring\nideas intolerable.  Could you endure studying literary theory, or\nworking in middle management at a large company?[10]\nIn fact, if your goal is to start a startup, you can stick\neven more closely to the ideal of a liberal education than past\ngenerations have. Back when students focused mainly on getting a\njob after college, they thought at least a little about how the\ncourses they took might look to an employer.  And perhaps even\nworse, they might shy away from taking a difficult class lest they\nget a low grade, which would harm their all-important GPA.  Good\nnews: users don't care what your GPA\nwas.  And I've never heard of investors caring either.  Y Combinator\ncertainly never asks what classes you took in college or what grades\nyou got in them.\nThanks to Sam Altman, Paul Buchheit, John Collison, Patrick\nCollison, Jessica Livingston, Robert Morris, Geoff Ralston, and\nFred Wilson for reading drafts of this.April 2006(This essay is derived from a talk at the 2006 \nStartup School.)The startups we've funded so far are pretty quick, but they seem\nquicker to learn some lessons than others.  I think it's because\nsome things about startups are kind of counterintuitive.We've now \ninvested \nin enough companies that I've learned a trick\nfor determining which points are the counterintuitive ones:\nthey're the ones I have to keep repeating.So I'm going to number these points, and maybe with future startups\nI'll be able to pull off a form of Huffman coding. I'll make them\nall read this, and then instead of nagging them in detail, I'll\njust be able to say: number four!\n1. Release Early.The thing I probably repeat most is this recipe for a startup: get\na version 1 out fast, then improve it based on users' reactions.By \"release early\" I don't mean you should release something full\nof bugs, but that you should release something minimal.  Users hate\nbugs, but they don't seem to mind a minimal version 1, if there's\nmore coming soon.There are several reasons it pays to get version 1 done fast.  One\nis that this is simply the right way to write software, whether for\na startup or not.  I've been repeating that since 1993, and I haven't seen much since to\ncontradict it.  I've seen a lot of startups die because they were\ntoo slow to release stuff, and none because they were too quick.\n[1]One of the things that will surprise you if you build something\npopular is that you won't know your users.  Reddit now has almost half a million\nunique visitors a month.  Who are all those people?  They have no\nidea.  No web startup does.  And since you don't know your users,\nit's dangerous to guess what they'll like.  Better to release\nsomething and let them tell you.Wufoo took this to heart and released\ntheir form-builder before the underlying database.  You can't even\ndrive the thing yet, but 83,000 people came to sit in the driver's\nseat and hold the steering wheel.  And Wufoo got valuable feedback\nfrom it: Linux users complained they used too much Flash, so they\nrewrote their software not to.  If they'd waited to release everything\nat once, they wouldn't have discovered this problem till it was\nmore deeply wired in.Even if you had no users, it would still be important to release\nquickly, because for a startup the initial release acts as a shakedown\ncruise.  If anything major is broken-- if the idea's no good,\nfor example, or the founders hate one another-- the stress of getting\nthat first version out will expose it.  And if you have such problems\nyou want to find them early.Perhaps the most important reason to release early, though, is that\nit makes you work harder.  When you're working on something that\nisn't released, problems are intriguing.  In something that's out\nthere, problems are alarming.  There is a lot more urgency once you\nrelease.  And I think that's precisely why people put it off.  They\nknow they'll have to work a lot harder once they do. \n[2]\n2. Keep Pumping Out Features.Of course, \"release early\" has a second component, without which\nit would be bad advice.  If you're going to start with something\nthat doesn't do much, you better improve it fast.What I find myself repeating is \"pump out features.\"  And this rule\nisn't just for the initial stages.  This is something all startups\nshould do for as long as they want to be considered startups.I don't mean, of course, that you should make your application ever\nmore complex.  By \"feature\" I mean one unit of hacking-- one quantum\nof making users' lives better.As with exercise, improvements beget improvements.  If you run every\nday, you'll probably feel like running tomorrow.  But if you skip\nrunning for a couple weeks, it will be an effort to drag yourself\nout.  So it is with hacking: the more ideas you implement, the more\nideas you'll have.  You should make your system better at least in\nsome small way every day or two.This is not just a good way to get development done; it is also a\nform of marketing.  Users love a site that's constantly improving.\nIn fact, users expect a site to improve.  Imagine if you visited a\nsite that seemed very good, and then returned two months later and\nnot one thing had changed.  Wouldn't it start to seem lame? \n[3]They'll like you even better when you improve in response to their\ncomments, because customers are used to companies ignoring them.\nIf you're the rare exception-- a company that actually listens--\nyou'll generate fanatical loyalty.  You won't need to advertise,\nbecause your users will do it for you.This seems obvious too, so why do I have to keep repeating it?  I\nthink the problem here is that people get used to how things are.\nOnce a product gets past the stage where it has glaring flaws, you\nstart to get used to it, and gradually whatever features it happens\nto have become its identity.  For example, I doubt many people at\nYahoo (or Google for that matter) realized how much better web mail\ncould be till Paul Buchheit showed them.I think the solution is to assume that anything you've made is far\nshort of what it could be.  Force yourself, as a sort of intellectual\nexercise, to keep thinking of improvements.  Ok, sure, what you\nhave is perfect.  But if you had to change something, what would\nit be?If your product seems finished, there are two possible explanations:\n(a) it is finished, or (b) you lack imagination.  Experience suggests\n(b) is a thousand times more likely.\n3. Make Users Happy.Improving constantly is an instance of a more general rule: make\nusers happy.  One thing all startups have in common is that they\ncan't force anyone to do anything.  They can't force anyone to use\ntheir software, and they can't force anyone to do deals with them.\nA startup has to sing for its supper.  That's why the successful\nones make great things.  They have to, or die.When you're running a startup you feel like a little bit of debris\nblown about by powerful winds.  The most powerful wind is users.\nThey can either catch you and loft you up into the sky, as they did\nwith Google, or leave you flat on the pavement, as they do with\nmost startups.  Users are a fickle wind, but more powerful than any\nother.  If they take you up, no competitor can keep you down.As a little piece of debris, the rational thing for you to do is\nnot to lie flat, but to curl yourself into a shape the wind will\ncatch.I like the wind metaphor because it reminds you how impersonal the\nstream of traffic is.  The vast majority of people who visit your\nsite will be casual visitors.  It's them you have to design your\nsite for.  The people who really care will find what they want by\nthemselves.The median visitor will arrive with their finger poised on the Back\nbutton.  Think about your own experience: most links you\nfollow lead to something lame.  Anyone who has used the web for\nmore than a couple weeks has been trained to click on Back after\nfollowing a link.  So your site has to say \"Wait!  Don't click on\nBack.  This site isn't lame.  Look at this, for example.\"There are two things you have to do to make people pause.  The most\nimportant is to explain, as concisely as possible, what the hell\nyour site is about.  How often have you visited a site that seemed\nto assume you already knew what they did?  For example, the corporate\nsite that says the\ncompany makes\n\n  enterprise content management solutions for business that enable\n  organizations to unify people, content and processes to minimize\n  business risk, accelerate time-to-value and sustain lower total\n  cost of ownership.\n\nAn established company may get away with such an opaque description,\nbut no startup can.  A startup\nshould be able to explain in one or two sentences exactly what it\ndoes. \n[4]\nAnd not just to users.  You need this for everyone:\ninvestors, acquirers, partners, reporters, potential employees, and\neven current employees.  You probably shouldn't even start a company\nto do something that can't be described compellingly in one or two\nsentences.The other thing I repeat is to give people everything you've got,\nright away.  If you have something impressive, try to put it on the\nfront page, because that's the only one most visitors will see.\nThough indeed there's a paradox here: the more you push the good\nstuff toward the front, the more likely visitors are to explore\nfurther. \n[5]In the best case these two suggestions get combined: you tell\nvisitors what your site is about by showing them.  One of the\nstandard pieces of advice in fiction writing is \"show, don't tell.\"\nDon't say that a character's angry; have him grind his teeth, or\nbreak his pencil in half.  Nothing will explain what your site does\nso well as using it.The industry term here is \"conversion.\"  The job of your site is\nto convert casual visitors into users-- whatever your definition\nof a user is.  You can measure this in your growth rate.  Either\nyour site is catching on, or it isn't, and you must know which.  If\nyou have decent growth, you'll win in the end, no matter how obscure\nyou are now.  And if you don't, you need to fix something.\n4. Fear the Right Things.Another thing I find myself saying a lot is \"don't worry.\"  Actually,\nit's more often \"don't worry about this; worry about that instead.\"\nStartups are right to be paranoid, but they sometimes fear the wrong\nthings.Most visible disasters are not so alarming as they seem.  Disasters\nare normal in a startup: a founder quits, you discover a patent\nthat covers what you're doing, your servers keep crashing, you run\ninto an insoluble technical problem, you have to change your name,\na deal falls through-- these are all par for the course.  They won't\nkill you unless you let them.Nor will most competitors.  A lot of startups worry \"what if Google\nbuilds something like us?\"  Actually big companies are not the ones\nyou have to worry about-- not even Google.  The people at Google\nare smart, but no smarter than you; they're not as motivated, because\nGoogle is not going to go out of business if this one product fails;\nand even at Google they have a lot of bureaucracy to slow them down.What you should fear, as a startup, is not the established players,\nbut other startups you don't know exist yet.  They're way more\ndangerous than Google because, like you, they're cornered animals.Looking just at existing competitors can give you a false sense of\nsecurity.  You should compete against what someone else could be\ndoing, not just what you can see people doing.  A corollary is that\nyou shouldn't relax just because you have no visible competitors\nyet.  No matter what your idea, there's someone else out there\nworking on the same thing.That's the downside of it being easier to start a startup: more people\nare doing it.  But I disagree with Caterina Fake when she says that\nmakes this a bad time to start a startup.  More people are starting\nstartups, but not as many more as could.  Most college graduates\nstill think they have to get a job.  The average person can't ignore\nsomething that's been beaten into their head since they were three\njust because serving web pages recently got a lot cheaper.And in any case, competitors are not the biggest threat.  Way more\nstartups hose themselves than get crushed by competitors.  There\nare a lot of ways to do it, but the three main ones are internal\ndisputes, inertia, and ignoring users.  Each is, by itself, enough\nto kill you.  But if I had to pick the worst, it would be ignoring\nusers.  If you want a recipe for a startup that's going to die,\nhere it is: a couple of founders who have some great idea they know\neveryone is going to love, and that's what they're going to build,\nno matter what.Almost everyone's initial plan is broken.  If companies stuck to\ntheir initial plans, Microsoft would be selling programming languages,\nand Apple would be selling printed circuit boards.  In both cases\ntheir customers told them what their business should be-- and they\nwere smart enough to listen.As Richard Feynman said, the imagination of nature is greater than\nthe imagination of man.  You'll find more interesting things by\nlooking at the world than you could ever produce just by thinking.\nThis principle is very powerful.  It's why the best abstract painting\nstill falls short of Leonardo, for example.  And it applies to\nstartups too.  No idea for a product could ever be so clever as the\nones you can discover by smashing a beam of prototypes into a beam\nof users.\n5. Commitment Is a Self-Fulfilling Prophecy.I now have enough experience with startups to be able to say what\nthe most important quality is in a startup founder, and it's not\nwhat you might think.  The most important quality in a startup\nfounder is determination.  Not intelligence-- determination.This is a little depressing.  I'd like to believe Viaweb succeeded\nbecause we were smart, not merely determined.  A lot of people in\nthe startup world want to believe that.  Not just founders, but\ninvestors too.  They like the idea of inhabiting a world ruled by\nintelligence.  And you can tell they really believe this, because\nit affects their investment decisions.Time after time VCs invest in startups founded by eminent professors.\nThis may work in biotech, where a lot of startups simply commercialize\nexisting research, but in software you want to invest in students,\nnot professors.  Microsoft, Yahoo, and Google were all founded by\npeople who dropped out of school to do it.  What students lack in\nexperience they more than make up in dedication.Of course, if you want to get rich, it's not enough merely to be\ndetermined.  You have to be smart too, right?  I'd like to think\nso, but I've had an experience that convinced me otherwise: I spent\nseveral years living in New York.You can lose quite a lot in the brains department and it won't kill\nyou.  But lose even a little bit in the commitment department, and\nthat will kill you very rapidly.Running a startup is like walking on your hands: it's possible, but\nit requires extraordinary effort.  If an ordinary employee were\nasked to do the things a startup founder has to, he'd be very\nindignant.  Imagine if you were hired at some big company, and in\naddition to writing software ten times faster than you'd ever had\nto before, they expected you to answer support calls, administer\nthe servers, design the web site, cold-call customers, find the\ncompany office space, and go out and get everyone lunch.And to do all this not in the calm, womb-like atmosphere of a big\ncompany, but against a backdrop of constant disasters.  That's the\npart that really demands determination.  In a startup, there's\nalways some disaster happening.  So if you're the least bit inclined\nto find an excuse to quit, there's always one right there.But if you lack commitment, chances are it will have been hurting\nyou long before you actually quit.  Everyone who deals with startups\nknows how important commitment is, so if they sense you're ambivalent,\nthey won't give you much attention.  If you lack commitment, you'll\njust find that for some mysterious reason good things happen to\nyour competitors but not to you.  If you lack commitment, it will\nseem to you that you're unlucky.Whereas if you're determined to stick around, people will pay\nattention to you, because odds are they'll have to deal with you\nlater.  You're a local, not just a tourist, so everyone has to come\nto terms with you.At Y Combinator we sometimes mistakenly fund teams who have the\nattitude that they're going to give this startup thing a shot for\nthree months, and if something great happens, they'll stick with\nit-- \"something great\" meaning either that someone wants to buy\nthem or invest millions of dollars in them.  But if this is your\nattitude, \"something great\" is very unlikely to happen to you,\nbecause both acquirers and investors judge you by your level of\ncommitment.If an acquirer thinks you're going to stick around no matter what,\nthey'll be more likely to buy you, because if they don't and you\nstick around, you'll probably grow, your price will go up, and\nthey'll be left wishing they'd bought you earlier.  Ditto for\ninvestors.  What really motivates investors, even big VCs, is not\nthe hope of good returns, but the fear of missing out. \n[6]\nSo if\nyou make it clear you're going to succeed no matter what, and the only\nreason you need them is to make it happen a little faster, you're\nmuch more likely to get money.You can't fake this.  The only way to convince everyone that you're\nready to fight to the death is actually to be ready to.You have to be the right kind of determined, though.  I carefully\nchose the word determined rather than stubborn, because stubbornness\nis a disastrous quality in a startup.  You have to be determined,\nbut flexible, like a running back.  A successful running back doesn't\njust put his head down and try to run through people.  He improvises:\nif someone appears in front of him, he runs around them; if someone\ntries to grab him, he spins out of their grip; he'll even run in\nthe wrong direction briefly if that will help.  The one thing he'll\nnever do is stand still. \n[7]\n6. There Is Always Room.I was talking recently to a startup founder about whether it might\nbe good to add a social component to their software.  He said he\ndidn't think so, because the whole social thing was tapped out.\nReally?  So in a hundred years the only social networking sites\nwill be the Facebook, MySpace, Flickr, and Del.icio.us?  Not likely.There is always room for new stuff.  At every point in history,\neven the darkest bits of the dark ages, people were discovering\nthings that made everyone say \"why didn't anyone think of that\nbefore?\"  We know this continued to be true up till 2004, when the\nFacebook was founded-- though strictly speaking someone else did\nthink of that.The reason we don't see the opportunities all around us is that we\nadjust to however things are, and assume that's how things have to\nbe.  For example, it would seem crazy to most people to try to make\na better search engine than Google.  Surely that field, at least,\nis tapped out.  Really?  In a hundred years-- or even twenty-- are\npeople still going to search for information using something like\nthe current Google?  Even Google probably doesn't think that.In particular, I don't think there's any limit to the number of\nstartups.  Sometimes you hear people saying \"All these guys starting\nstartups now are going to be disappointed. How many little startups\nare Google and Yahoo going to buy, after all?\" That sounds cleverly\nskeptical, but I can prove it's mistaken.  No one proposes that\nthere's some limit to the number of people who can be employed in\nan economy consisting of big, slow-moving companies with a couple\nthousand people each.  Why should there be any limit to the number\nwho could be employed by small, fast-moving companies with ten each?\nIt seems to me the only limit would be the number of people who\nwant to work that hard.The limit on the number of startups is not the number that can get\nacquired by Google and Yahoo-- though it seems even that should\nbe unlimited, if the startups were actually worth buying-- but the\namount of wealth that can be created.  And I don't think there's\nany limit on that, except cosmological ones.So for all practical purposes, there is no limit to the number of\nstartups.  Startups make wealth, which means they make things people\nwant, and if there's a limit on the number of things people want,\nwe are nowhere near it.  I still don't even have a flying car.\n7. Don't Get Your Hopes Up.This is another one I've been repeating since long before Y Combinator.\nIt was practically the corporate motto at Viaweb.Startup founders are naturally optimistic.  They wouldn't do it\notherwise.  But you should treat your optimism the way you'd treat\nthe core of a nuclear reactor: as a source of power that's also\nvery dangerous.  You have to build a shield around it, or it will\nfry you.The shielding of a reactor is not uniform; the reactor would be\nuseless if it were.  It's pierced in a few places to let pipes in.\nAn optimism shield has to be pierced too.  I think the place to\ndraw the line is between what you expect of yourself, and what you\nexpect of other people.  It's ok to be optimistic about what you\ncan do, but assume the worst about machines and other people.This is particularly necessary in a startup, because you tend to\nbe pushing the limits of whatever you're doing.  So things don't\nhappen in the smooth, predictable way they do in the rest of the\nworld.  Things change suddenly, and usually for the worse.Shielding your optimism is nowhere more important than with deals.\nIf your startup is doing a deal, just assume it's not going to\nhappen.  The VCs who say they're going to invest in you aren't.\nThe company that says they're going to buy you isn't.  The big\ncustomer who wants to use your system in their whole company won't.\nThen if things work out you can be pleasantly surprised.The reason I warn startups not to get their hopes up is not to save\nthem from being disappointed when things fall through.  It's\nfor a more practical reason: to prevent them from leaning their\ncompany against something that's going to fall over, taking them\nwith it.For example, if someone says they want to invest in you, there's a\nnatural tendency to stop looking for other investors.  That's why\npeople proposing deals seem so positive: they want you to\nstop looking.  And you want to stop too, because doing deals is a\npain.  Raising money, in particular, is a huge time sink.  So you\nhave to consciously force yourself to keep looking.Even if you ultimately do the first deal, it will be to your advantage\nto have kept looking, because you'll get better terms.  Deals are\ndynamic; unless you're negotiating with someone unusually honest,\nthere's not a single point where you shake hands and the deal's\ndone. There are usually a lot of subsidiary questions to be cleared\nup after the handshake, and if the other side senses weakness-- if\nthey sense you need this deal-- they will be very tempted to screw\nyou in the details.VCs and corp dev guys are professional negotiators.  They're trained\nto take advantage of weakness. \n[8]\nSo while they're often nice\nguys, they just can't help it.  And as pros they do this more than\nyou.  So don't even try to bluff them.  The only way a startup can\nhave any leverage in a deal is genuinely not to need it.  And if\nyou don't believe in a deal, you'll be less likely to depend on it.So I want to plant a hypnotic suggestion in your heads: when you\nhear someone say the words \"we want to invest in you\" or \"we want\nto acquire you,\" I want the following phrase to appear automatically\nin your head: don't get your hopes up.  Just continue running\nyour company as if this deal didn't exist.  Nothing is more likely\nto make it close.The way to succeed in a startup is to focus on the goal of getting\nlots of users, and keep walking swiftly toward it while investors\nand acquirers scurry alongside trying to wave money in your face.\nSpeed, not MoneyThe way I've described it, starting a startup sounds pretty stressful.\nIt is.  When I talk to the founders of the companies we've funded,\nthey all say the same thing: I knew it would be hard, but I didn't\nrealize it would be this hard.So why do it?  It would be worth enduring a lot of pain and stress\nto do something grand or heroic, but just to make money?  Is making\nmoney really that important?No, not really.  It seems ridiculous to me when people take business\ntoo seriously.  I regard making money as a boring errand to be got\nout of the way as soon as possible.  There is nothing grand or\nheroic about starting a startup per se.So why do I spend so much time thinking about startups?  I'll tell\nyou why.  Economically, a startup is best seen not as a way to get\nrich, but as a way to work faster.  You have to make a living, and\na startup is a way to get that done quickly, instead of letting it\ndrag on through your whole life.\n[9]We take it for granted most of the time, but human life is fairly\nmiraculous.  It is also palpably short.  You're given this marvellous\nthing, and then poof, it's taken away.  You can see why people\ninvent gods to explain it.  But even to people who don't believe\nin gods, life commands respect.  There are times in most of our\nlives when the days go by in a blur, and almost everyone has a\nsense, when this happens, of wasting something precious.  As Ben\nFranklin said, if you love life, don't waste time, because time is\nwhat life is made of.So no, there's nothing particularly grand about making money.  That's\nnot what makes startups worth the trouble.  What's important about\nstartups is the speed.  By compressing the dull but necessary task\nof making a living into the smallest possible time, you show respect\nfor life, and there is something grand about that.Notes[1]\nStartups can die from releasing something full of bugs, and not\nfixing them fast enough, but I don't know of any that died from\nreleasing something stable but minimal very early, then promptly\nimproving it.[2]\nI know this is why I haven't released Arc.  The moment I do,\nI'll have people nagging me for features.[3]\nA web site is different from a book or movie or desktop application\nin this respect.  Users judge a site not as a single snapshot, but\nas an animation with multiple frames.  Of the two, I'd say the rate of\nimprovement is more important to users than where you currently\nare.[4]\nIt should not always tell this to users, however.  For example,\nMySpace is basically a replacement mall for mallrats.  But it was\nwiser for them, initially, to pretend that the site was about bands.[5]\nSimilarly, don't make users register to try your site.  Maybe\nwhat you have is so valuable that visitors should gladly register\nto get at it.  But they've been trained to expect the opposite.\nMost of the things they've tried on the web have sucked-- and\nprobably especially those that made them register.[6]\nVCs have rational reasons for behaving this way. They don't\nmake their money (if they make money) off their median investments.\nIn a typical fund, half the companies fail, most of the rest generate\nmediocre returns, and one or two \"make the fund\" by succeeding\nspectacularly.  So if they miss just a few of the most promising\nopportunities, it could hose the whole fund.[7]\nThe attitude of a running back doesn't translate to soccer.\nThough it looks great when a forward dribbles past multiple defenders,\na player who persists in trying such things will do worse in the\nlong term than one who passes.[8]\nThe reason Y Combinator never negotiates valuations\nis that we're not professional negotiators, and don't want to turn\ninto them.[9]\nThere are two ways to do \nwork you love: (a) to make money, then work\non what you love, or (b) to get a job where you get paid to work on\nstuff you love.  In practice the first phases of both\nconsist mostly of unedifying schleps, and in (b) the second phase is less\nsecure.Thanks to Sam Altman, Trevor Blackwell, Beau Hartshorne, Jessica \nLivingston, and Robert Morris for reading drafts of this.May 2001\n\n(I wrote this article to help myself understand exactly\nwhat McCarthy discovered.  You don't need to know this stuff\nto program in Lisp, but it should be helpful to \nanyone who wants to\nunderstand the essence of Lisp \u0097 both in the sense of its\norigins and its semantic core.  The fact that it has such a core\nis one of Lisp's distinguishing features, and the reason why,\nunlike other languages, Lisp has dialects.)In 1960, John \nMcCarthy published a remarkable paper in\nwhich he did for programming something like what Euclid did for\ngeometry. He showed how, given a handful of simple\noperators and a notation for functions, you can\nbuild a whole programming language.\nHe called this language Lisp, for \"List Processing,\"\nbecause one of his key ideas was to use a simple\ndata structure called a list for both\ncode and data.It's worth understanding what McCarthy discovered, not\njust as a landmark in the history of computers, but as\na model for what programming is tending to become in\nour own time.  It seems to me that there have been\ntwo really clean, consistent models of programming so\nfar: the C model and the Lisp model.\nThese two seem points of high ground, with swampy lowlands\nbetween them.  As computers have grown more powerful,\nthe new languages being developed have been moving\nsteadily toward the Lisp model.  A popular recipe\nfor new programming languages in the past 20 years \nhas been to take the C model of computing and add to\nit, piecemeal, parts taken from the Lisp model,\nlike runtime typing and garbage collection.In this article I'm going to try to explain in the\nsimplest possible terms what McCarthy discovered.\nThe point is not just to learn about an interesting\ntheoretical result someone figured out forty years ago,\nbut to show where languages are heading.\nThe unusual thing about Lisp \u0097 in fact, the defining\nquality of Lisp \u0097 is that it can be written in\nitself.  To understand what McCarthy meant by this,\nwe're going to retrace his steps, with his mathematical\nnotation translated into running Common Lisp code.Aaron Swartz created a scraped\nfeed\nof the essays page.October 2015This will come as a surprise to a lot of people, but in some cases\nit's possible to detect bias in a selection process without knowing\nanything about the applicant pool.  Which is exciting because among\nother things it means third parties can use this technique to detect\nbias whether those doing the selecting want them to or not.You can use this technique whenever (a) you have at least\na random sample of the applicants that were selected, (b) their\nsubsequent performance is measured, and (c) the groups of\napplicants you're comparing have roughly equal distribution of ability.How does it work?  Think about what it means to be biased.  What\nit means for a selection process to be biased against applicants\nof type x is that it's harder for them to make it through.  Which\nmeans applicants of type x have to be better to get selected than\napplicants not of type x.\n[1]\nWhich means applicants of type x\nwho do make it through the selection process will outperform other\nsuccessful applicants.  And if the performance of all the successful\napplicants is measured, you'll know if they do.Of course, the test you use to measure performance must be a valid\none.  And in particular it must not be invalidated by the bias you're\ntrying to measure.\nBut there are some domains where performance can be measured, and\nin those detecting bias is straightforward. Want to know if the\nselection process was biased against some type of applicant?  Check\nwhether they outperform the others.  This is not just a heuristic\nfor detecting bias.  It's what bias means.For example, many suspect that venture capital firms are biased\nagainst female founders. This would be easy to detect: among their\nportfolio companies, do startups with female founders outperform\nthose without?  A couple months ago, one VC firm (almost certainly\nunintentionally) published a study showing bias of this type. First\nRound Capital found that among its portfolio companies, startups\nwith female founders outperformed\nthose without by 63%. \n[2]The reason I began by saying that this technique would come as a\nsurprise to many people is that we so rarely see analyses of this\ntype.  I'm sure it will come as a surprise to First Round that they\nperformed one. I doubt anyone there realized that by limiting their\nsample to their own portfolio, they were producing a study not of\nstartup trends but of their own biases when selecting companies.I predict we'll see this technique used more in the future.  The\ninformation needed to conduct such studies is increasingly available.\nData about who applies for things is usually closely guarded by the\norganizations selecting them, but nowadays data about who gets\nselected is often publicly available to anyone who takes the trouble\nto aggregate it.\nNotes[1]\nThis technique wouldn't work if the selection process looked\nfor different things from different types of applicants\u2014for\nexample, if an employer hired men based on their ability but women\nbased on their appearance.[2]\nAs Paul Buchheit points out, First Round excluded their most \nsuccessful investment, Uber, from the study.  And while it \nmakes sense to exclude outliers from some types of studies, \nstudies of returns from startup investing, which is all about \nhitting outliers, are not one of them.\nThanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading\ndrafts of this.November 2005In the next few years, venture capital funds will find themselves\nsqueezed from four directions.  They're already stuck with a seller's\nmarket, because of the huge amounts they raised at the end of the\nBubble and still haven't invested.  This by itself is not the end\nof the world.  In fact, it's just a more extreme version of the\nnorm\nin the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because\nit's getting so cheap to start a startup.  The four causes: open\nsource, which makes software free; Moore's law, which makes hardware\ngeometrically closer to free; the Web, which makes promotion free\nif you're good; and better languages, which make development a lot\ncheaper.When we started our startup in 1995, the first three were our biggest\nexpenses.  We had to pay $5000 for the Netscape Commerce Server,\nthe only software that then supported secure http connections.  We\npaid $3000 for a server with a 90 MHz processor and 32 meg of\nmemory.  And we paid a PR firm about $30,000 to promote our launch.Now you could get all three for nothing.  You can get the software\nfor free; people throw away computers more powerful than our first\nserver; and if you make something good you can generate ten times\nas much traffic by word of mouth online than our first PR firm got\nthrough the print media.And of course another big change for the average startup is that\nprogramming languages have improved-- or rather, the median language has.  At most startups ten years\nago, software development meant ten programmers writing code in\nC++.  Now the same work might be done by one or two using Python\nor Ruby.During the Bubble, a lot of people predicted that startups would\noutsource their development to India.  I think a better model for\nthe future is David Heinemeier Hansson, who outsourced his development\nto a more powerful language instead.  A lot of well-known applications\nare now, like BaseCamp, written by just one programmer.  And one\nguy is more than 10x cheaper than ten, because (a) he won't waste\nany time in meetings, and (b) since he's probably a founder, he can\npay himself nothing.Because starting a startup is so cheap, venture capitalists now\noften want to give startups more money than the startups want to\ntake.  VCs like to invest several million at a time.  But as one\nVC told me after a startup he funded would only take about half a\nmillion, \"I don't know what we're going to do.  Maybe we'll just\nhave to give some of it back.\" Meaning give some of the fund back\nto the institutional investors who supplied it, because it wasn't\ngoing to be possible to invest it all.Into this already bad situation comes the third problem: Sarbanes-Oxley.\nSarbanes-Oxley is a law, passed after the Bubble, that drastically\nincreases the regulatory burden on public companies. And in addition\nto the cost of compliance, which is at least two million dollars a\nyear, the law introduces frightening legal exposure for corporate\nofficers.  An experienced CFO I know said flatly: \"I would not\nwant to be CFO of a public company now.\"You might think that responsible corporate governance is an area\nwhere you can't go too far.  But you can go too far in any law, and\nthis remark convinced me that Sarbanes-Oxley must have.  This CFO\nis both the smartest and the most upstanding money guy I know.  If\nSarbanes-Oxley deters people like him from being CFOs of public  \ncompanies, that's proof enough that it's broken.Largely because of Sarbanes-Oxley, few startups go public now.  For\nall practical purposes, succeeding now equals getting bought.  Which\nmeans VCs are now in the business of finding promising little 2-3\nman startups and pumping them up into companies that cost $100\nmillion to acquire.   They didn't mean to be in this business; it's\njust what their business has evolved into.Hence the fourth problem: the acquirers have begun to realize they\ncan buy wholesale.  Why should they wait for VCs to make the startups\nthey want more expensive?  Most of what the VCs add, acquirers don't\nwant anyway.  The acquirers already have brand recognition and HR\ndepartments.  What they really want is the software and the developers,\nand that's what the startup is in the early phase: concentrated\nsoftware and developers.Google, typically, seems to have been the first to figure this out.\n\"Bring us your startups early,\" said Google's speaker at the Startup School.  They're quite\nexplicit about it: they like to acquire startups at just the point\nwhere they would do a Series A round.  (The Series A round is the\nfirst round of real VC funding; it usually happens in the first\nyear.) It is a brilliant strategy, and one that other big technology\ncompanies will no doubt try to duplicate.  Unless they want to have \nstill more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the\npeople there are rich, or expect to be when their options vest.\nOrdinary employees find it very hard to recommend an acquisition;\nit's just too annoying to see a bunch of twenty year olds get rich\nwhen you're still working for salary.  Even if it's the right thing   \nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\nThey need to do two things, one of which won't surprise them, and  \nanother that will seem an anathema.Let's start with the obvious one: lobby to get Sarbanes-Oxley  \nloosened.  This law was created to prevent future Enrons, not to\ndestroy the IPO market.  Since the IPO market was practically dead\nwhen it passed, few saw what bad effects it would have.  But now \nthat technology has recovered from the last bust, we can see clearly\nwhat a bottleneck Sarbanes-Oxley has become.Startups are fragile plants\u2014seedlings, in fact.  These seedlings\nare worth protecting, because they grow into the trees of the\neconomy.  Much of the economy's growth is their growth.  I think\nmost politicians realize that.  But they don't realize just how   \nfragile startups are, and how easily they can become collateral\ndamage of laws meant to fix some other problem.Still more dangerously, when you destroy startups, they make very\nlittle noise.  If you step on the toes of the coal industry, you'll\nhear about it.  But if you inadvertantly squash the startup industry,\nall that happens is that the founders of the next Google stay in \ngrad school instead of starting a company.My second suggestion will seem shocking to VCs: let founders cash  \nout partially in the Series A round.  At the moment, when VCs invest\nin a startup, all the stock they get is newly issued and all the \nmoney goes to the company.  They could buy some stock directly from\nthe founders as well.Most VCs have an almost religious rule against doing this.  They\ndon't want founders to get a penny till the company is sold or goes\npublic.  VCs are obsessed with control, and they worry that they'll\nhave less leverage over the founders if the founders have any money.This is a dumb plan.  In fact, letting the founders sell a little stock\nearly would generally be better for the company, because it would\ncause the founders' attitudes toward risk to be aligned with the\nVCs'.  As things currently work, their attitudes toward risk tend\nto be diametrically opposed: the founders, who have nothing, would\nprefer a 100% chance of $1 million to a 20% chance of $10 million,\nwhile the VCs can afford to be \"rational\" and prefer the latter.Whatever they say, the reason founders are selling their companies\nearly instead of doing Series A rounds is that they get paid up\nfront.  That first million is just worth so much more than the\nsubsequent ones.  If founders could sell a little stock early,\nthey'd be happy to take VC money and bet the rest on a bigger\noutcome.So why not let the founders have that first million, or at least\nhalf million?  The VCs would get same number of shares for the   \nmoney.  So what if some of the money would go to the  \nfounders instead of the company?Some VCs will say this is\nunthinkable\u2014that they want all their money to be put to work\ngrowing the company.  But the fact is, the huge size of current VC\ninvestments is dictated by the structure\nof VC funds, not the needs of startups.  Often as not these large  \ninvestments go to work destroying the company rather than growing\nit.The angel investors who funded our startup let the founders sell\nsome stock directly to them, and it was a good deal for everyone. \nThe angels made a huge return on that investment, so they're happy.\nAnd for us founders it blunted the terrifying all-or-nothingness\nof a startup, which in its raw form is more a distraction than a\nmotivator.If VCs are frightened at the idea of letting founders partially\ncash out, let me tell them something still more frightening: you\nare now competing directly with Google.\nThanks to Trevor Blackwell, Sarah Harlin, Jessica\nLivingston, and Robert Morris for reading drafts of this.May 2021There's one kind of opinion I'd be very afraid to express publicly.\nIf someone I knew to be both a domain expert and a reasonable person\nproposed an idea that sounded preposterous, I'd be very reluctant\nto say \"That will never work.\"Anyone who has studied the history of ideas, and especially the\nhistory of science, knows that's how big things start. Someone\nproposes an idea that sounds crazy, most people dismiss it, then\nit gradually takes over the world.Most implausible-sounding ideas are in fact bad and could be safely\ndismissed. But not when they're proposed by reasonable domain\nexperts. If the person proposing the idea is reasonable, then they\nknow how implausible it sounds. And yet they're proposing it anyway.\nThat suggests they know something you don't. And if they have deep\ndomain expertise, that's probably the source of it.\n[1]Such ideas are not merely unsafe to dismiss, but disproportionately\nlikely to be interesting. When the average person proposes an\nimplausible-sounding idea, its implausibility is evidence of their\nincompetence. But when a reasonable domain expert does it, the\nsituation is reversed. There's something like an efficient market\nhere: on average the ideas that seem craziest will, if correct,\nhave the biggest effect. So if you can eliminate the theory that\nthe person proposing an implausible-sounding idea is incompetent,\nits implausibility switches from evidence that it's boring to\nevidence that it's exciting.\n[2]Such ideas are not guaranteed to work. But they don't have to be.\nThey just have to be sufficiently good bets \u2014 to have sufficiently\nhigh expected value. And I think on average they do. I think if you\nbet on the entire set of implausible-sounding ideas proposed by\nreasonable domain experts, you'd end up net ahead.The reason is that everyone is too conservative. The word \"paradigm\"\nis overused, but this is a case where it's warranted. Everyone is\ntoo much in the grip of the current paradigm. Even the people who\nhave the new ideas undervalue them initially. Which means that\nbefore they reach the stage of proposing them publicly, they've\nalready subjected them to an excessively strict filter.\n[3]The wise response to such an idea is not to make statements, but\nto ask questions, because there's a real mystery here. Why has this\nsmart and reasonable person proposed an idea that seems so wrong?\nAre they mistaken, or are you? One of you has to be. If you're the\none who's mistaken, that would be good to know, because it means\nthere's a hole in your model of the world. But even if they're\nmistaken, it should be interesting to learn why. A trap that an\nexpert falls into is one you have to worry about too.This all seems pretty obvious. And yet there are clearly a lot of\npeople who don't share my fear of dismissing new ideas. Why do they\ndo it? Why risk looking like a jerk now and a fool later, instead\nof just reserving judgement?One reason they do it is envy. If you propose a radical new idea\nand it succeeds, your reputation (and perhaps also your wealth)\nwill increase proportionally. Some people would be envious if that\nhappened, and this potential envy propagates back into a conviction\nthat you must be wrong.Another reason people dismiss new ideas is that it's an easy way\nto seem sophisticated. When a new idea first emerges, it usually\nseems pretty feeble. It's a mere hatchling. Received wisdom is a\nfull-grown eagle by comparison. So it's easy to launch a devastating\nattack on a new idea, and anyone who does will seem clever to those\nwho don't understand this asymmetry.This phenomenon is exacerbated by the difference between how those\nworking on new ideas and those attacking them are rewarded. The\nrewards for working on new ideas are weighted by the value of the\noutcome. So it's worth working on something that only has a 10%\nchance of succeeding if it would make things more than 10x better.\nWhereas the rewards for attacking new ideas are roughly constant;\nsuch attacks seem roughly equally clever regardless of the target.People will also attack new ideas when they have a vested interest\nin the old ones. It's not surprising, for example, that some of\nDarwin's harshest critics were churchmen. People build whole careers\non some ideas. When someone claims they're false or obsolete, they\nfeel threatened.The lowest form of dismissal is mere factionalism: to automatically\ndismiss any idea associated with the opposing faction. The lowest\nform of all is to dismiss an idea because of who proposed it.But the main thing that leads reasonable people to dismiss new ideas\nis the same thing that holds people back from proposing them: the\nsheer pervasiveness of the current paradigm. It doesn't just affect\nthe way we think; it is the Lego blocks we build thoughts out of.\nPopping out of the current paradigm is something only a few people\ncan do. And even they usually have to suppress their intuitions at\nfirst, like a pilot flying through cloud who has to trust his\ninstruments over his sense of balance.\n[4]Paradigms don't just define our present thinking. They also vacuum\nup the trail of crumbs that led to them, making our standards for\nnew ideas impossibly high. The current paradigm seems so perfect\nto us, its offspring, that we imagine it must have been accepted\ncompletely as soon as it was discovered \u2014 that whatever the church thought\nof the heliocentric model, astronomers must have been convinced as\nsoon as Copernicus proposed it. Far, in fact, from it. Copernicus\npublished the heliocentric model in 1532, but it wasn't till the\nmid seventeenth century that the balance of scientific opinion\nshifted in its favor.\n[5]Few understand how feeble new ideas look when they first appear.\nSo if you want to have new ideas yourself, one of the most valuable\nthings you can do is to learn what they look like when they're born.\nRead about how new ideas happened, and try to get yourself into the\nheads of people at the time. How did things look to them, when the\nnew idea was only half-finished, and even the person who had it was\nonly half-convinced it was right?But you don't have to stop at history. You can observe big new ideas\nbeing born all around you right now. Just look for a reasonable\ndomain expert proposing something that sounds wrong.If you're nice, as well as wise, you won't merely resist attacking\nsuch people, but encourage them. Having new ideas is a lonely\nbusiness. Only those who've tried it know how lonely. These people\nneed your help. And if you help them, you'll probably learn something\nin the process.Notes[1]\nThis domain expertise could be in another field. Indeed,\nsuch crossovers tend to be particularly promising.[2]\nI'm not claiming this principle extends much beyond math,\nengineering, and the hard sciences. In politics, for example,\ncrazy-sounding ideas generally are as bad as they sound. Though\narguably this is not an exception, because the people who propose\nthem are not in fact domain experts; politicians are domain experts\nin political tactics, like how to get elected and how to get\nlegislation passed, but not in the world that policy acts upon.\nPerhaps no one could be.[3]\nThis sense of \"paradigm\" was defined by Thomas Kuhn in his\nStructure of Scientific Revolutions, but I also recommend his\nCopernican Revolution, where you can see him at work developing the\nidea.[4]\nThis is one reason people with a touch of Asperger's may have\nan advantage in discovering new ideas. They're always flying on\ninstruments.[5]\nHall, Rupert. From Galileo to Newton. Collins, 1963. This\nbook is particularly good at getting into contemporaries' heads.Thanks to Trevor Blackwell, Patrick Collison, Suhail Doshi, Daniel\nGackle, Jessica Livingston, and Robert Morris for reading drafts of this.May 2003If Lisp is so great, why don't more people use it?  I was    \nasked this question by a student in the audience at a \ntalk I gave recently.  Not for the first time, either.In languages, as in so many things, there's not much     \ncorrelation between popularity and quality.  Why does   \nJohn Grisham (King of Torts sales rank, 44) outsell\nJane Austen (Pride and Prejudice sales rank, 6191)?\nWould even Grisham claim that it's because he's a better\nwriter?Here's the first sentence of Pride and Prejudice:\n\nIt is a truth universally acknowledged, that a single man \nin possession of a good fortune must be in want of a\nwife.\n\n\"It is a truth universally acknowledged?\"  Long words for\nthe first sentence of a love story.Like Jane Austen, Lisp looks hard.  Its syntax, or lack\nof syntax, makes it look completely unlike \nthe languages\nmost people are used to.  Before I learned Lisp, I was afraid\nof it too.  I recently came across a notebook from 1983\nin which I'd written:\n\nI suppose I should learn Lisp, but it seems so foreign.\n\nFortunately, I was 19 at the time and not too resistant to learning\nnew things.  I was so ignorant that learning\nalmost anything meant learning new things.People frightened by Lisp make up other reasons for not\nusing it.  The standard\nexcuse, back when C was the default language, was that Lisp\nwas too slow.  Now that Lisp dialects are among\nthe faster\nlanguages available, that excuse has gone away.\nNow the standard excuse is openly circular: that other languages\nare more popular.(Beware of such reasoning.  It gets you Windows.)Popularity is always self-perpetuating, but it's especially\nso in programming languages. More libraries\nget written for popular languages, which makes them still\nmore popular.  Programs often have to work with existing programs,\nand this is easier if they're written in the same language,\nso languages spread from program to program like a virus.\nAnd managers prefer popular languages, because they give them \nmore leverage over developers, who can more easily be replaced.Indeed, if programming languages were all more or less equivalent,\nthere would be little justification for using any but the most\npopular.  But they aren't all equivalent, not by a long\nshot.  And that's why less popular languages, like Jane Austen's \nnovels, continue to survive at all.  When everyone else is reading \nthe latest John Grisham novel, there will always be a few people \nreading Jane Austen instead.January 2003(This article is derived from a keynote talk at the fall 2002 meeting\nof NEPLS.)Visitors to this country are often surprised to find that\nAmericans like to begin a conversation by asking \"what do you do?\"\nI've never liked this question.  I've rarely had a\nneat answer to it.  But I think I have finally solved the problem.\nNow, when someone asks me what I do, I look them straight\nin the eye and say \"I'm designing a \nnew dialect of Lisp.\"   \nI recommend this answer to anyone who doesn't like being asked what\nthey do.  The conversation will turn immediately to other topics.I don't consider myself to be doing research on programming languages.\nI'm just designing one, in the same way that someone might design\na building or a chair or a new typeface.\nI'm not trying to discover anything new.  I just want\nto make a language that will be good to program in.  In some ways,\nthis assumption makes life a lot easier.The difference between design and research seems to be a question\nof new versus good.  Design doesn't have to be new, but it has to  \nbe good.  Research doesn't have to be good, but it has to be new.\nI think these two paths converge at the top: the best design\nsurpasses its predecessors by using new ideas, and the best research\nsolves problems that are not only new, but actually worth solving.\nSo ultimately we're aiming for the same destination, just approaching\nit from different directions.What I'm going to talk about today is what your target looks like\nfrom the back.  What do you do differently when you treat\nprogramming languages as a design problem instead of a research topic?The biggest difference is that you focus more on the user.\nDesign begins by asking, who is this\nfor and what do they need from it?  A good architect,\nfor example, does not begin by creating a design that he then\nimposes on the users, but by studying the intended users and figuring\nout what they need.Notice I said \"what they need,\" not \"what they want.\"  I don't mean\nto give the impression that working as a designer means working as \na sort of short-order cook, making whatever the client tells you\nto.  This varies from field to field in the arts, but\nI don't think there is any field in which the best work is done by\nthe people who just make exactly what the customers tell them to.The customer is always right in\nthe sense that the measure of good design is how well it works\nfor the user.  If you make a novel that bores everyone, or a chair\nthat's horribly uncomfortable to sit in, then you've done a bad\njob, period.  It's no defense to say that the novel or the chair  \nis designed according to the most advanced theoretical principles.And yet, making what works for the user doesn't mean simply making\nwhat the user tells you to.  Users don't know what all the choices\nare, and are often mistaken about what they really want.The answer to the paradox, I think, is that you have to design\nfor the user, but you have to design what the user needs, not simply  \nwhat he says he wants.\nIt's much like being a doctor.  You can't just treat a patient's\nsymptoms.  When a patient tells you his symptoms, you have to figure\nout what's actually wrong with him, and treat that.This focus on the user is a kind of axiom from which most of the\npractice of good design can be derived, and around which most design\nissues center.If good design must do what the user needs, who is the user?  When\nI say that design must be for users, I don't mean to imply that good \ndesign aims at some kind of  \nlowest common denominator.  You can pick any group of users you\nwant.  If you're designing a tool, for example, you can design it\nfor anyone from beginners to experts, and what's good design\nfor one group might be bad for another.  The point\nis, you have to pick some group of users.  I don't think you can\neven talk about good or bad design except with\nreference to some intended user.You're most likely to get good design if the intended users include\nthe designer himself.  When you design something\nfor a group that doesn't include you, it tends to be for people\nyou consider to be less sophisticated than you, not more sophisticated.That's a problem, because looking down on the user, however benevolently,\nseems inevitably to corrupt the designer.\nI suspect that very few housing\nprojects in the US were designed by architects who expected to live\nin them.   You can see the same thing\nin programming languages.  C, Lisp, and Smalltalk were created for\ntheir own designers to use.  Cobol, Ada, and Java, were created   \nfor other people to use.If you think you're designing something for idiots, the odds are\nthat you're not designing something good, even for idiots.\nEven if you're designing something for the most sophisticated\nusers, though, you're still designing for humans.  It's different \nin research.  In math you\ndon't choose abstractions because they're\neasy for humans to understand; you choose whichever make the\nproof shorter.  I think this is true for the sciences generally.\nScientific ideas are not meant to be ergonomic.Over in the arts, things are very different.  Design is\nall about people.  The human body is a strange\nthing, but when you're designing a chair,\nthat's what you're designing for, and there's no way around it.\nAll the arts have to pander to the interests and limitations\nof humans.   In painting, for example, all other things being\nequal a painting with people in it will be more interesting than\none without.  It is not merely an accident of history that\nthe great paintings of the Renaissance are all full of people.\nIf they hadn't been, painting as a medium wouldn't have the prestige\nthat it does.Like it or not, programming languages are also for people,\nand I suspect the human brain is just as lumpy and idiosyncratic\nas the human body.  Some ideas are easy for people to grasp\nand some aren't.  For example, we seem to have a very limited\ncapacity for dealing with detail.  It's this fact that makes\nprograming languages a good idea in the first place; if we\ncould handle the detail, we could just program in machine\nlanguage.Remember, too, that languages are not\nprimarily a form for finished programs, but something that\nprograms have to be developed in.  Anyone in the arts could\ntell you that you might want different mediums for the\ntwo situations.  Marble, for example, is a nice, durable\nmedium for finished ideas, but a hopelessly inflexible one\nfor developing new ideas.A program, like a proof,\nis a pruned version of a tree that in the past has had\nfalse starts branching off all over it.  So the test of\na language is not simply how clean the finished program looks\nin it, but how clean the path to the finished program was.\nA design choice that gives you elegant finished programs\nmay not give you an elegant design process.  For example, \nI've written a few macro-defining macros full of nested\nbackquotes that look now like little gems, but writing them\ntook hours of the ugliest trial and error, and frankly, I'm still\nnot entirely sure they're correct.We often act as if the test of a language were how good\nfinished programs look in it.\nIt seems so convincing when you see the same program\nwritten in two languages, and one version is much shorter.\nWhen you approach the problem from the direction of the\narts, you're less likely to depend on this sort of\ntest.  You don't want to end up with a programming\nlanguage like marble.For example, it is a huge win in developing software to\nhave an interactive toplevel, what in Lisp is called a\nread-eval-print loop.  And when you have one this has\nreal effects on the design of the language.  It would not\nwork well for a language where you have to declare\nvariables before using them, for example.  When you're\njust typing expressions into the toplevel, you want to be \nable to set x to some value and then start doing things\nto x.  You don't want to have to declare the type of x\nfirst.  You may dispute either of the premises, but if\na language has to have a toplevel to be convenient, and\nmandatory type declarations are incompatible with a\ntoplevel, then no language that makes type declarations  \nmandatory could be convenient to program in.In practice, to get good design you have to get close, and stay\nclose, to your users.  You have to calibrate your ideas on actual\nusers constantly, especially in the beginning.  One of the reasons\nJane Austen's novels are so good is that she read them out loud to\nher family.  That's why she never sinks into self-indulgently arty\ndescriptions of landscapes,\nor pretentious philosophizing.  (The philosophy's there, but it's\nwoven into the story instead of being pasted onto it like a label.)\nIf you open an average \"literary\" novel and imagine reading it out loud\nto your friends as something you'd written, you'll feel all too\nkeenly what an imposition that kind of thing is upon the reader.In the software world, this idea is known as Worse is Better.\nActually, there are several ideas mixed together in the concept of\nWorse is Better, which is why people are still arguing about\nwhether worse\nis actually better or not.  But one of the main ideas in that\nmix is that if you're building something new, you should get a\nprototype in front of users as soon as possible.The alternative approach might be called the Hail Mary strategy.\nInstead of getting a prototype out quickly and gradually refining\nit, you try to create the complete, finished, product in one long\ntouchdown pass.  As far as I know, this is a\nrecipe for disaster.  Countless startups destroyed themselves this\nway during the Internet bubble.  I've never heard of a case\nwhere it worked.What people outside the software world may not realize is that\nWorse is Better is found throughout the arts.\nIn drawing, for example, the idea was discovered during the\nRenaissance.  Now almost every drawing teacher will tell you that\nthe right way to get an accurate drawing is not to\nwork your way slowly around the contour of an object, because errors will\naccumulate and you'll find at the end that the lines don't meet.\nInstead you should draw a few quick lines in roughly the right place,\nand then gradually refine this initial sketch.In most fields, prototypes\nhave traditionally been made out of different materials.\nTypefaces to be cut in metal were initially designed  \nwith a brush on paper.  Statues to be cast in bronze   \nwere modelled in wax.  Patterns to be embroidered on tapestries\nwere drawn on paper with ink wash.  Buildings to be\nconstructed from stone were tested on a smaller scale in wood.What made oil paint so exciting, when it\nfirst became popular in the fifteenth century, was that you\ncould actually make the finished work from the prototype.\nYou could make a preliminary drawing if you wanted to, but you\nweren't held to it; you could work out all the details, and\neven make major changes, as you finished the painting.You can do this in software too.  A prototype doesn't have to\nbe just a model; you can refine it into the finished product.\nI think you should always do this when you can.  It lets you\ntake advantage of new insights you have along the way.  But\nperhaps even more important, it's good for morale.Morale is key in design.  I'm surprised people\ndon't talk more about it.  One of my first\ndrawing teachers told me: if you're bored when you're\ndrawing something, the drawing will look boring.\nFor example, suppose you have to draw a building, and you\ndecide to draw each brick individually.  You can do this\nif you want, but if you get bored halfway through and start\nmaking the bricks mechanically instead of observing each one,   \nthe drawing will look worse than if you had merely suggested\nthe bricks.Building something by gradually refining a prototype is good\nfor morale because it keeps you engaged.  In software, my  \nrule is: always have working code.  If you're writing\nsomething that you'll be able to test in an hour, then you\nhave the prospect of an immediate reward to motivate you.\nThe same is true in the arts, and particularly in oil painting.\nMost painters start with a blurry sketch and gradually\nrefine it.\nIf you work this way, then in principle\nyou never have to end the day with something that actually\nlooks unfinished.  Indeed, there is even a saying among\npainters: \"A painting is never finished, you just stop\nworking on it.\"  This idea will be familiar to anyone who\nhas worked on software.Morale is another reason that it's hard to design something\nfor an unsophisticated user.   It's hard to stay interested in\nsomething you don't like yourself.  To make something  \ngood, you have to be thinking, \"wow, this is really great,\"\nnot \"what a piece of shit; those fools will love it.\"Design means making things for humans.  But it's not just the\nuser who's human.  The designer is human too.Notice all this time I've been talking about \"the designer.\"\nDesign usually has to be under the control of a single person to\nbe any good.   And yet it seems to be possible for several people\nto collaborate on a research project.  This seems to\nme one of the most interesting differences between research and\ndesign.There have been famous instances of collaboration in the arts,\nbut most of them seem to have been cases of molecular bonding rather\nthan nuclear fusion.  In an opera it's common for one person to\nwrite the libretto and another to write the music.   And during the Renaissance, \njourneymen from northern\nEurope were often employed to do the landscapes in the\nbackgrounds of Italian paintings.  But these aren't true collaborations.\nThey're more like examples of Robert Frost's\n\"good fences make good neighbors.\"  You can stick instances\nof good design together, but within each individual project,\none person has to be in control.I'm not saying that good design requires that one person think\nof everything.  There's nothing more valuable than the advice\nof someone whose judgement you trust.  But after the talking is\ndone, the decision about what to do has to rest with one person.Why is it that research can be done by collaborators and  \ndesign can't?  This is an interesting question.  I don't \nknow the answer.  Perhaps,\nif design and research converge, the best research is also\ngood design, and in fact can't be done by collaborators.\nA lot of the most famous scientists seem to have worked alone.\nBut I don't know enough to say whether there\nis a pattern here.  It could be simply that many famous scientists\nworked when collaboration was less common.Whatever the story is in the sciences, true collaboration\nseems to be vanishingly rare in the arts.  Design by committee is a\nsynonym for bad design.  Why is that so?  Is there some way to\nbeat this limitation?I'm inclined to think there isn't-- that good design requires\na dictator.  One reason is that good design has to   \nbe all of a piece.  Design is not just for humans, but\nfor individual humans.  If a design represents an idea that  \nfits in one person's head, then the idea will fit in the user's\nhead too.Related:December 2001 (rev. May 2002)\n\n(This article came about in response to some questions on\nthe LL1 mailing list.  It is now\nincorporated in Revenge of the Nerds.)When McCarthy designed Lisp in the late 1950s, it was\na radical departure from existing languages,\nthe most important of which was Fortran.Lisp embodied nine new ideas:\n1. Conditionals.  A conditional is an if-then-else\nconstruct.  We take these for granted now.  They were \ninvented\nby McCarthy in the course of developing Lisp. \n(Fortran at that time only had a conditional\ngoto, closely based on the branch instruction in the \nunderlying hardware.)  McCarthy, who was on the Algol committee, got\nconditionals into Algol, whence they spread to most other\nlanguages.2. A function type. In Lisp, functions are first class \nobjects-- they're a data type just like integers, strings,\netc, and have a literal representation, can be stored in variables,\ncan be passed as arguments, and so on.3. Recursion.  Recursion existed as a mathematical concept\nbefore Lisp of course, but Lisp was the first programming language to support\nit.  (It's arguably implicit in making functions first class\nobjects.)4. A new concept of variables.  In Lisp, all variables\nare effectively pointers. Values are what\nhave types, not variables, and assigning or binding\nvariables means copying pointers, not what they point to.5. Garbage-collection.6. Programs composed of expressions. Lisp programs are \ntrees of expressions, each of which returns a value.  \n(In some Lisps expressions\ncan return multiple values.)  This is in contrast to Fortran\nand most succeeding languages, which distinguish between\nexpressions and statements.It was natural to have this\ndistinction in Fortran because (not surprisingly in a language\nwhere the input format was punched cards) the language was\nline-oriented.  You could not nest statements.  And\nso while you needed expressions for math to work, there was\nno point in making anything else return a value, because\nthere could not be anything waiting for it.This limitation\nwent away with the arrival of block-structured languages,\nbut by then it was too late. The distinction between\nexpressions and statements was entrenched.  It spread from \nFortran into Algol and thence to both their descendants.When a language is made entirely of expressions, you can\ncompose expressions however you want.  You can say either\n(using Arc syntax)(if foo (= x 1) (= x 2))or(= x (if foo 1 2))7. A symbol type.  Symbols differ from strings in that\nyou can test equality by comparing a pointer.8. A notation for code using trees of symbols.9. The whole language always available.  \nThere is\nno real distinction between read-time, compile-time, and runtime.\nYou can compile or run code while reading, read or run code\nwhile compiling, and read or compile code at runtime.Running code at read-time lets users reprogram Lisp's syntax;\nrunning code at compile-time is the basis of macros; compiling\nat runtime is the basis of Lisp's use as an extension\nlanguage in programs like Emacs; and reading at runtime\nenables programs to communicate using s-expressions, an\nidea recently reinvented as XML.\nWhen Lisp was first invented, all these ideas were far\nremoved from ordinary programming practice, which was\ndictated largely by the hardware available in the late 1950s.Over time, the default language, embodied\nin a succession of popular languages, has\ngradually evolved toward Lisp.  1-5 are now widespread.\n6 is starting to appear in the mainstream.\nPython has a form of 7, though there doesn't seem to be\nany syntax for it.  \n8, which (with 9) is what makes Lisp macros\npossible, is so far still unique to Lisp,\nperhaps because (a) it requires those parens, or something \njust as bad, and (b) if you add that final increment of power, \nyou can no \nlonger claim to have invented a new language, but only\nto have designed a new dialect of Lisp ; -)Though useful to present-day programmers, it's\nstrange to describe Lisp in terms of its\nvariation from the random expedients other languages\nadopted.  That was not, probably, how McCarthy\nthought of it.  Lisp wasn't designed to fix the mistakes\nin Fortran; it came about more as the byproduct of an\nattempt to axiomatize computation.August 2021When people say that in their experience all programming languages\nare basically equivalent, they're making a statement not about\nlanguages but about the kind of programming they've done.99.5% of programming consists of gluing together calls to library\nfunctions. All popular languages are equally good at this. So one\ncan easily spend one's whole career operating in the intersection\nof popular programming languages.But the other .5% of programming is disproportionately interesting.\nIf you want to learn what it consists of, the weirdness of weird\nlanguages is a good clue to follow.Weird languages aren't weird by accident. Not the good ones, at\nleast. The weirdness of the good ones usually implies the existence\nof some form of programming that's not just the usual gluing together\nof library calls.A concrete example: Lisp macros. Lisp macros seem weird even to\nmany Lisp programmers. They're not only not in the intersection of\npopular languages, but by their nature would be hard to implement\nproperly in a language without turning it into a dialect of\nLisp. And macros are definitely evidence of techniques that go\nbeyond glue programming. For example, solving problems by first\nwriting a language for problems of that type, and then writing\nyour specific application in it. Nor is this all you can do with\nmacros; it's just one region in a space of program-manipulating\ntechniques that even now is far from fully explored.So if you want to expand your concept of what programming can be,\none way to do it is by learning weird languages. Pick a language\nthat most programmers consider weird but whose median user is smart,\nand then focus on the differences between this language and the\nintersection of popular languages. What can you say in this language\nthat would be impossibly inconvenient to say in others? In the\nprocess of learning how to say things you couldn't previously say,\nyou'll probably be learning how to think things you couldn't\npreviously think.\nThanks to Trevor Blackwell, Patrick Collison, Daniel Gackle, Amjad\nMasad, and Robert Morris for reading drafts of this.\nJanuary 2015Corporate Development, aka corp dev, is the group within companies\nthat buys other companies. If you're talking to someone from corp\ndev, that's why, whether you realize it yet or not.It's usually a mistake to talk to corp dev unless (a) you want to\nsell your company right now and (b) you're sufficiently likely to\nget an offer at an acceptable price.  In practice that means startups\nshould only talk to corp dev when they're either doing really well\nor really badly.  If you're doing really badly, meaning the company\nis about to die, you may as well talk to them, because you have\nnothing to lose. And if you're doing really well, you can safely\ntalk to them, because you both know the price will have to be high,\nand if they show the slightest sign of wasting your time, you'll\nbe confident enough to tell them to get lost.The danger is to companies in the middle.  Particularly to young\ncompanies that are growing fast, but haven't been doing it for long\nenough to have grown big yet.  It's usually a mistake for a promising\ncompany less than a year old even to talk to corp dev.But it's a mistake founders constantly make.  When someone from\ncorp dev wants to meet, the founders tell themselves they should\nat least find out what they want.  Besides, they don't want to\noffend Big Company by refusing to meet.Well, I'll tell you what they want.  They want to talk about buying\nyou.  That's what the title \"corp dev\" means.   So before agreeing\nto meet with someone from corp dev, ask yourselves, \"Do we want to\nsell the company right now?\"  And if the answer is no, tell them\n\"Sorry, but we're focusing on growing the company.\"  They won't be\noffended.  And certainly the founders of Big Company won't be\noffended. If anything they'll think more highly of you.  You'll\nremind them of themselves.  They didn't sell either; that's why\nthey're in a position now to buy other companies.\n[1]Most founders who get contacted by corp dev already know what it\nmeans.  And yet even when they know what corp dev does and know\nthey don't want to sell, they take the meeting.  Why do they do it?\nThe same mix of denial and wishful thinking that underlies most\nmistakes founders make. It's flattering to talk to someone who wants\nto buy you.  And who knows, maybe their offer will be surprisingly\nhigh.  You should at least see what it is, right?No.  If they were going to send you an offer immediately by email,\nsure, you might as well open it.  But that is not how conversations\nwith corp dev work.  If you get an offer at all, it will be at the\nend of a long and unbelievably distracting process.  And if the\noffer is surprising, it will be surprisingly low.Distractions are the thing you can least afford in a startup.  And\nconversations with corp dev are the worst sort of distraction,\nbecause as well as consuming your attention they undermine your\nmorale.  One of the tricks to surviving a grueling process is not\nto stop and think how tired you are.  Instead you get into a sort\nof flow. \n[2]\nImagine what it would do to you if at mile 20 of a\nmarathon, someone ran up beside you and said \"You must feel really\ntired.  Would you like to stop and take a rest?\"  Conversations\nwith corp dev are like that but worse, because the suggestion of\nstopping gets combined in your mind with the imaginary high price\nyou think they'll offer.And then you're really in trouble.  If they can, corp dev people\nlike to turn the tables on you. They like to get you to the point\nwhere you're trying to convince them to buy instead of them trying\nto convince you to sell.  And surprisingly often they succeed.This is a very slippery slope, greased with some of the most powerful\nforces that can work on founders' minds, and attended by an experienced\nprofessional whose full time job is to push you down it.Their tactics in pushing you down that slope are usually fairly\nbrutal. Corp dev people's whole job is to buy companies, and they\ndon't even get to choose which.  The only way their performance is\nmeasured is by how cheaply they can buy you, and the more ambitious\nones will stop at nothing to achieve that. For example, they'll\nalmost always start with a lowball offer, just to see if you'll\ntake it. Even if you don't, a low initial offer will demoralize you\nand make you easier to manipulate.And that is the most innocent of their tactics. Just wait till\nyou've agreed on a price and think you have a done deal, and then\nthey come back and say their boss has vetoed the deal and won't do\nit for more than half the agreed upon price. Happens all the time.\nIf you think investors can behave badly, it's nothing compared to\nwhat corp dev people can do.  Even corp dev people at companies\nthat are otherwise benevolent.I remember once complaining to a\nfriend at Google about some nasty trick their corp dev people had\npulled on a YC startup.\"What happened to Don't be Evil?\" I asked.\"I don't think corp dev got the memo,\" he replied.The tactics you encounter in M&A conversations can be like nothing\nyou've experienced in the otherwise comparatively \nupstanding world\nof Silicon Valley.  It's as if a chunk of genetic material from the\nold-fashioned robber baron business world got incorporated into the\nstartup world.\n[3]The simplest way to protect yourself is to use the trick that John\nD. Rockefeller, whose grandfather was an alcoholic, used to protect\nhimself from becoming one.  He once told a Sunday school class\n\n  Boys, do you know why I never became a drunkard?  Because I never\n  took the first drink.\n\nDo you want to sell your company right now?  Not eventually, right\nnow.  If not, just don't take the first meeting.  They won't be\noffended.  And you in turn will be guaranteed to be spared one of\nthe worst experiences that can happen to a startup.If you do want to sell, there's another set of \ntechniques\n for doing\nthat.  But the biggest mistake founders make in dealing with corp\ndev is not doing a bad job of talking to them when they're ready\nto, but talking to them before they are.  So if you remember only\nthe title of this essay, you already know most of what you need to\nknow about M&A in the first year.Notes[1]\nI'm not saying you should never sell.  I'm saying you should\nbe clear in your own mind about whether you want to sell or not,\nand not be led by manipulation or wishful thinking into trying to\nsell earlier than you otherwise would have.[2]\nIn a startup, as in most competitive sports, the task at hand\nalmost does this for you; you're too busy to feel tired.  But when\nyou lose that protection, e.g. at the final whistle, the fatigue\nhits you like a wave.  To talk to corp dev is to let yourself feel\nit mid-game.[3]\nTo be fair, the apparent misdeeds of corp dev people are magnified\nby the fact that they function as the face of a large organization\nthat often doesn't know its own mind.  Acquirers can be surprisingly\nindecisive about acquisitions, and their flakiness is indistinguishable\nfrom dishonesty by the time it filters down to you.Thanks to Marc Andreessen, Jessica Livingston, Geoff\nRalston, and Qasar Younis for reading drafts of this.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nOctober 2010\n\n(I wrote this"}, {"role": "user", "content": "What is the best thing to do in San Francisco? Don't give information outside the document or repeat your findings"}]